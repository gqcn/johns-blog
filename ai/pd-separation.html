<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-docs/AI技术/分布式推理/PD(Prefill&Decode)分离介绍" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.6.3"><title data-rh=true>PD(Prefill&Decode)分离 | John's Blog</title><meta data-rh=true name=viewport content="width=device-width,initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:url content=https://johng.cn/ai/pd-separation><meta data-rh=true property=og:locale content=en><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-default-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-default-current><meta data-rh=true property=og:title content="PD(Prefill&Decode)分离 | John's Blog"><meta data-rh=true name=description content=深入探讨LLM推理中的PD(Prefill&Decode)分离技术，分析其原理、指标、优势及实现方案，提升大模型推理性能和用户体验><meta data-rh=true property=og:description content=深入探讨LLM推理中的PD(Prefill&Decode)分离技术，分析其原理、指标、优势及实现方案，提升大模型推理性能和用户体验><meta data-rh=true name=keywords content="AI技术,LLM推理优化,PD分离,Prefill,Decode,TTFT,TPOT,TBT,Continuous Batching,KV Cache,大模型部署"><link data-rh=true rel=icon href=/img/favicon.ico><link data-rh=true rel=canonical href=https://johng.cn/ai/pd-separation><link data-rh=true rel=alternate href=https://johng.cn/ai/pd-separation hreflang=en><link data-rh=true rel=alternate href=https://johng.cn/ai/pd-separation hreflang=x-default><link data-rh=true rel=preconnect href=https://XGS1CPQERK-dsn.algolia.net crossorigin><link rel=search type=application/opensearchdescription+xml title="John's Blog" href=/opensearch.xml><script src=https://hm.baidu.com/hm.js?6b4ae23dc83ee5efe875b7172af6c7c1 async></script><script src=https://cdn.wwads.cn/js/makemoney.js async></script><link rel=stylesheet href=/assets/css/styles.208c11f9.css><script src=/assets/js/runtime~main.aa643263.js defer></script><script src=/assets/js/main.56284325.js defer></script><body class=navigation-with-keyboard><script>!function(){var t,e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t=null!==e?e:"light",document.documentElement.setAttribute("data-theme",t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/><b class="navbar__title text--truncate">John's Blog</b></a><a aria-current=page class="navbar__item navbar__link navbar__link--active" sidebarid=mainSidebar href=/ai>AI技术</a><a class="navbar__item navbar__link" sidebarid=mainSidebar href=/cloud-native>云原生</a><a class="navbar__item navbar__link" sidebarid=mainSidebar href=/notes>日常笔记</a><a class="navbar__item navbar__link" sidebarid=mainSidebar href=/programming>开发语言</a><a class="navbar__item navbar__link" sidebarid=mainSidebar href=/architecture>技术架构</a><a class="navbar__item navbar__link" sidebarid=mainSidebar href=/observability>可观测性</a><a class="navbar__item navbar__link" sidebarid=mainSidebar href=/database-and-middleware>数据库与中间件</a><a class="navbar__item navbar__link" href=/aboutme>关于我</a></div><div class="navbar__items navbar__items--right"><a href=https://goframe.org/ target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-goframe-link"></a><a href=https://github.com/gqcn target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-github-link"></a><div class=navbarSearchContainer_Bca1><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_z2l0"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class=sidebar_njMd><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/ai>AI技术</a><button aria-label="Collapse sidebar category 'AI技术'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/nvidia>NVIDIA</a><button aria-label="Expand sidebar category 'NVIDIA'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" tabindex=0 href=/ai/distributed-inference>分布式推理</a><button aria-label="Collapse sidebar category '分布式推理'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/ai/pd-separation>PD(Prefill&Decode)分离</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/ai/nvidia-dynamo>NVIDIA Dynamo: 分布式AI推理的高效引擎</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/ai/single-node-multi-gpu-nvlink-communication-issue>单机多卡部署，GPU之间无法使用NVLINK通信问题排查</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/scheduling>算力资源调度</a><button aria-label="Expand sidebar category '算力资源调度'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ai/nfd-gfd>NFD&GFD</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ai/common-acceleration-cards>常见智算加速卡汇总</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ai/rdma>RDMA技术架构深度解析</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ai/general-ai-model-and-reasoning-ai-model-difference>通用大模型和推理大模型区别</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ai/common-ai-model-training-inference-framework>常见AI模型训练推理框架对比</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ai/ai-training-inference-scenarios>AI模型训练推理常见业务场景痛点</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/cloud-native>云原生</a><button aria-label="Expand sidebar category '云原生'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/notes>日常笔记</a><button aria-label="Expand sidebar category '日常笔记'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/programming>开发语言</a><button aria-label="Expand sidebar category '开发语言'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/architecture>技术架构</a><button aria-label="Expand sidebar category '技术架构'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/management>技术管理</a><button aria-label="Expand sidebar category '技术管理'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/observability>可观测性</a><button aria-label="Expand sidebar category '可观测性'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/database-and-middleware>数据库与中间件</a><button aria-label="Expand sidebar category '数据库与中间件'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/operating-systems-and-networks>操作系统和网络</a><button aria-label="Expand sidebar category '操作系统和网络'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div></ul></nav></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/ai><span itemprop=name>AI技术</span></a><meta itemprop=position content=1><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/ai/distributed-inference><span itemprop=name>分布式推理</span></a><meta itemprop=position content=2><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link itemprop=name>PD(Prefill&Decode)分离</span><meta itemprop=position content=3></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id=引言>引言<a href=#引言 class=hash-link aria-label="Direct link to 引言" title="Direct link to 引言">​</a></h2>
<p>随着大型语言模型（<code>LLM</code>）在各个领域的广泛应用，如何高效地部署和推理这些模型成为了一个关键挑战。传统的<code>LLM</code>推理架构将整个推理过程视为一个整体，但这种做法往往无法充分利用硬件资源，导致性能瓶颈和用户体验下降。</p>
<p><code>PD（Prefill & Decode）</code>分离技术作为一种创新的架构设计，通过将<code>LLM</code>推理过程分解为两个独立的阶段，并针对每个阶段的特性进行专门优化，显著提升了推理效率和用户体验。本文将深入探讨<code>PD</code>分离技术的原理、实现方案以及带来的优势。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=llm推理基础认识prefill和decode>LLM推理基础：认识Prefill和Decode<a href=#llm推理基础认识prefill和decode class=hash-link aria-label="Direct link to LLM推理基础：认识Prefill和Decode" title="Direct link to LLM推理基础：认识Prefill和Decode">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=推理过程概述>推理过程概述<a href=#推理过程概述 class=hash-link aria-label="Direct link to 推理过程概述" title="Direct link to 推理过程概述">​</a></h3>
<p>在深入了解<code>PD</code>分离之前，我们需要先理解<code>LLM</code>的推理过程。整个<code>LLM</code>推理过程可以分为两个截然不同的阶段：</p>
<ul>
<li>
<p><strong>Prefill阶段（预填充阶段）</strong>：这是计算密集型阶段，<code>LLM</code>并行处理所有用户输入的<code>token</code>，计算出对应的<code>KV Cache</code>，并生成第一个输出<code>token</code>。</p>
</li>
<li>
<p><strong>Decode阶段（解码阶段）</strong>：这是显存密集型阶段，模型基于已有的<code>KV Cache</code>，通过自回归方式顺序生成后续的<code>token</code>，每次迭代只产生一个新<code>token</code>。</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=性能评估指标>性能评估指标<a href=#性能评估指标 class=hash-link aria-label="Direct link to 性能评估指标" title="Direct link to 性能评估指标">​</a></h3>
<p>为了准确衡量<code>LLM</code>推理系统的性能，业界定义了几个关键指标：</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=prefill性能评估指标>Prefill性能评估指标<a href=#prefill性能评估指标 class=hash-link aria-label="Direct link to Prefill性能评估指标" title="Direct link to Prefill性能评估指标">​</a></h4>
<ul>
<li>
<p><strong>TTFT(Time To First Token)</strong>：表示从接收用户请求到生成第一个<code>token</code>所用的时间</p>
<p>例如：<code>P90 TTFT SLO = 0.4s</code> 意味着<code>90%</code>的请求的<code>TTFT</code>值都必须<code>≤0.4</code>秒</p>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=decode性能评估指标>Decode性能评估指标<a href=#decode性能评估指标 class=hash-link aria-label="Direct link to Decode性能评估指标" title="Direct link to Decode性能评估指标">​</a></h4>
<ul>
<li>
<p><strong>TPOT(Time Per Output Token)</strong>：表示生成每一个响应<code>token</code>所用的平均时间</p>
<p>例如：<code>P90 TPOT SLO = 0.04s</code> 意味着<code>90%</code>的请求的<code>TPOT</code>值都必须<code>≤0.04</code>秒</p>
</li>
<li>
<p><strong>TBT(Token By Token)</strong>：表示连续生成两个<code>token</code>之间的时间间隔，这是衡量用户体验流畅度的重要指标。<code>TBT</code>越稳定，用户感知到的响应就越连贯</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=两阶段特性深度对比>两阶段特性深度对比<a href=#两阶段特性深度对比 class=hash-link aria-label="Direct link to 两阶段特性深度对比" title="Direct link to 两阶段特性深度对比">​</a></h3>
<p>通过下表我们可以清晰地看到<code>Prefill</code>和<code>Decode</code>阶段在各个维度上的显著差异：</p>
<table><thead><tr><th><strong>特性</strong><th><strong>Prefill（预填充）阶段</strong><th><strong>Decode（解码）阶段</strong><tbody><tr><td><strong>计算模式</strong><td>并行计算（所有输入<code>Token</code>同时处理）<td>串行计算（逐个<code>Token</code>生成）<tr><td><strong>计算强度</strong><td>计算密集型（矩阵乘法为主）<td>内存带宽受限（访存频繁）<tr><td><strong>GPU利用率</strong><td>高（接近<code>100%</code>）<td>极低（约<code>1%</code>）<tr><td><strong>关键性能指标</strong><td>首次<code>Token</code>时间（<code>TTFT</code>）<td><code>Token</code>生成时间（<code>TPOT</code>）<tr><td><strong>主要瓶颈</strong><td>算力（<code>FLOPs</code>）<td>内存带宽（<code>Memory Bandwidth</code>）<tr><td><strong>显存占用</strong><td>临时高（需缓存输入序列）<td>持续高（需保存<code>KV Cache</code>）<tr><td><strong>批处理优化空间</strong><td>大（可合并多请求输入）<td>小（动态调整生成任务）<tr><td><strong>典型延迟</strong><td>短（毫秒级，如<code>0.2</code>秒处理<code>255 Token</code>）<td>长（秒级，如<code>32</code>秒生成<code>256 Token</code>）<tr><td><strong>加速手段</strong><td><code>Tensor Core</code>加速、<code>FP16/INT8</code>量化<td>内存访问优化、<code>KV Cache</code>压缩<tr><td><strong>通信需求</strong><td>低（单节点可完成）<td>高（分布式需同步<code>KV Cache</code>）<tr><td><strong>调度优先级</strong><td>高（优先保证<code>TTFT</code>）<td>中（需稳定<code>TPOT</code>）</table>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=为什么需要pd分离>为什么需要PD分离？<a href=#为什么需要pd分离 class=hash-link aria-label="Direct link to 为什么需要PD分离？" title="Direct link to 为什么需要PD分离？">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=continuous-batching的挑战>Continuous Batching的挑战<a href=#continuous-batching的挑战 class=hash-link aria-label="Direct link to Continuous Batching的挑战" title="Direct link to Continuous Batching的挑战">​</a></h3>
<p>现代<code>LLM</code>推理系统广泛采用<code>Continuous Batching</code>技术来提高吞吐量。然而，<code>Prefill</code>和<code>Decode</code>阶段对批处理的响应特性截然不同：</p>
<p><img decoding=async loading=lazy alt="alt text" src=/assets/images/image-590ab5cac16623be593676fc19a8039f.png width=1192 height=680 class=img_ev3q></p>
<ul>
<li><strong>Prefill阶段</strong>：由于是计算密集型，随着<code>Batch Size</code>的增加，受算力限制，吞吐量增长趋势逐渐平缓</li>
<li><strong>Decode阶段</strong>：由于是内存带宽密集型，随着<code>Batch Size</code>的增加，吞吐量增长趋势越来越显著</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=传统架构的性能瓶颈>传统架构的性能瓶颈<a href=#传统架构的性能瓶颈 class=hash-link aria-label="Direct link to 传统架构的性能瓶颈" title="Direct link to 传统架构的性能瓶颈">​</a></h3>
<p>在传统的一体化部署模式中，当Prefill和Decode在同一设备上执行时，会出现严重的资源竞争问题：</p>
<p><img decoding=async loading=lazy alt="alt text" src=/assets/images/image-2-439c5e9456377be1ced9bd4b4893624f.png width=833 height=322 class=img_ev3q></p>
<p>如上图所示，当新的请求（<code>request5</code>或<code>request6</code>）到达时，系统会优先处理新请求的<code>Prefill</code>阶段，这会直接影响正在进行的<code>Decode</code>任务（<code>request2/3/4</code>），导致：</p>
<ol>
<li><strong>TBT不稳定</strong>：正在生成<code>token</code>的请求被打断，响应时延增加</li>
<li><strong>用户体验下降</strong>：<code>token</code>生成的连贯性被破坏</li>
<li><strong>资源利用率低</strong>：两个阶段的资源需求特性无法得到针对性优化</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=pd分离的解决方案>PD分离的解决方案<a href=#pd分离的解决方案 class=hash-link aria-label="Direct link to PD分离的解决方案" title="Direct link to PD分离的解决方案">​</a></h3>
<p>为了解决上述问题，<code>PD</code>分离架构将<code>Prefill</code>和<code>Decode</code>部署在不同规格的集群中：</p>
<p><img decoding=async loading=lazy alt="alt text" src=/assets/images/image-1-c356caba5b0f52a9fc8483b13b07ecd9.png width=841 height=431 class=img_ev3q></p>
<p>通过这种分离部署方案，配合智能的任务调度策略，可以在满足<code>TTFT</code>和<code>TBT</code>指标的前提下，结合<code>Continuous Batching</code>机制最大化<code>Decode</code>阶段的并发处理能力，从而在提供更好用户体验的同时，显著提升算力利用率。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=pd分离的核心原理>PD分离的核心原理<a href=#pd分离的核心原理 class=hash-link aria-label="Direct link to PD分离的核心原理" title="Direct link to PD分离的核心原理">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=传统架构的局限性>传统架构的局限性<a href=#传统架构的局限性 class=hash-link aria-label="Direct link to 传统架构的局限性" title="Direct link to 传统架构的局限性">​</a></h3>
<p>在传统<code>LLM</code>推理系统中，<code>Prefill</code>和<code>Decode</code>阶段通常在同一计算设备上顺序执行。这种架构虽然实现简单，但存在根本性的效率问题：</p>
<ol>
<li><strong>资源需求不匹配</strong>：<code>Prefill</code>阶段需要大量并行计算能力，而<code>Decode</code>阶段更依赖高带宽内存访问</li>
<li><strong>相互干扰</strong>：两个阶段共享同一计算资源时，它们的不同需求特性会相互干扰</li>
<li><strong>优化困难</strong>：难以为每个阶段选择最优的配置和优化策略</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=pd分离的技术思路>PD分离的技术思路<a href=#pd分离的技术思路 class=hash-link aria-label="Direct link to PD分离的技术思路" title="Direct link to PD分离的技术思路">​</a></h3>
<p><code>PD</code>分离技术的核心思想是<strong>解耦和专门化</strong>：</p>
<ol>
<li><strong>阶段解耦</strong>：将<code>Prefill</code>和<code>Decode</code>这两个阶段从逻辑和物理上完全分离</li>
<li><strong>设备专门化</strong>：为每个阶段选择最适合其特性的硬件配置</li>
<li><strong>优化独立化</strong>：为每个阶段采用最优的并行策略和优化技术</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=具体实现架构>具体实现架构<a href=#具体实现架构 class=hash-link aria-label="Direct link to 具体实现架构" title="Direct link to 具体实现架构">​</a></h3>
<p>在<code>PD</code>分离架构中：</p>
<ul>
<li><strong>Prefill集群</strong>：部署在高算力GPU（如<code>A100</code>、<code>H100</code>）上，充分利用其强大的并行计算能力，专注于快速处理输入序列</li>
<li><strong>Decode集群</strong>：部署在大显存、高内存带宽的GPU上，专注于高效的<code>token</code>生成和<code>KV Cache</code>管理</li>
<li><strong>网络互连</strong>：两个集群通过高速网络（如<code>NVLink</code>或<code>RDMA</code>）传输中间状态，主要是<code>KV Cache</code>数据</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=关键技术挑战>关键技术挑战<a href=#关键技术挑战 class=hash-link aria-label="Direct link to 关键技术挑战" title="Direct link to 关键技术挑战">​</a></h3>
<p><code>PD</code>分离系统需要解决几个核心技术问题：</p>
<ol>
<li><strong>高效数据传输</strong>：如何在<code>Prefill</code>和<code>Decode</code>节点间高效传输大量的<code>KV Cache</code>数据</li>
<li><strong>智能调度策略</strong>：如何设计调度算法确保请求在不同阶段间的平滑流转</li>
<li><strong>并行策略优化</strong>：如何为每个阶段选择最优的张量并行、流水线并行等策略</li>
<li><strong>状态一致性</strong>：如何保证分布式环境下<code>KV Cache</code>的一致性和正确性</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=技术发展现状>技术发展现状<a href=#技术发展现状 class=hash-link aria-label="Direct link to 技术发展现状" title="Direct link to 技术发展现状">​</a></h3>
<p>现代<code>PD</code>分离系统（如<code>DistServe</code>、<code>Mooncake</code>等）通过以下创新技术已经成功解决了这些挑战：</p>
<ul>
<li><strong>压缩传输算法</strong>：减少<code>KV Cache</code>传输开销</li>
<li><strong>预测调度策略</strong>：基于负载预测的智能任务分配</li>
<li><strong>异步处理机制</strong>：<code>overlap</code>计算和通信操作</li>
<li><strong>动态负载均衡</strong>：根据实时负载调整资源分配</li>
</ul>
<p>这些技术创新使得<code>PD</code>分离架构能够将额外开销控制在可接受范围内，同时实现显著的性能提升。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=pd分离带来的优势>PD分离带来的优势<a href=#pd分离带来的优势 class=hash-link aria-label="Direct link to PD分离带来的优势" title="Direct link to PD分离带来的优势">​</a></h2>
<p>通过将<code>Prefill</code>和<code>Decode</code>阶段分离部署，这种架构在多个方面带来了显著的改进，特别是在处理长上下文（<code>long context</code>）场景时，其优势更加明显。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=资源分配与利用的优化>资源分配与利用的优化<a href=#资源分配与利用的优化 class=hash-link aria-label="Direct link to 资源分配与利用的优化" title="Direct link to 资源分配与利用的优化">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=异构设备的充分利用>异构设备的充分利用<a href=#异构设备的充分利用 class=hash-link aria-label="Direct link to 异构设备的充分利用" title="Direct link to 异构设备的充分利用">​</a></h4>
<ul>
<li>
<p><strong>成本效益最大化</strong>：<code>Prefill</code>阶段计算密集，适合部署在高算力<code>GPU</code>（如<code>A100</code>、<code>H100</code>等高端计算卡）上；而<code>Decode</code>阶段显存密集，可以采用低算力但大显存的<code>GPU</code>（如大内存的<code>L40</code>等）。这种差异化配置能够显著降低硬件总成本，同时提高每种设备的利用率。</p>
</li>
<li>
<p><strong>弹性资源管理</strong>：可以根据实际负载情况独立地为<code>Prefill</code>和<code>Decode</code>集群进行扩缩容。在业务高峰期，可以针对性地为瓶颈阶段分配更多资源，而不必为整个系统进行等比例扩容，大大提高了系统的弹性和成本效益。</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=性能指标的全面提升>性能指标的全面提升<a href=#性能指标的全面提升 class=hash-link aria-label="Direct link to 性能指标的全面提升" title="Direct link to 性能指标的全面提升">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=多指标同步优化>多指标同步优化<a href=#多指标同步优化 class=hash-link aria-label="Direct link to 多指标同步优化" title="Direct link to 多指标同步优化">​</a></h4>
<ul>
<li>
<p><strong>避免性能权衡</strong>：在传统架构中，优化<code>TTFT</code>往往会影响<code>TPOT</code>，反之亦然。<code>PD</code>分离允许在<code>Prefill</code>阶段限制<code>Batch Size</code>以减少<code>TTFT</code>，同时在<code>Decode</code>阶段增大<code>Batch Size</code>以提高并发处理能力。这种策略能够同时改善所有关键性能指标，而不需要在不同指标之间做权衡。</p>
</li>
<li>
<p><strong>稳定的用户体验</strong>：通过分离部署，新请求的<code>Prefill</code>计算不会占用<code>Decode</code>阶段的资源，从而保证了稳定的<code>TBT</code>，为用户提供更加流畅和一致的交互体验。</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=技术实现的灵活性>技术实现的灵活性<a href=#技术实现的灵活性 class=hash-link aria-label="Direct link to 技术实现的灵活性" title="Direct link to 技术实现的灵活性">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=独立优化策略>独立优化策略<a href=#独立优化策略 class=hash-link aria-label="Direct link to 独立优化策略" title="Direct link to 独立优化策略">​</a></h4>
<ul>
<li>
<p><strong>阶段特化优化</strong>：可以为<code>Prefill</code>和<code>Decode</code>阶段分别采用最适合的模型优化技术。例如，在<code>Prefill</code>阶段采用量化、矩阵分解等计算优化技术，而在<code>Decode</code>阶段专注于<code>Continuous Batching</code>和<code>KV Cache</code>管理优化。</p>
</li>
<li>
<p><strong>硬件生态扩展</strong>：这种分离架构为使用不同类型的硬件加速器打开了可能性。可以在<code>Prefill</code>阶段使用传统<code>GPU</code>，而在<code>Decode</code>阶段尝试使用专用的推理芯片或其他新兴硬件，进一步降低成本并提高效率。</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=系统可靠性与可维护性提升>系统可靠性与可维护性提升<a href=#系统可靠性与可维护性提升 class=hash-link aria-label="Direct link to 系统可靠性与可维护性提升" title="Direct link to 系统可靠性与可维护性提升">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=故障隔离与高可用>故障隔离与高可用<a href=#故障隔离与高可用 class=hash-link aria-label="Direct link to 故障隔离与高可用" title="Direct link to 故障隔离与高可用">​</a></h4>
<ul>
<li>
<p><strong>单点故障消除</strong>：当<code>Prefill</code>或<code>Decode</code>集群中的某个节点出现故障时，不会直接影响另一阶段的正常运行，显著提高了系统的整体可靠性和可用性。</p>
</li>
<li>
<p><strong>运维灵活性</strong>：可以独立地对<code>Prefill</code>或<code>Decode</code>集群进行版本升级、配置调整或维护操作，大大减少了系统维护对线上服务的影响，提高了运维效率。</p>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=扩展性与未来适应性>扩展性与未来适应性<a href=#扩展性与未来适应性 class=hash-link aria-label="Direct link to 扩展性与未来适应性" title="Direct link to 扩展性与未来适应性">​</a></h4>
<ul>
<li>
<p><strong>技术演进适应</strong>：随着新的硬件技术和算法优化的出现，可以独立地在某一阶段应用新技术，而不需要重新设计整个系统架构。</p>
</li>
<li>
<p><strong>业务场景适配</strong>：不同的业务场景对<code>Prefill</code>和<code>Decode</code>的性能要求不同，分离架构允许根据具体业务需求灵活调整两个阶段的资源配比和优化策略。</p>
</li>
</ul>
<p>通过这些全方位的优势，<code>PD</code>分离架构不仅解决了传统<code>LLM</code>推理系统的性能瓶颈问题，还为大模型的高效部署和应用提供了一个更加灵活、可扩展的技术方案。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=参考资料>参考资料<a href=#参考资料 class=hash-link aria-label="Direct link to 参考资料" title="Direct link to 参考资料">​</a></h2>
<ul>
<li><a href=https://www.eet-china.com/mp/a412848.html target=_blank rel="noopener noreferrer">https://www.eet-china.com/mp/a412848.html</a></li>
</ul></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/ai/distributed-inference><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>分布式推理</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/ai/nvidia-dynamo><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>NVIDIA Dynamo: 分布式AI推理的高效引擎</div></a></nav><div class=docusaurus-mt-lg></div></div></div><div class="col col--3"><div class=tocContainer_PXzm><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#引言 class="table-of-contents__link toc-highlight">引言</a><li><a href=#llm推理基础认识prefill和decode class="table-of-contents__link toc-highlight">LLM推理基础：认识Prefill和Decode</a><ul><li><a href=#推理过程概述 class="table-of-contents__link toc-highlight">推理过程概述</a><li><a href=#性能评估指标 class="table-of-contents__link toc-highlight">性能评估指标</a><li><a href=#两阶段特性深度对比 class="table-of-contents__link toc-highlight">两阶段特性深度对比</a></ul><li><a href=#为什么需要pd分离 class="table-of-contents__link toc-highlight">为什么需要PD分离？</a><ul><li><a href=#continuous-batching的挑战 class="table-of-contents__link toc-highlight">Continuous Batching的挑战</a><li><a href=#传统架构的性能瓶颈 class="table-of-contents__link toc-highlight">传统架构的性能瓶颈</a><li><a href=#pd分离的解决方案 class="table-of-contents__link toc-highlight">PD分离的解决方案</a></ul><li><a href=#pd分离的核心原理 class="table-of-contents__link toc-highlight">PD分离的核心原理</a><ul><li><a href=#传统架构的局限性 class="table-of-contents__link toc-highlight">传统架构的局限性</a><li><a href=#pd分离的技术思路 class="table-of-contents__link toc-highlight">PD分离的技术思路</a><li><a href=#具体实�现架构 class="table-of-contents__link toc-highlight">具体实现架构</a><li><a href=#关键技术挑战 class="table-of-contents__link toc-highlight">关键技术挑战</a><li><a href=#技术发展现状 class="table-of-contents__link toc-highlight">技术发展现状</a></ul><li><a href=#pd分离带来的优势 class="table-of-contents__link toc-highlight">PD分离带来的优势</a><ul><li><a href=#资源分配与利用的优化 class="table-of-contents__link toc-highlight">资源分配与利用的优化</a><li><a href=#性能指标的全面提升 class="table-of-contents__link toc-highlight">性能指标的全面提升</a><li><a href=#技术实现的灵活性 class="table-of-contents__link toc-highlight">技术实现的灵活性</a><li><a href=#系统可靠性与可维护性提升 class="table-of-contents__link toc-highlight">系统可靠性与可维护性提升</a></ul><li><a href=#参考资料 class="table-of-contents__link toc-highlight">参考资料</a></ul></div><div class=tocAdBanner_imxD></div></div></div></div></div></main></div></div></div><footer class=footer><div class="container container-fluid"><div class="footer__bottom text--center"><div class=footer__copyright>Copyright 2025 johng.cn</div></div></div></footer></div>