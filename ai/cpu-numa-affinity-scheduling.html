<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-docs/AI技术/AI基础架构/算力调度/CPU亲和性与NUMA亲和性调度" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.6.3"><title data-rh=true>CPU亲和性与NUMA亲和性调度 | John's Blog</title><meta data-rh=true name=viewport content="width=device-width,initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:url content=https://johng.cn/ai/cpu-numa-affinity-scheduling><meta data-rh=true property=og:locale content=en><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=author content="John Guo"><meta data-rh=true property=og:image content=https://johng.cn/img/favicon.png><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-default-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-default-current><meta data-rh=true property=og:title content="CPU亲和性与NUMA亲和性调度 | John's Blog"><meta data-rh=true name=description content=深入探讨AI模型开发训练推理场景下的CPU亲和性与NUMA亲和性调度技术，介绍NUMA架构原理、GPU拓扑分析、Docker和Kubernetes环境下的亲和性配置方案，通过实际案例优化AI工作负载的性能表现><meta data-rh=true property=og:description content=深入探讨AI模型开发训练推理场景下的CPU亲和性与NUMA亲和性调度技术，介绍NUMA架构原理、GPU拓扑分析、Docker和Kubernetes环境下的亲和性配置方案，通过实际案例优化AI工作负载的性能表现><meta data-rh=true name=keywords content="CPU亲和性,NUMA亲和性,NUMA架构,GPU调度,AI训练,AI推理,性能优化,Docker亲和性,Kubernetes亲和性,CPU绑核,内存访问优化,跨NUMA节点,nvidia-smi,GPU拓扑,分布式训练,NVLINK,资源调度,Kubernetes Topology Manager,CPU Manager,静态策略"><link data-rh=true rel=icon href=/img/favicon.ico><link data-rh=true rel=canonical href=https://johng.cn/ai/cpu-numa-affinity-scheduling><link data-rh=true rel=alternate href=https://johng.cn/ai/cpu-numa-affinity-scheduling hreflang=en><link data-rh=true rel=alternate href=https://johng.cn/ai/cpu-numa-affinity-scheduling hreflang=x-default><link data-rh=true rel=preconnect href=https://XGS1CPQERK-dsn.algolia.net crossorigin><link rel=alternate type=application/rss+xml href=/blog/rss.xml title="John's Blog RSS Feed"><link rel=alternate type=application/atom+xml href=/blog/atom.xml title="John's Blog Atom Feed"><link rel=search type=application/opensearchdescription+xml title="John's Blog" href=/opensearch.xml><script src=https://hm.baidu.com/hm.js?6b4ae23dc83ee5efe875b7172af6c7c1 async></script><link rel=stylesheet href=/assets/css/styles.2f56d3c4.css><script src=/assets/js/runtime~main.672256aa.js defer></script><script src=/assets/js/main.0ff0c585.js defer></script><body class=navigation-with-keyboard><script>!function(){var t,e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t=null!==e?e:"light",document.documentElement.setAttribute("data-theme",t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/><b class="navbar__title text--truncate">John's Blog</b></a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/ai>AI技术</a><a class="navbar__item navbar__link" href=/cloud-native>云原生</a><a class="navbar__item navbar__link" href=/notes>日常笔记</a><a class="navbar__item navbar__link" href=/programming>开发语言</a><a class="navbar__item navbar__link" href=/architecture>技术架构</a><a class="navbar__item navbar__link" href=/observability>可观测性</a><a class="navbar__item navbar__link" href=/life>生活笔记</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href=/aboutme>关于我</a><a class="navbar__item navbar__link" href=/blog>博客</a><a href=https://goframe.org/ target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-goframe-link"></a><a href=https://github.com/gqcn target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-github-link"></a><div class=navbarSearchContainer_Bca1><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_z2l0"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class=sidebar_njMd><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/ai>AI技术</a><button aria-label="Collapse sidebar category 'AI技术'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/app>AI应用技术</a><button aria-label="Expand sidebar category 'AI应用技术'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" tabindex=0 href=/ai/infra>AI基础架构</a><button aria-label="Collapse sidebar category 'AI基础架构'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/vgpu>vGPU</a><button aria-label="Expand sidebar category 'vGPU'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/rdma>RDMA</a><button aria-label="Expand sidebar category 'RDMA'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/nvidia>NVIDIA</a><button aria-label="Expand sidebar category 'NVIDIA'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/training>开发训练</a><button aria-label="Expand sidebar category '开发训练'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/inference>推理服务</a><button aria-label="Expand sidebar category '推理服务'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" tabindex=0 href=/ai/scheduling>算力调度</a><button aria-label="Collapse sidebar category '算力调度'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class=menu__link tabindex=0 href=/ai/hybrid-scheduling-affinity-toleration>混合调度的亲和性、污点容忍设计思考</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class=menu__link tabindex=0 href=/ai/hybrid-scheduling-scale-down-control>混部调度中如何控制在线服务的缩容逻辑</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/ai/cpu-numa-affinity-scheduling>CPU亲和性与NUMA亲和性调度</a></ul><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/ai/what-is-llmops-mlops>LLMOps介绍</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/ai/common-acceleration-cards>常见智算加速卡汇总</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/ai/common-ai-model-training-inference-framework>常见AI模型训练推理框架对比</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/ai/ai-training-inference-scenarios>AI模型训练推理常见业务场景痛点</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/basic>AI入门知识</a><button aria-label="Expand sidebar category 'AI入门知识'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/cloud-native>云原生</a><button aria-label="Expand sidebar category '云原生'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/notes>日常笔记</a><button aria-label="Expand sidebar category '日常笔记'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/programming>开发语言</a><button aria-label="Expand sidebar category '开发语言'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/architecture>技术架构</a><button aria-label="Expand sidebar category '技术架构'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/management>技术管理</a><button aria-label="Expand sidebar category '技术管理'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/observability>可观测性</a><button aria-label="Expand sidebar category '可观测性'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/database-and-middleware>数据库与中间件</a><button aria-label="Expand sidebar category '数据库与中间件'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/operating-systems-and-networks>操作系统和网络</a><button aria-label="Expand sidebar category '操作系统和网络'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/life>生活笔记</a><button aria-label="Expand sidebar category '生活笔记'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div></ul></nav></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/ai><span itemprop=name>AI技术</span></a><meta itemprop=position content=1><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/ai/infra><span itemprop=name>AI基础架构</span></a><meta itemprop=position content=2><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/ai/scheduling><span itemprop=name>算力调度</span></a><meta itemprop=position content=3><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link itemprop=name>CPU亲和性与NUMA亲和性调度</span><meta itemprop=position content=4></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><div class="theme-admonition theme-admonition-danger admonition_xJq3 alert alert--danger"><div class=admonitionHeading_Gvgb><span class=admonitionIcon_Rf37><svg viewBox="0 0 12 16"><path fill-rule=evenodd d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"/></svg></span>注意</div><div class=admonitionContent_BuS1><p>本文的内容还在编写中，示例代码还未进行严格测试。</div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=引言>引言<a href=#引言 class=hash-link aria-label="Direct link to 引言" title="Direct link to 引言">​</a></h2>
<p>在<code>AI</code>模型开发、训练和推理场景中，计算密集型任务对硬件资源的高效利用提出了极高要求。除了<code>GPU</code>算力本身，<code>CPU</code>和内存的访问效率同样是影响整体性能的关键因素。不合理的<code>CPU</code>与内存分配可能导致跨<code>NUMA</code>节点的内存访问、频繁的进程迁移等问题，从而显著降低系统性能。</p>
<p><code>CPU</code>亲和性（<code>CPU Affinity</code>）和<code>NUMA</code>亲和性（<code>NUMA Affinity</code>）调度技术通过精确控制进程与<code>CPU</code>核心、内存节点的绑定关系，最大化利用硬件拓扑结构，减少资源访问延迟，提升<code>AI</code>工作负载的性能表现。本文将深入介绍这些核心技术的原理、应用场景及在<code>Docker</code>和<code>Kubernetes</code>环境下的具体实现方法。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=numa架构简介>NUMA架构简介<a href=#numa架构简介 class=hash-link aria-label="Direct link to NUMA架构简介" title="Direct link to NUMA架构简介">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=什么是numa>什么是NUMA<a href=#什么是numa class=hash-link aria-label="Direct link to 什么是NUMA" title="Direct link to 什么是NUMA">​</a></h3>
<p><code>NUMA</code>（<code>Non-Uniform Memory Access</code>，非统一内存访问）是现代多处理器系统采用的一种内存访问架构。在<code>NUMA</code>架构中，系统内存被划分为多个<code>NUMA</code>节点（<code>NUMA Node</code>），每个节点包含一组<code>CPU</code>核心和本地内存。</p>
<p>与传统的<code>UMA</code>（<code>Uniform Memory Access</code>，统一内存访问）架构相比，<code>NUMA</code>架构具有以下特点：</p>
<ul>
<li><strong>本地访问优化</strong>：<code>CPU</code>访问本地<code>NUMA</code>节点的内存速度最快</li>
<li><strong>远程访问代价</strong>：<code>CPU</code>访问其他<code>NUMA</code>节点的内存（跨节点访问）会产生额外延迟</li>
<li><strong>可扩展性强</strong>：通过增加<code>NUMA</code>节点可以线性扩展系统规模</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=numa架构的层级结构>NUMA架构的层级结构<a href=#numa架构的层级结构 class=hash-link aria-label="Direct link to NUMA架构的层级结构" title="Direct link to NUMA架构的层级结构">​</a></h3>
<p>例如一个典型的<code>NUMA</code>系统具有以下层级结构：</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-text codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token plain">┌─────────────────────────────────────────────────────────────┐</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">│                        NUMA 系统                              │</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">├──────────────────────────┬──────────────────────────────────┤</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">│     NUMA Node 0          │        NUMA Node 1               │</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">├──────────────────────────┼──────────────────────────────────┤</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">│  CPU 0-31, 64-95         │     CPU 32-63, 96-127            │</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">│  Local Memory            │     Local Memory                 │</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">│  PCIe Devices (GPU0-3)   │     PCIe Devices (GPU4-7)        │</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">└──────────────────────────┴──────────────────────────────────┘</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<p>在上述架构中：</p>
<ul>
<li>每个<code>NUMA</code>节点包含一组<code>CPU</code>核心</li>
<li>每个<code>NUMA</code>节点有自己的本地内存</li>
<li><code>PCIe</code>设备（如<code>GPU</code>）也归属于特定的<code>NUMA</code>节点</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=numa对性能的影响>NUMA对性能的影响<a href=#numa对性能的影响 class=hash-link aria-label="Direct link to NUMA对性能的影响" title="Direct link to NUMA对性能的影响">​</a></h3>
<p>在<code>AI</code>训练和推理场景中，<code>NUMA</code>架构对性能的影响主要体现在：</p>
<ol>
<li>
<p><strong>内存访问延迟</strong>：本地内存访问延迟通常为<code>100-200ns</code>，而跨<code>NUMA</code>节点访问延迟可达<code>300-500ns</code>，差距达<code>2-3</code>倍</p>
</li>
<li>
<p><strong>内存带宽</strong>：跨节点访问会占用<code>NUMA</code>互连总线，降低整体内存带宽</p>
</li>
<li>
<p><strong>PCIe通信延迟</strong>：</p>
<ul>
<li><code>GPU</code>物理连接在特定<code>NUMA</code>节点的<code>PCIe</code>总线上</li>
<li><code>CPU</code>访问非本地<code>NUMA</code>节点的<code>GPU</code>需要经过<code>NUMA</code>互联（<code>QPI</code>/<code>UPI</code>）</li>
<li>跨<code>NUMA</code>的<code>PCIe</code>访问会增加<code>100-200ns</code>额外延迟</li>
<li><code>DMA(Direct Memory Access)</code>传输带宽可能降低<code>50%</code>以上</li>
</ul>
</li>
<li>
<p><strong>GPU-CPU通信效率</strong>：</p>
<ul>
<li><code>GPU kernel</code>启动、状态查询、寄存器访问等<code>PCIe</code>事务受跨<code>NUMA</code>影响</li>
<li>频繁的<code>CPU-GPU</code>交互会放大跨<code>NUMA</code>开销</li>
<li>影响整体<code>GPU</code>利用率和吞吐量</li>
</ul>
</li>
<li>
<p><strong>数据传输效率</strong>：跨<code>NUMA</code>节点的数据传输会影响<code>GPU</code>与<code>CPU</code>之间的数据交换效率，包括：</p>
<ul>
<li>主机到设备（<code>Host-to-Device</code>）传输</li>
<li>设备到主机（<code>Device-to-Host</code>）传输</li>
<li>统一内存（<code>Unified Memory</code>）的页面迁移</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=cpunuma亲和性的重要性>CPU&NUMA亲和性的重要性<a href=#cpunuma亲和性的重要性 class=hash-link aria-label="Direct link to CPU&NUMA亲和性的重要性" title="Direct link to CPU&NUMA亲和性的重要性">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=cpu亲和性的作用>CPU亲和性的作用<a href=#cpu亲和性的作用 class=hash-link aria-label="Direct link to CPU亲和性的作用" title="Direct link to CPU亲和性的作用">​</a></h3>
<p><code>CPU</code>亲和性（<code>CPU Affinity</code>）是指将进程或线程绑定到特定的<code>CPU</code>核心上运行。在<code>AI</code>工作负载中，合理的<code>CPU</code>亲和性配置可以：</p>
<ol>
<li><strong>减少上下文切换</strong>：避免进程在不同<code>CPU</code>核心间频繁迁移，减少上下文切换开销</li>
<li><strong>提升缓存命中率</strong>：进程固定在特定核心运行，可以更好地利用<code>CPU</code>的<code>L1</code>、<code>L2</code>、<code>L3</code>缓存</li>
<li><strong>降低延迟抖动</strong>：避免进程迁移带来的性能波动，提供更稳定的延迟表现</li>
<li><strong>避免资源竞争</strong>：将不同任务绑定到不同的核心，减少<code>CPU</code>资源争抢</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=numa亲和性的作用>NUMA亲和性的作用<a href=#numa亲和性的作用 class=hash-link aria-label="Direct link to NUMA亲和性的作用" title="Direct link to NUMA亲和性的作用">​</a></h3>
<p><code>NUMA</code>亲和性（<code>NUMA Affinity</code>）是指将进程、内存分配和<code>I/O</code>设备绑定到同一<code>NUMA</code>节点，以<strong>优化内存访问性能</strong>。在<code>AI</code>场景中的价值包括：</p>
<ol>
<li><strong>降低内存访问延迟</strong>：确保进程访问本地内存，避免跨<code>NUMA</code>节点的远程内存访问</li>
<li><strong>提升内存带宽</strong>：本地内存访问可以充分利用<code>NUMA</code>节点的内存带宽</li>
<li><strong>优化GPU-CPU通信</strong>：将<code>GPU</code>、对应的<code>CPU</code>核心和内存绑定到同一<code>NUMA</code>节点，最小化数据传输延迟</li>
<li><strong>提高整体吞吐量</strong>：通过减少跨节点通信，提升系统整体的数据处理能力</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=ai模型训练推理场景中的影响>AI模型训练推理场景中的影响<a href=#ai模型训练推理场景中的影响 class=hash-link aria-label="Direct link to AI模型训练推理场景中的影响" title="Direct link to AI模型训练推理场景中的影响">​</a></h3>
<p>在<code>AI</code>模型训练和推理场景中，不合理的亲和性配置可能导致：</p>
<ul>
<li><strong>训练速度下降</strong>：数据加载、预处理过程中的跨<code>NUMA</code>访问会拖累整体训练速度</li>
<li><strong>推理延迟增加</strong>：推理服务中的请求处理涉及频繁的<code>CPU-GPU</code>交互，跨<code>NUMA</code>访问会增加延迟</li>
<li><strong>吞吐量降低</strong>：在高并发推理场景下，跨<code>NUMA</code>访问会成为系统瓶颈</li>
<li><strong>性能不稳定</strong>：进程迁移和非本地内存访问会导致性能抖动</li>
</ul>
<p>通过合理配置<code>CPU</code>亲和性和<code>NUMA</code>亲和性，通常可以获得<code>10%-30%</code>的性能提升，在某些场景下甚至可以达到<code>50%</code>以上的优化效果。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=cpunuma亲和性示例分析>CPU&NUMA亲和性示例分析<a href=#cpunuma亲和性示例分析 class=hash-link aria-label="Direct link to CPU&NUMA亲和性示例分析" title="Direct link to CPU&NUMA亲和性示例分析">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=理解nvidia-smi-topo命令输出>理解nvidia-smi topo命令输出<a href=#理解nvidia-smi-topo命令输出 class=hash-link aria-label="Direct link to 理解nvidia-smi topo命令输出" title="Direct link to 理解nvidia-smi topo命令输出">​</a></h3>
<p><code>NVIDIA</code>提供了<code>nvidia-smi topo -m</code>命令来查看<code>GPU</code>之间以及<code>GPU</code>与<code>CPU</code>之间的拓扑关系。让我们详细分析提供的拓扑信息：</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-text codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token plain">        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU0     X      PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-31,64-95      0               N/A</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU1    PIX      X      NODE    NODE    SYS     SYS     SYS     SYS     0-31,64-95      0               N/A</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU2    NODE    NODE     X      PIX     SYS     SYS     SYS     SYS     0-31,64-95      0               N/A</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU3    NODE    NODE    PIX      X      SYS     SYS     SYS     SYS     0-31,64-95      0               N/A</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU4    SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    32-63,96-127    1               N/A</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU5    SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    32-63,96-127    1               N/A</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      PIX     32-63,96-127    1               N/A</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU7    SYS     SYS     SYS     SYS     NODE    NODE    PIX      X      32-63,96-127    1               N/A</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=连接类型说明>连接类型说明<a href=#连接类型说明 class=hash-link aria-label="Direct link to 连接类型说明" title="Direct link to 连接类型说明">​</a></h3>
<p>矩阵中的符号表示<code>GPU</code>之间的连接类型，按照性能从高到低排列：</p>
<table><thead><tr><th>连接类型<th>说明<th>典型带宽<th>适用场景<tbody><tr><td><code>X</code><td>自身<td><code>N/A</code><td>同一<code>GPU</code><tr><td><code>NV#</code><td><code>NVLink</code>连接（#表示链接数量）<td><code>300-900 GB/s</code><td>高性能<code>GPU</code>间通信（如<code>NVLink 3.0/4.0</code>）<tr><td><code>PIX</code><td>同一<code>PCIe</code>交换机<br>可能包含<code>NVLink Bridge</code><td><code>PCIe: 16-64 GB/s</code><br><code>NVLink Bridge: 100-200 GB/s</code><td>同<code>PCIe</code>树<code>GPU</code>通信<br>部分消费级<code>GPU</code>使用<code>NVLink Bridge</code>互联<tr><td><code>NODE</code><td>同一<code>NUMA</code>节点，不同<code>PCIe</code>交换机<td><code>16-32 GB/s</code><td>同<code>NUMA</code>节点<code>GPU</code>跨<code>PCIe</code>交换机通信<tr><td><code>SYS</code><td>跨<code>NUMA</code>节点，通过<code>CPU</code>互联<td><code>8-16 GB/s</code><td>跨<code>NUMA</code>节点<code>GPU</code>通信，性能最低</table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=cpu-affinity字段解析>CPU Affinity字段解析<a href=#cpu-affinity字段解析 class=hash-link aria-label="Direct link to CPU Affinity字段解析" title="Direct link to CPU Affinity字段解析">​</a></h3>
<p><code>CPU Affinity</code>列显示了每个<code>GPU</code>关联的最优<code>CPU</code>核心范围：</p>
<ul>
<li>
<p><strong>GPU0-GPU3</strong>: <code>0-31, 64-95</code></p>
<ul>
<li>物理核心：<code>0-31</code></li>
<li>超线程核心：<code>64-95</code></li>
<li>总共<code>32</code>个物理核心，<code>64</code>个逻辑核心（启用超线程）</li>
</ul>
</li>
<li>
<p><strong>GPU4-GPU7</strong>: <code>32-63, 96-127</code></p>
<ul>
<li>物理核心：<code>32-63</code></li>
<li>超线程核心：<code>96-127</code></li>
<li>总共<code>32</code>个物理核心，<code>64</code>个逻辑核心（启用超线程）</li>
</ul>
</li>
</ul>
<p><strong>关键理解</strong>：当进程需要使用特定<code>GPU</code>时，应将其绑定到该<code>GPU</code>的<code>CPU Affinity</code>范围内的核心，以实现最优性能。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=numa-affinity字段解析>NUMA Affinity字段解析<a href=#numa-affinity字段解析 class=hash-link aria-label="Direct link to NUMA Affinity字段解析" title="Direct link to NUMA Affinity字段解析">​</a></h3>
<p><code>NUMA Affinity</code>列显示了每个<code>GPU</code>所属的<code>NUMA</code>节点：</p>
<ul>
<li><strong>GPU0-GPU3</strong>: 属于<code>NUMA Node 0</code></li>
<li><strong>GPU4-GPU7</strong>: 属于<code>NUMA Node 1</code></li>
</ul>
<p>这意味着：</p>
<ul>
<li>使用<code>GPU0-GPU3</code>的任务应绑定到<code>NUMA Node 0</code>的<code>CPU</code>核心和内存</li>
<li>使用<code>GPU4-GPU7</code>的任务应绑定到<code>NUMA Node 1</code>的<code>CPU</code>核心和内存</li>
</ul>
<p><strong>NUMA与PCIe拓扑的关系</strong>：</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-text codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token plain">NUMA Node 0                          NUMA Node 1</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">├── CPU 0-31, 64-95                  ├── CPU 32-63, 96-127</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">├── Local Memory                     ├── Local Memory</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">└── PCIe Root Complex                └── PCIe Root Complex</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    ├── PCIe Switch 0                    ├── PCIe Switch 2</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    │   ├── GPU0                         │   ├── GPU4</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    │   └── GPU1                         │   └── GPU5</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    └── PCIe Switch 1                    └── PCIe Switch 3</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        ├── GPU2                             ├── GPU6</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        └── GPU3                             └── GPU7</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<p><strong>关键理解</strong>：</p>
<ul>
<li>每个<code>GPU</code>物理连接到特定<code>NUMA</code>节点的<code>PCIe</code>总线</li>
<li><code>CPU</code>访问非本地<code>NUMA</code>节点的<code>GPU</code>需要经过<code>NUMA</code>互联（<code>QPI/UPI</code>）</li>
<li>跨<code>NUMA</code>的<code>PCIe</code>访问会显著增加延迟并降低带宽</li>
<li>最佳性能需要<code>CPU</code>、内存、<code>GPU</code>都在同一<code>NUMA</code>节点</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=拓扑分析总结>拓扑分析总结<a href=#拓扑分析总结 class=hash-link aria-label="Direct link to 拓扑分析总结" title="Direct link to 拓扑分析总结">​</a></h3>
<p>从上述拓扑信息可以得出以下结论：</p>
<ol>
<li>
<p><strong>GPU分组</strong>：</p>
<ul>
<li>组1：<code>GPU0-GPU3</code>（<code>NUMA Node 0</code>）</li>
<li>组2：<code>GPU4-GPU7</code>（<code>NUMA Node 1</code>）</li>
</ul>
</li>
<li>
<p><strong>GPU间连接模式</strong>：</p>
<ul>
<li><code>GPU0-GPU1</code>通过<code>PIX</code>连接（同<code>PCIe</code>交换机）</li>
<li><code>GPU2-GPU3</code>通过<code>PIX</code>连接（同<code>PCIe</code>交换机）</li>
<li><code>GPU0/1</code>与<code>GPU2/3</code>通过<code>NODE</code>连接（同<code>NUMA</code>节点，不同<code>PCIe</code>交换机）</li>
<li>跨<code>NUMA</code>节点的<code>GPU</code>通过<code>SYS</code>连接（性能最低）</li>
</ul>
</li>
<li>
<p><strong>最佳实践</strong>：</p>
<ul>
<li>单卡任务：使用任意一张<code>GPU</code>，绑定到对应的<code>CPU</code>核心和<code>NUMA</code>节点</li>
<li>2卡任务：优先选择<code>GPU0-GPU1</code>或<code>GPU2-GPU3</code>等<code>PIX</code>连接的<code>GPU</code>对</li>
<li>4卡任务：选择<code>GPU0-GPU3</code>或<code>GPU4-GPU7</code>（同<code>NUMA</code>节点，性能最优）</li>
<li>5卡任务：例如<code>GPU0-GPU4</code>，会跨<code>NUMA</code>节点，<code>GPU4</code>与<code>GPU0-3</code>通信性能较低（<code>SYS</code>连接）</li>
<li>跨<code>NUMA</code>任务：尽量选择4+4的组合（如<code>GPU0-3</code> + <code>GPU4-7</code>在不同节点），避免5卡这种不均衡配置</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=docker中的cpunuma亲和性配置>Docker中的CPU&NUMA亲和性配置<a href=#docker中的cpunuma亲和性配置 class=hash-link aria-label="Direct link to Docker中的CPU&NUMA亲和性配置" title="Direct link to Docker中的CPU&NUMA亲和性配置">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=docker的亲和性参数>Docker的亲和性参数<a href=#docker的亲和性参数 class=hash-link aria-label="Direct link to Docker的亲和性参数" title="Direct link to Docker的亲和性参数">​</a></h3>
<p><code>Docker</code>提供了多个参数来控制容器的<code>CPU</code>和<code>NUMA</code>亲和性：</p>
<table><thead><tr><th>参数<th>说明<th>示例<tbody><tr><td><code>--cpuset-cpus</code><td>指定容器可使用的<code>CPU</code>核心<td><code>--cpuset-cpus="0-31,64-95"</code><tr><td><code>--cpuset-mems</code><td>指定容器可使用的<code>NUMA</code>内存节点<td><code>--cpuset-mems="0"</code><tr><td><code>--cpus</code><td>限制容器可使用的<code>CPU</code>核心数量<td><code>--cpus="8.0"</code><tr><td><code>--cpu-shares</code><td>设置<code>CPU</code>权重（相对值）<td><code>--cpu-shares=1024</code></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=示例1单gpu训练任务>示例1：单GPU训练任务<a href=#示例1单gpu训练任务 class=hash-link aria-label="Direct link to 示例1：单GPU训练任务" title="Direct link to 示例1：单GPU训练任务">​</a></h3>
<p>假设我们要运行一个使用<code>GPU0</code>的训练任务：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token comment" style=color:#8292a2;font-style:italic># 查看GPU0的CPU亲和性和NUMA亲和性</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token comment" style=color:#8292a2;font-style:italic># GPU0: CPU Affinity: 0-31,64-95, NUMA Affinity: 0</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token function" style=color:#e6db74>docker</span><span class="token plain"> run </span><span class="token parameter variable" style=color:#f8f8f2>-d</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token parameter variable" style=color:#f8f8f2>--name</span><span class="token plain"> ai-training-gpu0 </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token parameter variable" style=color:#f8f8f2>--gpus</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>'"device=0"'</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  --cpuset-cpus</span><span class="token operator" style=color:#66d9ef>=</span><span class="token string" style=color:#a6e22e>"0-31,64-95"</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  --cpuset-mems</span><span class="token operator" style=color:#66d9ef>=</span><span class="token string" style=color:#a6e22e>"0"</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  --shm-size</span><span class="token operator" style=color:#66d9ef>=</span><span class="token plain">16g </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token parameter variable" style=color:#f8f8f2>-v</span><span class="token plain"> /data:/data </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  my-training-image:latest </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  python train.py </span><span class="token parameter variable" style=color:#f8f8f2>--gpu</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>0</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<p><strong>参数说明</strong>：</p>
<ul>
<li><code>--gpus '"device=0"'</code>：使用<code>GPU0</code></li>
<li><code>--cpuset-cpus="0-31,64-95"</code>：将容器绑定到<code>GPU0</code>的亲和<code>CPU</code>核心</li>
<li><code>--cpuset-mems="0"</code>：将容器的内存分配限制在<code>NUMA Node 0</code></li>
<li><code>--shm-size=16g</code>：设置共享内存大小（<code>AI</code>训练通常需要较大的共享内存）</li>
</ul>
<p><strong>为什么单GPU也需要设置CPU亲和性？</strong></p>
<p>虽然是单<code>GPU</code>任务，设置<code>CPU</code>和<code>NUMA</code>亲和性仍然非常重要：</p>
<ol>
<li>
<p><strong>优化CPU-GPU数据传输（内存维度）</strong>：<code>GPU0</code>连接在<code>NUMA Node 0</code>，即使进程运行在<code>Node 0</code>的<code>CPU</code>上，如果内存分配来自<code>NUMA Node 1</code>，数据从<code>CPU</code>内存传输到<code>GPU</code>时仍需要跨<code>NUMA</code>节点，延迟会增加<code>2-3</code>倍。</p>
</li>
<li>
<p><strong>优化CPU-GPU通信（PCIe维度）</strong>：<code>GPU</code>物理连接在特定<code>NUMA</code>节点的<code>PCIe</code>总线上（如<code>GPU0</code>连接在<code>Node 0</code>的<code>PCIe</code>），如果<code>CPU</code>进程运行在其他<code>NUMA</code>节点（如<code>Node 1</code>），<code>CPU</code>与<code>GPU</code>的控制命令、状态查询等<code>PCIe</code>通信都需要经过<code>NUMA</code>互联总线（<code>QPI/UPI</code>），增加额外延迟。</p>
</li>
<li>
<p><strong>提升数据预处理性能</strong>：训练任务通常需要大量<code>CPU</code>进行数据加载和预处理（如图像解码、数据增强），这些操作的内存访问如果跨<code>NUMA</code>会显著降低吞吐量。</p>
</li>
<li>
<p><strong>减少延迟抖动</strong>：即使<code>CPU</code>绑核，内存分配策略不当或<code>PCIe</code>访问跨<code>NUMA</code>仍可能导致性能不稳定。</p>
</li>
<li>
<p><strong>实际性能提升</strong>：根据测试，单<code>GPU</code>训练任务正确设置亲和性后，通常可获得<code>10%-20%</code>的性能提升。</p>
</li>
</ol>
<p><strong>重要理解：CPU亲和性 ≠ NUMA内存亲和性</strong></p>
<p>这是一个常见误解，需要特别说明：</p>
<ul>
<li><strong><code>--cpuset-cpus</code>参数</strong>：控制容器进程可以在<strong>哪些<code>CPU</code>核心上执行</strong>（进程调度层面）。</li>
<li><strong><code>--cpuset-mems</code>参数</strong>：控制容器进程的<strong>内存分配来自哪些<code>NUMA</code>节点</strong>（内存分配层面），同时也会影响 <strong><code>GPU</code>-内存之间的<code>PCIe DMA</code>传输效率</strong>（因为<code>GPU</code>物理连接在特定<code>NUMA</code>节点的<code>PCIe</code>总线上）。</li>
</ul>
<p><strong>为什么指定了CPU亲和性还需要指定NUMA亲和性？</strong></p>
<p>即使使用<code>--cpuset-cpus="0-31,64-95"</code>将进程限制在<code>NUMA Node 0</code>的<code>CPU</code>上运行，<strong>并不意味着内存也会自动从<code>NUMA Node 0</code>分配</strong>。原因如下：</p>
<ol>
<li>
<p><strong>Linux内核的内存分配策略</strong>：</p>
<ul>
<li>如果不指定<code>--cpuset-mems</code>，内核可能使用默认的内存策略（如<code>default</code>或<code>interleave</code>）</li>
<li>内核会根据内存压力从任何有可用内存的<code>NUMA</code>节点分配</li>
<li>当<code>NUMA Node 0</code>内存不足或碎片化时，会自动从<code>Node 1</code>分配</li>
</ul>
</li>
<li>
<p><strong>实际场景示例</strong>：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token comment" style=color:#8292a2;font-style:italic># 只指定CPU亲和性，不指定内存亲和性</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token function" style=color:#e6db74>docker</span><span class="token plain"> run --cpuset-cpus</span><span class="token operator" style=color:#66d9ef>=</span><span class="token string" style=color:#a6e22e>"0-31,64-95"</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>..</span><span class="token plain">.</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token comment" style=color:#8292a2;font-style:italic># 容器内查看内存分布</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">numastat </span><span class="token parameter variable" style=color:#f8f8f2>-p</span><span class="token plain"> </span><span class="token operator" style=color:#66d9ef>&lt;</span><span class="token plain">pid</span><span class="token operator" style=color:#66d9ef>></span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token comment" style=color:#8292a2;font-style:italic># 结果可能显示：</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token comment" style=color:#8292a2;font-style:italic># Node 0: 8GB</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token comment" style=color:#8292a2;font-style:italic># Node 1: 24GB  ← 大部分内存来自Node 1！</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain" style=display:inline-block></span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
</li>
<li>
<p><strong><code>PCIe</code>通信与<code>DMA</code>传输的影响</strong>：</p>
<ul>
<li><code>GPU</code>通过<code>PCIe</code>连接到特定<code>NUMA</code>节点（<code>GPU0-3</code>连接到<code>Node 0</code>的<code>PCIe Root Complex</code>）</li>
<li>如果<code>CPU</code>在<code>Node 0</code>但使用<code>Node 1</code>的<code>GPU</code>，或者<code>CPU</code>在<code>Node 1</code>但使用<code>Node 0</code>的<code>GPU</code></li>
<li><code>CPU</code>与<code>GPU</code>之间的<code>PCIe</code>事务（寄存器访问、中断处理）都需要跨<code>NUMA</code>互联</li>
<li><strong><code>GPU</code>-内存<code>DMA</code>传输同样受<code>NUMA</code>影响</strong>：如果<code>GPU</code>在<code>Node 0</code>但内存在<code>Node 1</code>，<code>GPU</code>读写内存时需要通过<code>QPI/UPI</code>跨<code>NUMA</code>访问，<code>DMA</code>带宽可能降低50%以上</li>
<li>跨<code>NUMA</code>的<code>PCIe</code>访问延迟增加，<code>DMA</code>传输效率降低，<code>GPU</code>利用率下降</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=示例2多gpu训练任务同numa节点>示例2：多GPU训练任务（同NUMA节点）<a href=#示例2多gpu训练任务同numa节点 class=hash-link aria-label="Direct link to 示例2：多GPU训练任务（同NUMA节点）" title="Direct link to 示例2：多GPU训练任务（同NUMA节点）">​</a></h3>
<p>使用<code>GPU0-GPU3</code>进行<code>4</code>卡训练：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token function" style=color:#e6db74>docker</span><span class="token plain"> run </span><span class="token parameter variable" style=color:#f8f8f2>-d</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token parameter variable" style=color:#f8f8f2>--name</span><span class="token plain"> ai-training-4gpu </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token parameter variable" style=color:#f8f8f2>--gpus</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>'"device=0,1,2,3"'</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  --cpuset-cpus</span><span class="token operator" style=color:#66d9ef>=</span><span class="token string" style=color:#a6e22e>"0-31,64-95"</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  --cpuset-mems</span><span class="token operator" style=color:#66d9ef>=</span><span class="token string" style=color:#a6e22e>"0"</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  --shm-size</span><span class="token operator" style=color:#66d9ef>=</span><span class="token plain">32g </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token parameter variable" style=color:#f8f8f2>-v</span><span class="token plain"> /data:/data </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token parameter variable" style=color:#f8f8f2>-e</span><span class="token plain"> </span><span class="token assign-left variable" style=color:#f8f8f2>CUDA_VISIBLE_DEVICES</span><span class="token operator" style=color:#66d9ef>=</span><span class="token number" style=color:#ae81ff>0,1</span><span class="token plain">,2,3 </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  my-training-image:latest </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  torchrun </span><span class="token parameter variable" style=color:#f8f8f2>--nproc_per_node</span><span class="token operator" style=color:#66d9ef>=</span><span class="token number" style=color:#ae81ff>4</span><span class="token plain"> train.py</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<p><strong>关键点</strong>：</p>
<ul>
<li><code>4</code>张<code>GPU</code>都在<code>NUMA Node 0</code>，使用相同的<code>CPU</code>亲和性配置</li>
<li>增加共享内存大小以支持多<code>GPU</code>通信</li>
<li>使用<code>torchrun</code>启动分布式训练</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=示例3推理服务优化延迟>示例3：推理服务（优化延迟）<a href=#示例3推理服务优化延迟 class=hash-link aria-label="Direct link to 示例3：推理服务（优化延迟）" title="Direct link to 示例3：推理服务（优化延迟）">​</a></h3>
<p>运行一个使用<code>GPU0</code>的推理服务，需要绑定特定的<code>CPU</code>核心以降低延迟：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token function" style=color:#e6db74>docker</span><span class="token plain"> run </span><span class="token parameter variable" style=color:#f8f8f2>-d</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token parameter variable" style=color:#f8f8f2>--name</span><span class="token plain"> ai-inference-gpu0 </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token parameter variable" style=color:#f8f8f2>--gpus</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>'"device=0"'</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  --cpuset-cpus</span><span class="token operator" style=color:#66d9ef>=</span><span class="token string" style=color:#a6e22e>"0-7,64-71"</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  --cpuset-mems</span><span class="token operator" style=color:#66d9ef>=</span><span class="token string" style=color:#a6e22e>"0"</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token parameter variable" style=color:#f8f8f2>--memory</span><span class="token operator" style=color:#66d9ef>=</span><span class="token string" style=color:#a6e22e>"32g"</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  --memory-reservation</span><span class="token operator" style=color:#66d9ef>=</span><span class="token string" style=color:#a6e22e>"28g"</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token parameter variable" style=color:#f8f8f2>-p</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>8080</span><span class="token plain">:8080 </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token parameter variable" style=color:#f8f8f2>-v</span><span class="token plain"> /models:/models </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  my-inference-image:latest </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  python serve.py </span><span class="token parameter variable" style=color:#f8f8f2>--gpu</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>0</span><span class="token plain"> </span><span class="token parameter variable" style=color:#f8f8f2>--port</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>8080</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<p><strong>优化说明</strong>：</p>
<ul>
<li><code>--cpuset-cpus="0-7,64-71"</code>：只使用<code>8</code>个物理核心（及其超线程），减少调度开销</li>
<li><code>--memory-reservation</code>：预留内存，减少内存回收带来的延迟抖动</li>
<li>绑定到<code>NUMA Node 0</code>确保本地内存访问</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=示例4跨numa节点的5卡训练>示例4：跨NUMA节点的5卡训练<a href=#示例4跨numa节点的5卡训练 class=hash-link aria-label="Direct link to 示例4：跨NUMA节点的5卡训练" title="Direct link to 示例4：跨NUMA节点的5卡训练">​</a></h3>
<p>当需要使用<code>5</code>张<code>GPU</code>时（<code>GPU0-4</code>，跨越两个<code>NUMA</code>节点），需要考虑跨<code>NUMA</code>节点的配置：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token function" style=color:#e6db74>docker</span><span class="token plain"> run </span><span class="token parameter variable" style=color:#f8f8f2>-d</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token parameter variable" style=color:#f8f8f2>--name</span><span class="token plain"> ai-training-5gpu </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token parameter variable" style=color:#f8f8f2>--gpus</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>'"device=0,1,2,3,4"'</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  --cpuset-cpus</span><span class="token operator" style=color:#66d9ef>=</span><span class="token string" style=color:#a6e22e>"0-63,64-127"</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  --cpuset-mems</span><span class="token operator" style=color:#66d9ef>=</span><span class="token string" style=color:#a6e22e>"0,1"</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  --shm-size</span><span class="token operator" style=color:#66d9ef>=</span><span class="token plain">48g </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token parameter variable" style=color:#f8f8f2>-v</span><span class="token plain"> /data:/data </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token parameter variable" style=color:#f8f8f2>-e</span><span class="token plain"> </span><span class="token assign-left variable" style=color:#f8f8f2>CUDA_VISIBLE_DEVICES</span><span class="token operator" style=color:#66d9ef>=</span><span class="token number" style=color:#ae81ff>0,1</span><span class="token plain">,2,3,4 </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token parameter variable" style=color:#f8f8f2>-e</span><span class="token plain"> </span><span class="token assign-left variable" style=color:#f8f8f2>OMP_NUM_THREADS</span><span class="token operator" style=color:#66d9ef>=</span><span class="token number" style=color:#ae81ff>64</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  my-training-image:latest </span><span class="token punctuation" style=color:#f8f8f2>\</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  torchrun </span><span class="token parameter variable" style=color:#f8f8f2>--nproc_per_node</span><span class="token operator" style=color:#66d9ef>=</span><span class="token number" style=color:#ae81ff>5</span><span class="token plain"> train.py</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<p><strong>注意事项</strong>：</p>
<ul>
<li><strong>跨NUMA场景</strong>：<code>GPU0-3</code>在<code>NUMA Node 0</code>，<code>GPU4</code>在<code>NUMA Node 1</code>，必然存在跨<code>NUMA</code>通信</li>
<li><code>--cpuset-mems="0,1"</code>：允许使用两个<code>NUMA</code>节点的内存</li>
<li>增加<code>OMP_NUM_THREADS</code>以充分利用所有<code>CPU</code>核心</li>
<li>训练代码应注意<code>GPU</code>间通信模式，<code>GPU4</code>与<code>GPU0-3</code>的通信会经过<code>SYS</code>连接（性能较低）</li>
<li><strong>性能优化建议</strong>：如果可能，优先使用<code>4</code>卡（<code>GPU0-3</code>或<code>GPU4-7</code>，单<code>NUMA</code>节点）而非<code>5</code>卡，可避免跨<code>NUMA</code>开销</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=docker亲和性配置验证>Docker亲和性配置验证<a href=#docker亲和性配置验证 class=hash-link aria-label="Direct link to Docker亲和性配置验证" title="Direct link to Docker亲和性配置验证">​</a></h3>
<p>在容器内可以使用以下命令验证亲和性配置：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token comment" style=color:#8292a2;font-style:italic># 查看容器的CPU亲和性</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token function" style=color:#e6db74>docker</span><span class="token plain"> </span><span class="token builtin class-name" style=color:#e6db74>exec</span><span class="token plain"> ai-training-gpu0 taskset </span><span class="token parameter variable" style=color:#f8f8f2>-cp</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>1</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token comment" style=color:#8292a2;font-style:italic># 查看容器的NUMA策略</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token function" style=color:#e6db74>docker</span><span class="token plain"> </span><span class="token builtin class-name" style=color:#e6db74>exec</span><span class="token plain"> ai-training-gpu0 numactl </span><span class="token parameter variable" style=color:#f8f8f2>--show</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token comment" style=color:#8292a2;font-style:italic># 查看容器可见的GPU</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token function" style=color:#e6db74>docker</span><span class="token plain"> </span><span class="token builtin class-name" style=color:#e6db74>exec</span><span class="token plain"> ai-training-gpu0 nvidia-smi </span><span class="token parameter variable" style=color:#f8f8f2>-L</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token comment" style=color:#8292a2;font-style:italic># 查看进程的CPU绑定情况</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token function" style=color:#e6db74>docker</span><span class="token plain"> </span><span class="token builtin class-name" style=color:#e6db74>exec</span><span class="token plain"> ai-training-gpu0 </span><span class="token function" style=color:#e6db74>ps</span><span class="token plain"> </span><span class="token parameter variable" style=color:#f8f8f2>-eLo</span><span class="token plain"> pid,tid,psr,comm </span><span class="token operator" style=color:#66d9ef>|</span><span class="token plain"> </span><span class="token function" style=color:#e6db74>grep</span><span class="token plain"> python</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=kubernetes中cpunuma亲和性配置>Kubernetes中CPU&NUMA亲和性配置<a href=#kubernetes中cpunuma亲和性配置 class=hash-link aria-label="Direct link to Kubernetes中CPU&NUMA亲和性配置" title="Direct link to Kubernetes中CPU&NUMA亲和性配置">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=kubernetes的亲和性机制>Kubernetes的亲和性机制<a href=#kubernetes的亲和性机制 class=hash-link aria-label="Direct link to Kubernetes的亲和性机制" title="Direct link to Kubernetes的亲和性机制">​</a></h3>
<p><code>Kubernetes</code>提供了多种机制来控制<code>Pod</code>的<code>CPU</code>和<code>NUMA</code>亲和性：</p>
<ol>
<li><strong>CPU Manager</strong>：静态策略下实现<code>CPU</code>绑核</li>
<li><strong>Topology Manager</strong>：协调<code>CPU Manager</code>和<code>Device Manager</code>实现拓扑感知调度</li>
<li><strong>Device Plugin</strong>：管理<code>GPU</code>等设备资源</li>
<li><strong>节点亲和性</strong>：控制<code>Pod</code>调度到特定节点</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=前置条件配置>前置条件配置<a href=#前置条件配置 class=hash-link aria-label="Direct link to 前置条件配置" title="Direct link to 前置条件配置">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=1-启用kubelet特性>1. 启用Kubelet特性<a href=#1-启用kubelet特性 class=hash-link aria-label="Direct link to 1. 启用Kubelet特性" title="Direct link to 1. 启用Kubelet特性">​</a></h4>
<p>需要在<code>Kubelet</code>配置中启用以下特性：</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token comment" style=color:#8292a2;font-style:italic># /var/lib/kubelet/config.yaml</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">apiVersion</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> kubelet.config.k8s.io/v1beta1</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">kind</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> KubeletConfiguration</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">cpuManagerPolicy</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> static</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">cpuManagerReconcilePeriod</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> 10s</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">topologyManagerPolicy</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> single</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">numa</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">node  </span><span class="token comment" style=color:#8292a2;font-style:italic># 或 best-effort、restricted</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">topologyManagerScope</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> pod</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">reservedSystemCPUs</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"0,64"</span><span class="token plain">  </span><span class="token comment" style=color:#8292a2;font-style:italic># 为系统预留CPU核心</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token comment" style=color:#8292a2;font-style:italic># ...</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<p><strong>Topology Manager策略说明</strong>：</p>
<table><thead><tr><th>策略<th>说明<th>适用场景<tbody><tr><td><code>none</code><td>默认策略，不进行拓扑对齐<td>不关心<code>NUMA</code>亲和性<tr><td><code>best-effort</code><td>尽力对齐，对齐失败也允许调度<td>希望优化但不强制要求<tr><td><code>restricted</code><td>优先对齐，失败时放宽限制<td>平衡性能和调度成功率<tr><td><code>single-numa-node</code><td>强制所有资源在同一<code>NUMA</code>节点<td>对性能要求极高的场景</table>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=2-重启kubelet使配置生效>2. 重启Kubelet使配置生效<a href=#2-重启kubelet使配置生效 class=hash-link aria-label="Direct link to 2. 重启Kubelet使配置生效" title="Direct link to 2. 重启Kubelet使配置生效">​</a></h4>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token plain">systemctl restart kubelet</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=示例1单gpu训练任务numa-node-0>示例1：单GPU训练任务（NUMA Node 0）<a href=#示例1单gpu训练任务numa-node-0 class=hash-link aria-label="Direct link to 示例1：单GPU训练任务（NUMA Node 0）" title="Direct link to 示例1：单GPU训练任务（NUMA Node 0）">​</a></h3>
<p>创建一个使用<code>GPU0</code>的训练任务，绑定到<code>NUMA Node 0</code>：</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token key atrule">apiVersion</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> v1</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">kind</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> Pod</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">metadata</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> ai</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">training</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">gpu0</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">labels</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">app</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> ai</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">training</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">gpu-id</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"0"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">spec</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">restartPolicy</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> Never</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token comment" style=color:#8292a2;font-style:italic># 节点选择器：确保调度到有GPU0的节点</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">nodeSelector</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">nvidia.com/gpu.present</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"true"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">containers</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> training</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">image</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> my</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">training</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">image</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain">latest</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">command</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>[</span><span class="token string" style=color:#a6e22e>"python"</span><span class="token punctuation" style=color:#f8f8f2>,</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"train.py"</span><span class="token punctuation" style=color:#f8f8f2>,</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"--gpu"</span><span class="token punctuation" style=color:#f8f8f2>,</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"0"</span><span class="token punctuation" style=color:#f8f8f2>]</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">resources</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">requests</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">cpu</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"16"</span><span class="token plain">           </span><span class="token comment" style=color:#8292a2;font-style:italic># 请求16个CPU核心</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">memory</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"64Gi"</span><span class="token plain">      </span><span class="token comment" style=color:#8292a2;font-style:italic># 请求64GB内存</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">nvidia.com/gpu</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>1</span><span class="token plain">   </span><span class="token comment" style=color:#8292a2;font-style:italic># 请求1个GPU</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">limits</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">cpu</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"32"</span><span class="token plain">           </span><span class="token comment" style=color:#8292a2;font-style:italic># 限制最多使用32个CPU核心</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">memory</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"128Gi"</span><span class="token plain">     </span><span class="token comment" style=color:#8292a2;font-style:italic># 限制最多使用128GB内存</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">nvidia.com/gpu</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>1</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">env</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> CUDA_VISIBLE_DEVICES</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">value</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"0"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> OMP_NUM_THREADS</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">value</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"16"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token comment" style=color:#8292a2;font-style:italic># NUMA绑定环境变量（需要容器支持）</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> GOMP_CPU_AFFINITY</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">value</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"0-31 64-95"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">volumeMounts</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> data</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">mountPath</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> /data</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> shm</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">mountPath</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> /dev/shm</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">volumes</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> data</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">hostPath</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">path</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> /data</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">type</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> Directory</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> shm</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">emptyDir</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">medium</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> Memory</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">sizeLimit</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> 32Gi</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<p><strong>关键配置说明</strong>：</p>
<ol>
<li>
<p><strong>CPU请求和限制</strong>：</p>
<ul>
<li><code>requests.cpu: "16"</code>确保<code>Pod</code>获得足够的<code>CPU</code>资源</li>
<li>在<code>static</code>策略下，<code>Kubernetes</code>会为该<code>Pod</code>独占分配<code>16</code>个<code>CPU</code>核心</li>
</ul>
</li>
<li>
<p><strong>GPU资源</strong>：</p>
<ul>
<li><code>nvidia.com/gpu: 1</code>请求一个<code>GPU</code></li>
<li><code>Device Plugin</code>会根据调度策略分配合适的<code>GPU</code></li>
</ul>
</li>
<li>
<p><strong>共享内存</strong>：</p>
<ul>
<li>使用<code>emptyDir</code>卷挂载到<code>/dev/shm</code>，提供足够的共享内存</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=示例2多gpu训练任务4卡同numa节点>示例2：多GPU训练任务（4卡，同NUMA节点）<a href=#示例2多gpu训练任务4卡同numa节点 class=hash-link aria-label="Direct link to 示例2：多GPU训练任务（4卡，同NUMA节点）" title="Direct link to 示例2：多GPU训练任务（4卡，同NUMA节点）">​</a></h3>
<p>使用<code>GPU0-GPU3</code>进行<code>4</code>卡分布式训练：</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token key atrule">apiVersion</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> v1</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">kind</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> Pod</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">metadata</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> ai</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">training</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">4gpu</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">labels</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">app</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> ai</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">training</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">distributed</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">gpu-count</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"4"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">spec</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">restartPolicy</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> Never</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token comment" style=color:#8292a2;font-style:italic># 节点亲和性：选择有足够GPU的节点</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">affinity</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">nodeAffinity</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">requiredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">nodeSelectorTerms</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">matchExpressions</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">key</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> nvidia.com/gpu.count</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">            </span><span class="token key atrule">operator</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> Gt</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">            </span><span class="token key atrule">values</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>[</span><span class="token string" style=color:#a6e22e>"3"</span><span class="token punctuation" style=color:#f8f8f2>]</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">containers</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> training</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">image</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> my</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">training</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">image</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain">latest</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">command</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> torchrun</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">nproc_per_node=4</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">nnodes=1</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">node_rank=0</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> train.py</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">resources</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">requests</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">cpu</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"32"</span><span class="token plain">           </span><span class="token comment" style=color:#8292a2;font-style:italic># 4卡任务请求更多CPU</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">memory</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"128Gi"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">nvidia.com/gpu</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>4</span><span class="token plain">   </span><span class="token comment" style=color:#8292a2;font-style:italic># 请求4个GPU</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">limits</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">cpu</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"64"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">memory</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"256Gi"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">nvidia.com/gpu</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>4</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">env</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> CUDA_VISIBLE_DEVICES</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">value</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"0,1,2,3"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> NCCL_DEBUG</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">value</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"INFO"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> NCCL_IB_DISABLE</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">value</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"0"</span><span class="token plain">          </span><span class="token comment" style=color:#8292a2;font-style:italic># 启用InfiniBand（如果可用）</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> NCCL_SOCKET_IFNAME</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">value</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"eth0"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> OMP_NUM_THREADS</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">value</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"32"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">volumeMounts</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> data</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">mountPath</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> /data</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> shm</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">mountPath</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> /dev/shm</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">volumes</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> data</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">persistentVolumeClaim</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">claimName</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> training</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">data</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">pvc</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> shm</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">emptyDir</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">medium</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> Memory</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">sizeLimit</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> 64Gi</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=示例3推理服务固定gpu和cpu绑核>示例3：推理服务（固定GPU和CPU绑核）<a href=#示例3推理服务固定gpu和cpu绑核 class=hash-link aria-label="Direct link to 示例3：推理服务（固定GPU和CPU绑核）" title="Direct link to 示例3：推理服务（固定GPU和CPU绑核）">​</a></h3>
<p>部署一个低延迟的推理服务：</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token key atrule">apiVersion</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> apps/v1</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">kind</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> Deployment</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">metadata</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> ai</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">inference</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">service</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">spec</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">replicas</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>1</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">selector</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">matchLabels</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">app</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> ai</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">inference</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">template</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">metadata</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">labels</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">app</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> ai</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">inference</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">spec</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token comment" style=color:#8292a2;font-style:italic># 使用Guaranteed QoS确保独占CPU</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">containers</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> inference</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">image</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> my</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">inference</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">image</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain">latest</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">command</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>[</span><span class="token string" style=color:#a6e22e>"python"</span><span class="token punctuation" style=color:#f8f8f2>,</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"serve.py"</span><span class="token punctuation" style=color:#f8f8f2>,</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"--gpu"</span><span class="token punctuation" style=color:#f8f8f2>,</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"0"</span><span class="token punctuation" style=color:#f8f8f2>,</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"--port"</span><span class="token punctuation" style=color:#f8f8f2>,</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"8080"</span><span class="token punctuation" style=color:#f8f8f2>]</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">ports</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">containerPort</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>8080</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> http</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token key atrule">protocol</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> TCP</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">resources</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token comment" style=color:#8292a2;font-style:italic># requests和limits相同，确保Guaranteed QoS</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token key atrule">requests</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">            </span><span class="token key atrule">cpu</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"8"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">            </span><span class="token key atrule">memory</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"32Gi"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">            </span><span class="token key atrule">nvidia.com/gpu</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>1</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token key atrule">limits</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">            </span><span class="token key atrule">cpu</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"8"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">            </span><span class="token key atrule">memory</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"32Gi"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">            </span><span class="token key atrule">nvidia.com/gpu</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>1</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">env</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> CUDA_VISIBLE_DEVICES</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token key atrule">value</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"0"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> OMP_NUM_THREADS</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token key atrule">value</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"8"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token comment" style=color:#8292a2;font-style:italic># 固定CPU亲和性</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> GOMP_CPU_AFFINITY</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token key atrule">value</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"0-7 64-71"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">livenessProbe</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token key atrule">httpGet</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">            </span><span class="token key atrule">path</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> /health</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">            </span><span class="token key atrule">port</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>8080</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token key atrule">initialDelaySeconds</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>30</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token key atrule">periodSeconds</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>10</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">readinessProbe</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token key atrule">httpGet</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">            </span><span class="token key atrule">path</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> /ready</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">            </span><span class="token key atrule">port</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>8080</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token key atrule">initialDelaySeconds</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>20</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token key atrule">periodSeconds</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>5</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">volumeMounts</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> models</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token key atrule">mountPath</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> /models</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token key atrule">readOnly</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token boolean important" style=color:#ae81ff>true</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">volumes</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> models</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">persistentVolumeClaim</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">          </span><span class="token key atrule">claimName</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> models</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">pvc</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<p><strong>推理服务优化要点</strong>：</p>
<ol>
<li><strong>Guaranteed QoS</strong>：requests和limits相同，确保Pod获得独占CPU资源</li>
<li><strong>固定CPU数量</strong>：使用较少的CPU核心，减少调度开销</li>
<li><strong>健康检查</strong>：配置liveness和readiness探针确保服务稳定</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=示例4跨numa节点的5卡训练-1>示例4：跨NUMA节点的5卡训练<a href=#示例4跨numa节点的5卡训练-1 class=hash-link aria-label="Direct link to 示例4：跨NUMA节点的5卡训练" title="Direct link to 示例4：跨NUMA节点的5卡训练">​</a></h3>
<p>使用5张GPU进行分布式训练（跨NUMA节点）：</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token key atrule">apiVersion</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> v1</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">kind</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> Pod</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">metadata</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> ai</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">training</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">5gpu</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">labels</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">app</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> ai</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">training</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">distributed</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">gpu-count</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"5"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token key atrule">spec</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">restartPolicy</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> Never</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">containers</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> training</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">image</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> my</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">training</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">image</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain">latest</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">command</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> torchrun</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">nproc_per_node=5</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">nnodes=1</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">node_rank=0</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> train.py</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">resources</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">requests</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">cpu</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"48"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">memory</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"192Gi"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">nvidia.com/gpu</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>5</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">limits</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">cpu</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"96"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">memory</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"384Gi"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        </span><span class="token key atrule">nvidia.com/gpu</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>5</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">env</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> CUDA_VISIBLE_DEVICES</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">value</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"0,1,2,3,4"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> NCCL_DEBUG</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">value</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"INFO"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> NCCL_IB_DISABLE</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">value</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"0"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> NCCL_SOCKET_IFNAME</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">value</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"eth0"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token comment" style=color:#8292a2;font-style:italic># 跨NUMA节点，允许使用所有CPU</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> OMP_NUM_THREADS</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">value</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"48"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token comment" style=color:#8292a2;font-style:italic># NCCL拓扑感知</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> NCCL_TOPO_FILE</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">value</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token string" style=color:#a6e22e>"/etc/nccl/topo.xml"</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">volumeMounts</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> data</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">mountPath</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> /data</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> shm</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">mountPath</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> /dev/shm</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> nccl</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">topo</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">mountPath</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> /etc/nccl</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">readOnly</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> </span><span class="token boolean important" style=color:#ae81ff>true</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token key atrule">volumes</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> data</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">persistentVolumeClaim</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">claimName</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> training</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">data</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">pvc</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> shm</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">emptyDir</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">medium</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> Memory</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">sizeLimit</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> 96Gi</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">  </span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain"> </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> nccl</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">topo</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    </span><span class="token key atrule">configMap</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">      </span><span class="token key atrule">name</span><span class="token punctuation" style=color:#f8f8f2>:</span><span class="token plain"> nccl</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">topology</span><span class="token punctuation" style=color:#f8f8f2>-</span><span class="token plain">config</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<p><strong>跨NUMA配置说明</strong>：</p>
<ul>
<li><strong>资源分配</strong>：<code>5</code>卡场景下，<code>4</code>卡在<code>NUMA Node 0</code>，<code>1</code>卡在<code>NUMA Node 1</code></li>
<li><strong>CPU配置</strong>：分配<code>48</code>个物理核心，足够支持<code>5</code>卡训练的数据预处理</li>
<li><strong>内存配置</strong>：总共<code>192GB</code>请求内存，<code>Topology Manager</code>会尽量从两个<code>NUMA</code>节点分配</li>
<li><strong>性能考虑</strong>：<code>GPU4</code>与<code>GPU0-3</code>之间的通信会通过<code>SYS</code>连接，带宽较低（8-16 GB/s）</li>
<li><strong>替代方案</strong>：如果性能要求高，建议使用<code>4</code>卡（单<code>NUMA</code>节点）或<code>8</code>卡（双<code>NUMA</code>均衡）方案</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=使用topology-manager实现自动numa对齐>使用Topology Manager实现自动NUMA对齐<a href=#使用topology-manager实现自动numa对齐 class=hash-link aria-label="Direct link to 使用Topology Manager实现自动NUMA对齐" title="Direct link to 使用Topology Manager实现自动NUMA对齐">​</a></h3>
<p>当启用<code>Topology Manager</code>并设置为<code>single-numa-node</code>策略时，<code>Kubernetes</code>会自动确保：</p>
<ol>
<li><code>Pod</code>请求的所有<code>GPU</code>在同一<code>NUMA</code>节点</li>
<li><code>Pod</code>分配的<code>CPU</code>核心在同一<code>NUMA</code>节点</li>
<li><code>Pod</code>的内存分配优先从同一<code>NUMA</code>节点分配</li>
</ol>
<p><strong>验证Topology对齐</strong>：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token comment" style=color:#8292a2;font-style:italic># 进入Pod查看CPU绑定</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">kubectl </span><span class="token builtin class-name" style=color:#e6db74>exec</span><span class="token plain"> </span><span class="token parameter variable" style=color:#f8f8f2>-it</span><span class="token plain"> ai-training-gpu0 -- taskset </span><span class="token parameter variable" style=color:#f8f8f2>-cp</span><span class="token plain"> </span><span class="token number" style=color:#ae81ff>1</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token comment" style=color:#8292a2;font-style:italic># 查看NUMA策略</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">kubectl </span><span class="token builtin class-name" style=color:#e6db74>exec</span><span class="token plain"> </span><span class="token parameter variable" style=color:#f8f8f2>-it</span><span class="token plain"> ai-training-gpu0 -- numactl </span><span class="token parameter variable" style=color:#f8f8f2>--show</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain" style=display:inline-block></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain"></span><span class="token comment" style=color:#8292a2;font-style:italic># 查看GPU拓扑</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">kubectl </span><span class="token builtin class-name" style=color:#e6db74>exec</span><span class="token plain"> </span><span class="token parameter variable" style=color:#f8f8f2>-it</span><span class="token plain"> ai-training-gpu0 -- nvidia-smi topo </span><span class="token parameter variable" style=color:#f8f8f2>-m</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/ai/hybrid-scheduling-scale-down-control><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>混部调度中如何控制在线服务的缩容逻辑</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/ai/what-is-llmops-mlops><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>LLMOps介绍</div></a></nav><div class=docusaurus-mt-lg></div></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#引言 class="table-of-contents__link toc-highlight">引言</a><li><a href=#numa架构简介 class="table-of-contents__link toc-highlight">NUMA架构简介</a><ul><li><a href=#什么是numa class="table-of-contents__link toc-highlight">什么是NUMA</a><li><a href=#numa架构的层级结构 class="table-of-contents__link toc-highlight">NUMA架构的层级结构</a><li><a href=#numa对性能的影响 class="table-of-contents__link toc-highlight">NUMA对性能的影响</a></ul><li><a href=#cpunuma亲和性的重要性 class="table-of-contents__link toc-highlight">CPU&NUMA亲和性的重要性</a><ul><li><a href=#cpu亲和性的作用 class="table-of-contents__link toc-highlight">CPU亲和性的作用</a><li><a href=#numa亲和性的作用 class="table-of-contents__link toc-highlight">NUMA亲和性的作用</a><li><a href=#ai模型训练推理场景中的影响 class="table-of-contents__link toc-highlight">AI模型训练推理场景中的影响</a></ul><li><a href=#cpunuma亲和性示例分析 class="table-of-contents__link toc-highlight">CPU&NUMA亲和性示例分析</a><ul><li><a href=#理解nvidia-smi-topo命令输出 class="table-of-contents__link toc-highlight">理解nvidia-smi topo命令输出</a><li><a href=#连接类型说明 class="table-of-contents__link toc-highlight">连接类型说明</a><li><a href=#cpu-affinity字段解析 class="table-of-contents__link toc-highlight">CPU Affinity字段解析</a><li><a href=#numa-affinity�字段解析 class="table-of-contents__link toc-highlight">NUMA Affinity字段解析</a><li><a href=#拓扑分析总结 class="table-of-contents__link toc-highlight">拓扑分析总结</a></ul><li><a href=#docker中的cpunuma亲和性配置 class="table-of-contents__link toc-highlight">Docker中的CPU&NUMA亲和性配置</a><ul><li><a href=#docker的亲和性参数 class="table-of-contents__link toc-highlight">Docker的亲和性参数</a><li><a href=#示例1单gpu训练任务 class="table-of-contents__link toc-highlight">示例1：单GPU训练任务</a><li><a href=#示例2多gpu训练任务同numa节点 class="table-of-contents__link toc-highlight">示例2：多GPU训练任务（同NUMA节点）</a><li><a href=#示例3推理服务优化延迟 class="table-of-contents__link toc-highlight">示例3：推理服务（优化延迟）</a><li><a href=#示例4跨numa节点的5卡训练 class="table-of-contents__link toc-highlight">示例4：跨NUMA节点的5卡训练</a><li><a href=#docker亲和性配置验证 class="table-of-contents__link toc-highlight">Docker亲和性配置验证</a></ul><li><a href=#kubernetes中cpunuma亲和性配置 class="table-of-contents__link toc-highlight">Kubernetes中CPU&NUMA亲和性配置</a><ul><li><a href=#kubernetes的亲和性机制 class="table-of-contents__link toc-highlight">Kubernetes的亲和性机制</a><li><a href=#前置条件配置 class="table-of-contents__link toc-highlight">前置条件配置</a><li><a href=#示例1单gpu训练任务numa-node-0 class="table-of-contents__link toc-highlight">示例1：单GPU训练任务（NUMA Node 0）</a><li><a href=#示例2多gpu训练任务4卡同numa节点 class="table-of-contents__link toc-highlight">示例2：多GPU训练任务（4卡，同NUMA节点）</a><li><a href=#示例3推理服务固定gpu和cpu绑核 class="table-of-contents__link toc-highlight">示例3：推理服务（固定GPU和CPU绑核）</a><li><a href=#示例4跨numa节点的5卡训练-1 class="table-of-contents__link toc-highlight">示例4：跨NUMA节点的5卡训练</a><li><a href=#使用topology-manager实现自动numa对齐 class="table-of-contents__link toc-highlight">使用Topology Manager实现自动NUMA对齐</a></ul></ul></div></div></div></div></main></div></div></div><footer class=footer><div class="container container-fluid"><div class="footer__bottom text--center"><div class=footer__copyright>Copyright 2025 johng.cn</div></div></div></footer></div>