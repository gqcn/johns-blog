<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-docs/AI技术/AI基础架构/算力调度/CPU亲和性与NUMA亲和性调度" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.6.3"><title data-rh=true>CPU亲和性与NUMA亲和性调度 | John's Blog</title><meta data-rh=true name=viewport content="width=device-width,initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:url content=https://johng.cn/ai/cpu-numa-affinity-scheduling><meta data-rh=true property=og:locale content=en><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=author content="John Guo"><meta data-rh=true property=og:image content=https://johng.cn/img/favicon.png><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-default-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-default-current><meta data-rh=true property=og:title content="CPU亲和性与NUMA亲和性调度 | John's Blog"><meta data-rh=true name=description content=深入探讨AI模型开发训练推理场景下的CPU亲和性与NUMA亲和性调度技术，介绍NUMA架构原理、GPU拓扑分析、Docker和Kubernetes环境下的亲和性配置方案，通过实际案例优化AI工作负载的性能表现><meta data-rh=true property=og:description content=深入探讨AI模型开发训练推理场景下的CPU亲和性与NUMA亲和性调度技术，介绍NUMA架构原理、GPU拓扑分析、Docker和Kubernetes环境下的亲和性配置方案，通过实际案例优化AI工作负载的性能表现><meta data-rh=true name=keywords content="CPU亲和性,NUMA亲和性,NUMA架构,GPU调度,AI训练,AI推理,性能优化,Docker亲和性,Kubernetes亲和性,CPU绑核,内存访问优化,跨NUMA节点,nvidia-smi,GPU拓扑,分布式训练,NVLINK,资源调度,Kubernetes Topology Manager,CPU Manager,静态策略"><link data-rh=true rel=icon href=/img/favicon.ico><link data-rh=true rel=canonical href=https://johng.cn/ai/cpu-numa-affinity-scheduling><link data-rh=true rel=alternate href=https://johng.cn/ai/cpu-numa-affinity-scheduling hreflang=en><link data-rh=true rel=alternate href=https://johng.cn/ai/cpu-numa-affinity-scheduling hreflang=x-default><link data-rh=true rel=preconnect href=https://XGS1CPQERK-dsn.algolia.net crossorigin><link rel=alternate type=application/rss+xml href=/blog/rss.xml title="John's Blog RSS Feed"><link rel=alternate type=application/atom+xml href=/blog/atom.xml title="John's Blog Atom Feed"><link rel=search type=application/opensearchdescription+xml title="John's Blog" href=/opensearch.xml><script src=https://hm.baidu.com/hm.js?6b4ae23dc83ee5efe875b7172af6c7c1 async></script><link rel=stylesheet href=/assets/css/styles.2f56d3c4.css><script src=/assets/js/runtime~main.11cb46fe.js defer></script><script src=/assets/js/main.815918c6.js defer></script><body class=navigation-with-keyboard><script>!function(){var t,e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t=null!==e?e:"light",document.documentElement.setAttribute("data-theme",t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/><b class="navbar__title text--truncate">John's Blog</b></a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/ai>AI技术</a><a class="navbar__item navbar__link" href=/cloud-native>云原生</a><a class="navbar__item navbar__link" href=/notes>日常笔记</a><a class="navbar__item navbar__link" href=/programming>开发语言</a><a class="navbar__item navbar__link" href=/architecture>技术架构</a><a class="navbar__item navbar__link" href=/observability>可观测性</a><a class="navbar__item navbar__link" href=/life>生活笔记</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href=/aboutme>关于我</a><a class="navbar__item navbar__link" href=/blog>博客</a><a href=https://goframe.org/ target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-goframe-link"></a><a href=https://github.com/gqcn target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-github-link"></a><div class=navbarSearchContainer_Bca1><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_z2l0"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class=sidebar_njMd><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/ai>AI技术</a><button aria-label="Collapse sidebar category 'AI技术'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/app>AI应用技术</a><button aria-label="Expand sidebar category 'AI应用技术'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" tabindex=0 href=/ai/infra>AI基础架构</a><button aria-label="Collapse sidebar category 'AI基础架构'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/vgpu>vGPU</a><button aria-label="Expand sidebar category 'vGPU'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/rdma>RDMA</a><button aria-label="Expand sidebar category 'RDMA'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/nvidia>NVIDIA</a><button aria-label="Expand sidebar category 'NVIDIA'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/training>开发训练</a><button aria-label="Expand sidebar category '开发训练'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/inference>推理服务</a><button aria-label="Expand sidebar category '推理服务'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" tabindex=0 href=/ai/scheduling>算力调度</a><button aria-label="Collapse sidebar category '算力调度'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class=menu__link tabindex=0 href=/ai/hybrid-scheduling-affinity-toleration>混合调度的亲和性、污点容忍设计思考</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class=menu__link tabindex=0 href=/ai/hybrid-scheduling-scale-down-control>混部调度中如何控制在线服务的缩容逻辑</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/ai/cpu-numa-affinity-scheduling>CPU亲和性与NUMA亲和性调度</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class=menu__link tabindex=0 href=/ai/cpu-numa-affinity-in-docker>Docker中的CPU&NUMA亲和性配置</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class=menu__link tabindex=0 href=/ai/cpu-numa-affinity-in-kubernetes>Kubernetes中CPU&NUMA亲和性配置</a></ul><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/ai/what-is-llmops-mlops>LLMOps介绍</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/ai/common-acceleration-cards>常见智算加速卡汇总</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/ai/common-ai-model-training-inference-framework>常见AI模型训练推理框架对比</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/ai/ai-training-inference-scenarios>AI模型训练推理常见业务场景痛点</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/basic>AI入门知识</a><button aria-label="Expand sidebar category 'AI入门知识'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/cloud-native>云原生</a><button aria-label="Expand sidebar category '云原生'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/notes>日常笔记</a><button aria-label="Expand sidebar category '日常笔记'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/programming>开发语言</a><button aria-label="Expand sidebar category '开发语言'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/architecture>技术架构</a><button aria-label="Expand sidebar category '技术架构'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/management>技术管理</a><button aria-label="Expand sidebar category '技术管理'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/observability>可观测性</a><button aria-label="Expand sidebar category '可观测性'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/database-and-middleware>数据库与中间件</a><button aria-label="Expand sidebar category '数据库与中间件'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/operating-systems-and-networks>操作系统和网络</a><button aria-label="Expand sidebar category '操作系统和网络'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/life>生活笔记</a><button aria-label="Expand sidebar category '生活笔记'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div></ul></nav></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/ai><span itemprop=name>AI技术</span></a><meta itemprop=position content=1><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/ai/infra><span itemprop=name>AI基础架构</span></a><meta itemprop=position content=2><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/ai/scheduling><span itemprop=name>算力调度</span></a><meta itemprop=position content=3><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link itemprop=name>CPU亲和性与NUMA亲和性调度</span><meta itemprop=position content=4></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id=引言>引言<a href=#引言 class=hash-link aria-label="Direct link to 引言" title="Direct link to 引言">​</a></h2>
<p>在<code>AI</code>模型开发、训练和推理场景中，计算密集型任务对硬件资源的高效利用提出了极高要求。除了<code>GPU</code>算力本身，<code>CPU</code>和内存的访问效率同样是影响整体性能的关键因素。不合理的<code>CPU</code>与内存分配可能导致跨<code>NUMA</code>节点的内存访问、频繁的进程迁移等问题，从而显著降低系统性能。</p>
<p><code>CPU</code>亲和性（<code>CPU Affinity</code>）和<code>NUMA</code>亲和性（<code>NUMA Affinity</code>）调度技术通过精确控制进程与<code>CPU</code>核心、内存节点的绑定关系，最大化利用硬件拓扑结构，减少资源访问延迟，提升<code>AI</code>工作负载的性能表现。本文将深入介绍这些核心技术的原理、应用场景及在<code>Docker</code>和<code>Kubernetes</code>环境下的具体实现方法。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=numa架构简介>NUMA架构简介<a href=#numa架构简介 class=hash-link aria-label="Direct link to NUMA架构简介" title="Direct link to NUMA架构简介">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=什么是numa>什么是NUMA<a href=#什么是numa class=hash-link aria-label="Direct link to 什么是NUMA" title="Direct link to 什么是NUMA">​</a></h3>
<p><code>NUMA</code>（<code>Non-Uniform Memory Access</code>，非统一内存访问）是现代多处理器系统采用的一种内存访问架构。在<code>NUMA</code>架构中，系统内存被划分为多个<code>NUMA</code>节点（<code>NUMA Node</code>），每个节点包含一组<code>CPU</code>核心和本地内存。</p>
<p>与传统的<code>UMA</code>（<code>Uniform Memory Access</code>，统一内存访问）架构相比，<code>NUMA</code>架构具有以下特点：</p>
<ul>
<li><strong>本地访问优化</strong>：<code>CPU</code>访问本地<code>NUMA</code>节点的内存速度最快</li>
<li><strong>远程访问代价</strong>：<code>CPU</code>访问其他<code>NUMA</code>节点的内存（跨节点访问）会产生额外延迟</li>
<li><strong>可扩展性强</strong>：通过增加<code>NUMA</code>节点可以线性扩展系统规模</li>
</ul>
<p><img decoding=async loading=lazy alt=CPU处理器架构：SMP、NUMA、MPP src=/assets/images/image-7-63d636af9ae21944724181c150a612e1.png width=1080 height=772 class=img_ev3q></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=numa架构的层级结构>NUMA架构的层级结构<a href=#numa架构的层级结构 class=hash-link aria-label="Direct link to NUMA架构的层级结构" title="Direct link to NUMA架构的层级结构">​</a></h3>
<p>例如一个典型的<code>NUMA</code>系统具有以下层级结构：</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-text codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token plain">┌─────────────────────────────────────────────────────────────┐</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">│                        NUMA 系统                              │</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">├──────────────────────────┬──────────────────────────────────┤</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">│     NUMA Node 0          │        NUMA Node 1               │</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">├──────────────────────────┼──────────────────────────────────┤</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">│  CPU 0-31, 64-95         │     CPU 32-63, 96-127            │</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">│  Local Memory            │     Local Memory                 │</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">│  PCIe Devices (GPU0-3)   │     PCIe Devices (GPU4-7)        │</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">└──────────────────────────┴──────────────────────────────────┘</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<p>在上述架构中：</p>
<ul>
<li>每个<code>NUMA</code>节点包含一组<code>CPU</code>核心</li>
<li>每个<code>NUMA</code>节点有自己的本地内存</li>
<li><code>PCIe</code>设备（如<code>GPU</code>）也归属于特定的<code>NUMA</code>节点</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=numa对性能的影响>NUMA对性能的影响<a href=#numa对性能的影响 class=hash-link aria-label="Direct link to NUMA对性能的影响" title="Direct link to NUMA对性能的影响">​</a></h3>
<p>在<code>AI</code>训练和推理场景中，<code>NUMA</code>架构对性能的影响主要体现在：</p>
<ol>
<li>
<p><strong>内存访问延迟</strong>：本地内存访问延迟通常为<code>100-200ns</code>，而跨<code>NUMA</code>节点访问延迟可达<code>300-500ns</code>，差距达<code>2-3</code>倍</p>
</li>
<li>
<p><strong>内存带宽</strong>：跨节点访问会占用<code>NUMA</code>互连总线，降低整体内存带宽</p>
</li>
<li>
<p><strong>PCIe通信延迟</strong>：</p>
<ul>
<li><code>GPU</code>物理连接在特定<code>NUMA</code>节点的<code>PCIe</code>总线上</li>
<li><code>CPU</code>访问非本地<code>NUMA</code>节点的<code>GPU</code>需要经过<code>NUMA</code>互联（<code>QPI</code>/<code>UPI</code>）</li>
<li>跨<code>NUMA</code>的<code>PCIe</code>访问会增加<code>100-200ns</code>额外延迟</li>
<li><code>DMA(Direct Memory Access)</code>传输带宽可能降低<code>50%</code>以上</li>
</ul>
</li>
<li>
<p><strong>GPU-CPU通信效率</strong>：</p>
<ul>
<li><code>GPU kernel</code>启动、状态查询、寄存器访问等<code>PCIe</code>事务受跨<code>NUMA</code>影响</li>
<li>频繁的<code>CPU-GPU</code>交互会放大跨<code>NUMA</code>开销</li>
<li>影响整体<code>GPU</code>利用率和吞吐量</li>
</ul>
</li>
<li>
<p><strong>数据传输效率</strong>：跨<code>NUMA</code>节点的数据传输会影响<code>GPU</code>与<code>CPU</code>之间的数据交换效率，包括：</p>
<ul>
<li>主机到设备（<code>Host-to-Device</code>）传输</li>
<li>设备到主机（<code>Device-to-Host</code>）传输</li>
<li>统一内存（<code>Unified Memory</code>）的页面迁移</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=numa的更多介绍>NUMA的更多介绍<a href=#numa的更多介绍 class=hash-link aria-label="Direct link to NUMA的更多介绍" title="Direct link to NUMA的更多介绍">​</a></h3>
<p>关于<code>NUMA</code>架构的更多细节，可以参考我的另一篇文章：<a href=/operating-systems-and-networks/cpu-architecture-smp-numa-mpp>CPU处理器架构：SMP、NUMA、MPP</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=cpunuma亲和性的重要性>CPU&NUMA亲和性的重要性<a href=#cpunuma亲和性的重要性 class=hash-link aria-label="Direct link to CPU&NUMA亲和性的重要性" title="Direct link to CPU&NUMA亲和性的重要性">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=cpu亲和性的作用>CPU亲和性的作用<a href=#cpu亲和性的作用 class=hash-link aria-label="Direct link to CPU亲和性的作用" title="Direct link to CPU亲和性的作用">​</a></h3>
<p><code>CPU</code>亲和性（<code>CPU Affinity</code>）是指将进程或线程绑定到特定的<code>CPU</code>核心上运行。在<code>AI</code>工作负载中，合理的<code>CPU</code>亲和性配置可以：</p>
<ol>
<li><strong>减少上下文切换</strong>：避免进程在不同<code>CPU</code>核心间频繁迁移，减少上下文切换开销</li>
<li><strong>提升缓存命中率</strong>：进程固定在特定核心运行，可以更好地利用<code>CPU</code>的<code>L1</code>、<code>L2</code>、<code>L3</code>缓存</li>
<li><strong>降低延迟抖动</strong>：避免进程迁移带来的性能波动，提供更稳定的延迟表现</li>
<li><strong>避免资源竞争</strong>：将不同任务绑定到不同的核心，减少<code>CPU</code>资源争抢</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=numa亲和性的作用>NUMA亲和性的作用<a href=#numa亲和性的作用 class=hash-link aria-label="Direct link to NUMA亲和性的作用" title="Direct link to NUMA亲和性的作用">​</a></h3>
<p><code>NUMA</code>亲和性（<code>NUMA Affinity</code>）是指将进程、内存分配和<code>I/O</code>设备绑定到同一<code>NUMA</code>节点，以<strong>优化内存访问性能</strong>。在<code>AI</code>场景中的价值包括：</p>
<ol>
<li><strong>降低内存访问延迟</strong>：确保进程访问本地内存，避免跨<code>NUMA</code>节点的远程内存访问</li>
<li><strong>提升内存带宽</strong>：本地内存访问可以充分利用<code>NUMA</code>节点的内存带宽</li>
<li><strong>优化GPU-CPU通信</strong>：将<code>GPU</code>、对应的<code>CPU</code>核心和内存绑定到同一<code>NUMA</code>节点，最小化数据传输延迟</li>
<li><strong>提高整体吞吐量</strong>：通过减少跨节点通信，提升系统整体的数据处理能力</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=ai模型训练推理场景中的影响>AI模型训练推理场景中的影响<a href=#ai模型训练推理场景中的影响 class=hash-link aria-label="Direct link to AI模型训练推理场景中的影响" title="Direct link to AI模型训练推理场景中的影响">​</a></h3>
<p>在<code>AI</code>模型训练和推理场景中，不合理的亲和性配置可能导致：</p>
<ul>
<li><strong>训练速度下降</strong>：数据加载、预处理过程中的跨<code>NUMA</code>访问会拖累整体训练速度</li>
<li><strong>推理延迟增加</strong>：推理服务中的请求处理涉及频繁的<code>CPU-GPU</code>交互，跨<code>NUMA</code>访问会增加延迟</li>
<li><strong>吞吐量降低</strong>：在高并发推理场景下，跨<code>NUMA</code>访问会成为系统瓶颈</li>
<li><strong>性能不稳定</strong>：进程迁移和非本地内存访问会导致性能抖动</li>
</ul>
<p>通过合理配置<code>CPU</code>亲和性和<code>NUMA</code>亲和性，通常可以获得<code>10%-30%</code>的性能提升，在某些场景下甚至可以达到<code>50%</code>以上的优化效果。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=cpunuma亲和性示例分析>CPU&NUMA亲和性示例分析<a href=#cpunuma亲和性示例分析 class=hash-link aria-label="Direct link to CPU&NUMA亲和性示例分析" title="Direct link to CPU&NUMA亲和性示例分析">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=理解nvidia-smi-topo命令输出>理解nvidia-smi topo命令输出<a href=#理解nvidia-smi-topo命令输出 class=hash-link aria-label="Direct link to 理解nvidia-smi topo命令输出" title="Direct link to 理解nvidia-smi topo命令输出">​</a></h3>
<p><code>NVIDIA</code>提供了<code>nvidia-smi topo -m</code>命令来查看<code>GPU</code>之间以及<code>GPU</code>与<code>CPU</code>之间的拓扑关系。让我们详细分析提供的拓扑信息：</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-text codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token plain">        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU0     X      PIX     NODE    NODE    SYS     SYS     SYS     SYS     0-31,64-95      0               N/A</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU1    PIX      X      NODE    NODE    SYS     SYS     SYS     SYS     0-31,64-95      0               N/A</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU2    NODE    NODE     X      PIX     SYS     SYS     SYS     SYS     0-31,64-95      0               N/A</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU3    NODE    NODE    PIX      X      SYS     SYS     SYS     SYS     0-31,64-95      0               N/A</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU4    SYS     SYS     SYS     SYS      X      PIX     NODE    NODE    32-63,96-127    1               N/A</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU5    SYS     SYS     SYS     SYS     PIX      X      NODE    NODE    32-63,96-127    1               N/A</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      PIX     32-63,96-127    1               N/A</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU7    SYS     SYS     SYS     SYS     NODE    NODE    PIX      X      32-63,96-127    1               N/A</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=连接类型说明>连接类型说明<a href=#连接类型说明 class=hash-link aria-label="Direct link to 连接类型说明" title="Direct link to 连接类型说明">​</a></h3>
<p>矩阵中的符号表示<code>GPU</code>之间的连接类型，按照性能从高到低排列：</p>
<table><thead><tr><th>连接类型<th>说明<th>典型带宽<th>适用场景<tbody><tr><td><code>X</code><td>自身<td><code>N/A</code><td>同一<code>GPU</code><tr><td><code>NV#</code><td><code>NVLink</code>连接（#表示链接数量）<td><code>300-900 GB/s</code><td>高性能<code>GPU</code>间通信（如<code>NVLink 3.0/4.0</code>）<tr><td><code>PIX</code><td>同一<code>PCIe</code>交换机<br>可能包含<code>NVLink Bridge</code><td><code>PCIe: 16-64 GB/s</code><br><code>NVLink Bridge: 100-200 GB/s</code><td>同<code>PCIe</code>树<code>GPU</code>通信<br>部分消费级<code>GPU</code>使用<code>NVLink Bridge</code>互联<tr><td><code>NODE</code><td>同一<code>NUMA</code>节点，不同<code>PCIe</code>交换机<td><code>16-32 GB/s</code><td>同<code>NUMA</code>节点<code>GPU</code>跨<code>PCIe</code>交换机通信<tr><td><code>SYS</code><td>跨<code>NUMA</code>节点，通过<code>CPU</code>互联<td><code>8-16 GB/s</code><td>跨<code>NUMA</code>节点<code>GPU</code>通信，性能最低</table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=cpu-affinity字段解析>CPU Affinity字段解析<a href=#cpu-affinity字段解析 class=hash-link aria-label="Direct link to CPU Affinity字段解析" title="Direct link to CPU Affinity字段解析">​</a></h3>
<p><code>CPU Affinity</code>列显示了每个<code>GPU</code>关联的最优<code>CPU</code>核心范围：</p>
<ul>
<li>
<p><strong>GPU0-GPU3</strong>: <code>0-31, 64-95</code></p>
<ul>
<li>物理核心：<code>0-31</code></li>
<li>超线程核心：<code>64-95</code></li>
<li>总共<code>32</code>个物理核心，<code>64</code>个逻辑核心（启用超线程）</li>
</ul>
</li>
<li>
<p><strong>GPU4-GPU7</strong>: <code>32-63, 96-127</code></p>
<ul>
<li>物理核心：<code>32-63</code></li>
<li>超线程核心：<code>96-127</code></li>
<li>总共<code>32</code>个物理核心，<code>64</code>个逻辑核心（启用超线程）</li>
</ul>
</li>
</ul>
<p><strong>关键理解</strong>：当进程需要使用特定<code>GPU</code>时，应将其绑定到该<code>GPU</code>的<code>CPU Affinity</code>范围内的核心，以实现最优性能。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=numa-affinity字段解析>NUMA Affinity字段解析<a href=#numa-affinity字段解析 class=hash-link aria-label="Direct link to NUMA Affinity字段解析" title="Direct link to NUMA Affinity字段解析">​</a></h3>
<p><code>NUMA Affinity</code>列显示了每个<code>GPU</code>所属的<code>NUMA</code>节点：</p>
<ul>
<li><strong>GPU0-GPU3</strong>: 属于<code>NUMA Node 0</code></li>
<li><strong>GPU4-GPU7</strong>: 属于<code>NUMA Node 1</code></li>
</ul>
<p>这意味着：</p>
<ul>
<li>使用<code>GPU0-GPU3</code>的任务应绑定到<code>NUMA Node 0</code>的<code>CPU</code>核心和内存</li>
<li>使用<code>GPU4-GPU7</code>的任务应绑定到<code>NUMA Node 1</code>的<code>CPU</code>核心和内存</li>
</ul>
<p><strong>NUMA与PCIe拓扑的关系</strong>：</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-text codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token plain">NUMA Node 0                          NUMA Node 1</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">├── CPU 0-31, 64-95                  ├── CPU 32-63, 96-127</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">├── Local Memory                     ├── Local Memory</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">└── PCIe Root Complex                └── PCIe Root Complex</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    ├── PCIe Switch 0                    ├── PCIe Switch 2</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    │   ├── GPU0                         │   ├── GPU4</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    │   └── GPU1                         │   └── GPU5</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">    └── PCIe Switch 1                    └── PCIe Switch 3</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        ├── GPU2                             ├── GPU6</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">        └── GPU3                             └── GPU7</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<p><strong>关键理解</strong>：</p>
<ul>
<li>每个<code>GPU</code>物理连接到特定<code>NUMA</code>节点的<code>PCIe</code>总线</li>
<li><code>CPU</code>访问非本地<code>NUMA</code>节点的<code>GPU</code>需要经过<code>NUMA</code>互联（<code>QPI/UPI</code>）</li>
<li>跨<code>NUMA</code>的<code>PCIe</code>访问会显著增加延迟并降低带宽</li>
<li>最佳性能需要<code>CPU</code>、内存、<code>GPU</code>都在同一<code>NUMA</code>节点</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=gpu-numa-id字段解析>GPU NUMA ID字段解析<a href=#gpu-numa-id字段解析 class=hash-link aria-label="Direct link to GPU NUMA ID字段解析" title="Direct link to GPU NUMA ID字段解析">​</a></h3>
<p><code>GPU NUMA ID</code>列在支持的平台上显示<code>GPU</code>设备本身的<code>NUMA</code>节点<code>ID</code>。在上述示例中，所有<code>GPU</code>的<code>GPU NUMA ID</code>都显示为<code>N/A</code>，这表示：</p>
<p><strong><code>N/A</code>的含义</strong>：</p>
<ul>
<li><code>GPU</code>没有自己的独立<code>NUMA</code>节点</li>
<li><code>GPU</code>作为<code>PCIe</code>设备挂载在<code>CPU</code>的<code>NUMA</code>节点下</li>
<li>应该参考<code>NUMA Affinity</code>列来确定<code>GPU</code>所属的<code>NUMA</code>节点</li>
</ul>
<p><strong>何时会显示具体的NUMA ID</strong>：
在某些高端系统架构中（如<code>AMD Infinity Fabric</code>架构或<code>NVIDIA Grace Hopper Superchip</code>），<code>GPU</code>可能拥有自己的<code>NUMA</code>节点：</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-text codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token plain">        GPU0    ...    CPU Affinity    NUMA Affinity   GPU NUMA ID</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU0     X      ...    0-31,64-95      0               2</span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">GPU1    PIX     ...    0-31,64-95      0               3</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<p>在这种情况下：</p>
<ul>
<li><code>NUMA Affinity: 0</code>表示<code>GPU</code>物理连接在<code>NUMA Node 0</code>的<code>PCIe</code>总线上</li>
<li><code>GPU NUMA ID: 2</code>表示<code>GPU</code>本身被系统识别为<code>NUMA Node 2</code></li>
<li>这种架构下，<code>GPU</code>内存（显存）可以作为独立的<code>NUMA</code>节点被<code>CPU</code>直接访问</li>
<li>支持<code>CPU</code>直接访问<code>GPU</code>内存，实现更高效的异构内存管理</li>
</ul>
<p><strong>对配置的影响</strong>：</p>
<ul>
<li><strong>N/A场景</strong>（最常见）：按照<code>NUMA Affinity</code>列配置<code>CPU</code>和内存亲和性即可</li>
<li><strong>独立NUMA ID场景</strong>：需要考虑<code>GPU</code>显存作为额外<code>NUMA</code>节点的影响，可能需要配置更复杂的内存策略（如允许访问<code>GPU NUMA</code>节点）</li>
</ul>
<p><strong>验证命令</strong>：</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style=--prism-color:#f8f8f2;--prism-background-color:#272822><div class=codeBlockContent_biex><pre tabindex=0 class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style=color:#f8f8f2;background-color:#272822><code class=codeBlockLines_e6Vv><span class=token-line style=color:#f8f8f2><span class="token comment" style=color:#8292a2;font-style:italic># 查看系统所有NUMA节点</span><span class="token plain"></span><br></span><span class=token-line style=color:#f8f8f2><span class="token plain">numactl </span><span class="token parameter variable" style=color:#f8f8f2>--hardware</span><br></span></code></pre><div class=buttonGroup__atx><button type=button aria-label="Copy code to clipboard" title=Copy class=clean-btn><span class=copyButtonIcons_eSgA aria-hidden=true><svg viewBox="0 0 24 24" class=copyButtonIcon_y97N><path fill=currentColor d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"/></svg><svg viewBox="0 0 24 24" class=copyButtonSuccessIcon_LjdS><path fill=currentColor d=M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z /></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=拓扑分析总结>拓扑分析总结<a href=#拓扑分析总结 class=hash-link aria-label="Direct link to 拓扑分析总结" title="Direct link to 拓扑分析总结">​</a></h3>
<p>从上述拓扑信息可以得出以下结论：</p>
<ol>
<li>
<p><strong>GPU分组</strong>：</p>
<ul>
<li>组1：<code>GPU0-GPU3</code>（<code>NUMA Node 0</code>）</li>
<li>组2：<code>GPU4-GPU7</code>（<code>NUMA Node 1</code>）</li>
</ul>
</li>
<li>
<p><strong>GPU间连接模式</strong>：</p>
<ul>
<li><code>GPU0-GPU1</code>通过<code>PIX</code>连接（同<code>PCIe</code>交换机）</li>
<li><code>GPU2-GPU3</code>通过<code>PIX</code>连接（同<code>PCIe</code>交换机）</li>
<li><code>GPU0/1</code>与<code>GPU2/3</code>通过<code>NODE</code>连接（同<code>NUMA</code>节点，不同<code>PCIe</code>交换机）</li>
<li>跨<code>NUMA</code>节点的<code>GPU</code>通过<code>SYS</code>连接（性能最低）</li>
</ul>
</li>
<li>
<p><strong>最佳实践</strong>：</p>
<ul>
<li>单卡任务：使用任意一张<code>GPU</code>，绑定到对应的<code>CPU</code>核心和<code>NUMA</code>节点</li>
<li>2卡任务：优先选择<code>GPU0-GPU1</code>或<code>GPU2-GPU3</code>等<code>PIX</code>连接的<code>GPU</code>对</li>
<li>4卡任务：选择<code>GPU0-GPU3</code>或<code>GPU4-GPU7</code>（同<code>NUMA</code>节点，性能最优）</li>
<li>5卡任务：例如<code>GPU0-GPU4</code>，会跨<code>NUMA</code>节点，<code>GPU4</code>与<code>GPU0-3</code>通信性能较低（<code>SYS</code>连接）</li>
<li>跨<code>NUMA</code>任务：尽量选择<code>4+4</code>的组合（如<code>GPU0-3</code> + <code>GPU4-7</code>在不同节点），避免<code>5</code>卡这种不均衡配置</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=docker中的cpunuma亲和性配置>Docker中的CPU&NUMA亲和性配置<a href=#docker中的cpunuma亲和性配置 class=hash-link aria-label="Direct link to Docker中的CPU&NUMA亲和性配置" title="Direct link to Docker中的CPU&NUMA亲和性配置">​</a></h2>
<p>参考章节：<a href=/ai/cpu-numa-affinity-in-docker>Docker中的CPU&NUMA亲和性配置</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=kubernetes中cpunuma亲和性配置>Kubernetes中CPU&NUMA亲和性配置<a href=#kubernetes中cpunuma亲和性配置 class=hash-link aria-label="Direct link to Kubernetes中CPU&NUMA亲和性配置" title="Direct link to Kubernetes中CPU&NUMA亲和性配置">​</a></h2>
<p>参考章节：<a href=/cloud-native/kubernetes-cpu-numa-affinity>Kubernetes CPU&NUMA亲和性调度调度</a></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/ai/hybrid-scheduling-scale-down-control><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>混部调度中如何控制在线服务的缩容逻辑</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/ai/cpu-numa-affinity-in-docker><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>Docker中的CPU&NUMA亲和性配置</div></a></nav><div class=docusaurus-mt-lg></div></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#引言 class="table-of-contents__link toc-highlight">引言</a><li><a href=#numa架构简介 class="table-of-contents__link toc-highlight">NUMA架构简介</a><ul><li><a href=#什么是numa class="table-of-contents__link toc-highlight">什么是NUMA</a><li><a href=#numa架构的层级结构 class="table-of-contents__link toc-highlight">NUMA架构的层级结构</a><li><a href=#numa对性能的影响 class="table-of-contents__link toc-highlight">NUMA对性能的影响</a><li><a href=#numa的更多介绍 class="table-of-contents__link toc-highlight">NUMA的更多介绍</a></ul><li><a href=#cpunuma亲和性的重要性 class="table-of-contents__link toc-highlight">CPU&NUMA亲和性的重要性</a><ul><li><a href=#cpu亲和性的作用 class="table-of-contents__link toc-highlight">CPU亲和性的作用</a><li><a href=#numa亲和性的作用 class="table-of-contents__link toc-highlight">NUMA亲和性的作用</a><li><a href=#ai模型训练推理场景中的影响 class="table-of-contents__link toc-highlight">AI模型训练推理场景中的影响</a></ul><li><a href=#cpunuma亲和性示例分析 class="table-of-contents__link toc-highlight">CPU&NUMA亲和性示例分析</a><ul><li><a href=#理解nvidia-smi-topo命令输出 class="table-of-contents__link toc-highlight">理解nvidia-smi topo命令输出</a><li><a href=#连接类型说明 class="table-of-contents__link toc-highlight">连接类型说明</a><li><a href=#cpu-affinity字段解析 class="table-of-contents__link toc-highlight">CPU Affinity字段解析</a><li><a href=#numa-affinity字段解析 class="table-of-contents__link toc-highlight">NUMA Affinity字段解析</a><li><a href=#gpu-numa-id字段解析 class="table-of-contents__link toc-highlight">GPU NUMA ID字段解析</a><li><a href=#拓扑分析总结 class="table-of-contents__link toc-highlight">拓扑分析总结</a></ul><li><a href=#docker中的cpunuma亲和性配置 class="table-of-contents__link toc-highlight">Docker中的CPU&NUMA亲和性配置</a><li><a href=#kubernetes中cpunuma亲和性配置 class="table-of-contents__link toc-highlight">Kubernetes中CPU&NUMA亲和性配置</a></ul></div></div></div></div></main></div></div></div><footer class=footer><div class="container container-fluid"><div class="footer__bottom text--center"><div class=footer__copyright>Copyright 2025 johng.cn</div></div></div></footer></div>