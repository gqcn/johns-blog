<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-docs/AI技术/分布式推理/NVIDIA Dynamo: 分布式AI推理的高效引擎" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.6.3"><title data-rh=true>NVIDIA Dynamo: 分布式AI推理的高效引擎 | John's Blog</title><meta data-rh=true name=viewport content="width=device-width,initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:url content=https://johng.cn/ai/nvidia-dynamo><meta data-rh=true property=og:locale content=en><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-default-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-default-current><meta data-rh=true property=og:title content="NVIDIA Dynamo: 分布式AI推理的高效引擎 | John's Blog"><meta data-rh=true name=description content="介绍NVIDIA Dynamo，一个分布式AI推理的高效引擎，分析其架构设计、组件功能及工作原理，了解如何通过PD分离、智能路由和分布式KV缓存管理等技术提升大模型推理性能"><meta data-rh=true property=og:description content="介绍NVIDIA Dynamo，一个分布式AI推理的高效引擎，分析其架构设计、组件功能及工作原理，了解如何通过PD分离、智能路由和分布式KV缓存管理等技术提升大模型推理性能"><meta data-rh=true name=keywords content="NVIDIA Dynamo,分布式AI推理,大语言模型,LLM推理,推理优化,PD分离,KV缓存,推理引擎,高吞吐量,低延迟,NIXL,分布式KV缓存,智能路由,GPU调度,TensorRT-LLM,vLLM,SGLang"><link data-rh=true rel=icon href=/img/favicon.ico><link data-rh=true rel=canonical href=https://johng.cn/ai/nvidia-dynamo><link data-rh=true rel=alternate href=https://johng.cn/ai/nvidia-dynamo hreflang=en><link data-rh=true rel=alternate href=https://johng.cn/ai/nvidia-dynamo hreflang=x-default><link data-rh=true rel=preconnect href=https://XGS1CPQERK-dsn.algolia.net crossorigin><link rel=alternate type=application/rss+xml href=/blog/rss.xml title="John's Blog RSS Feed"><link rel=alternate type=application/atom+xml href=/blog/atom.xml title="John's Blog Atom Feed"><link rel=search type=application/opensearchdescription+xml title="John's Blog" href=/opensearch.xml><script src=https://hm.baidu.com/hm.js?6b4ae23dc83ee5efe875b7172af6c7c1 async></script><link rel=stylesheet href=/assets/css/styles.1f745325.css><script src=/assets/js/runtime~main.c30d173f.js defer></script><script src=/assets/js/main.66589fc4.js defer></script><body class=navigation-with-keyboard><script>!function(){var t,e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t=null!==e?e:"light",document.documentElement.setAttribute("data-theme",t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/><b class="navbar__title text--truncate">John's Blog</b></a><a aria-current=page class="navbar__item navbar__link navbar__link--active" sidebarid=mainSidebar href=/ai>AI技术</a><a class="navbar__item navbar__link" sidebarid=mainSidebar href=/cloud-native>云原生</a><a class="navbar__item navbar__link" sidebarid=mainSidebar href=/notes>日常笔记</a><a class="navbar__item navbar__link" sidebarid=mainSidebar href=/programming>开发语言</a><a class="navbar__item navbar__link" sidebarid=mainSidebar href=/architecture>技术架构</a><a class="navbar__item navbar__link" sidebarid=mainSidebar href=/observability>可观测性</a><a class="navbar__item navbar__link" sidebarid=mainSidebar href=/database-and-middleware>数据库与中间件</a><a class="navbar__item navbar__link" href=/aboutme>关于我</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href=/blog>博客</a><a href=https://goframe.org/ target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-goframe-link"></a><a href=https://github.com/gqcn target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-github-link"></a><div class=navbarSearchContainer_Bca1><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_z2l0"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class=sidebar_njMd><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/ai>AI技术</a><button aria-label="Collapse sidebar category 'AI技术'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/nvidia>NVIDIA</a><button aria-label="Expand sidebar category 'NVIDIA'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" tabindex=0 href=/ai/distributed-inference>分布式推理</a><button aria-label="Collapse sidebar category '分布式推理'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/ai/pd-separation>PD(Prefill&Decode)分离</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/ai/nvidia-dynamo>NVIDIA Dynamo: 分布式AI推理的高效引擎</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/ai/single-node-multi-gpu-nvlink-communication-issue>单机多卡部署，GPU之间无法使用NVLINK通信问题排查</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/scheduling>算力资源调度</a><button aria-label="Expand sidebar category '算力资源调度'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/rdma>RDMA</a><button aria-label="Expand sidebar category 'RDMA'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ai/common-acceleration-cards>常见智算加速卡汇总</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ai/general-ai-model-and-reasoning-ai-model-difference>通用大模型和推理大模型区别</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ai/common-ai-model-training-inference-framework>常见AI模型训练推理框架对比</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class=menu__link tabindex=0 href=/ai/ai-training-inference-scenarios>AI模型训练推理常见业务场景痛点</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/cloud-native>云原生</a><button aria-label="Expand sidebar category '云原生'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/notes>日常笔记</a><button aria-label="Expand sidebar category '日常笔记'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/programming>开发语言</a><button aria-label="Expand sidebar category '开发语言'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/architecture>技术架构</a><button aria-label="Expand sidebar category '技术架构'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/management>技术管理</a><button aria-label="Expand sidebar category '技术管理'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/observability>可观测性</a><button aria-label="Expand sidebar category '可观测性'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/database-and-middleware>数据库与中间件</a><button aria-label="Expand sidebar category '数据库与中间件'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/operating-systems-and-networks>操作系统和网络</a><button aria-label="Expand sidebar category '操作系统和网络'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div></ul></nav></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/ai><span itemprop=name>AI技术</span></a><meta itemprop=position content=1><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/ai/distributed-inference><span itemprop=name>分布式推理</span></a><meta itemprop=position content=2><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link itemprop=name>NVIDIA Dynamo: 分布式AI推理的高效引擎</span><meta itemprop=position content=3></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id=1-背景与用途>1. 背景与用途<a href=#1-背景与用途 class=hash-link aria-label="Direct link to 1. 背景与用途" title="Direct link to 1. 背景与用途">​</a></h2>
<p><code>NVIDIA Dynamo</code>是一个高吞吐量、低延迟的推理框架，专为在多节点分布式环境中部署生成式AI和推理模型而设计。随着AI模型规模的急剧增长（近年来开源模型大小增长了近<code>2000</code>倍），以及这些模型越来越多地集成到需要与多个其他模型交互的智能体（<code>Agent</code>）工作流中，传统的推理服务架构面临着巨大挑战。</p>
<p>在大规模生产环境中部署大型语言模型（<code>LLM</code>）和推理模型时，开发者面临以下关键挑战：</p>
<ol>
<li>
<p><strong>资源利用效率低下</strong>：传统LLM部署将推理的预填充（<code>Prefill</code>）和解码（<code>Decode</code>）阶段放在同一个<code>GPU</code>或节点上，尽管这两个阶段有着不同的资源需求，导致<code>GPU</code>资源利用率不高。</p>
</li>
<li>
<p><strong>分布式协调复杂</strong>：当模型需要分布在多个节点上时，需要精心的编排和协调，特别是当引入新的分布式推理优化方法（如分离式服务）时，这种复杂性会进一步增加。</p>
</li>
<li>
<p><strong>KV缓存管理困难</strong>：随着AI需求的增加，需要在<code>GPU</code>内存中存储的<code>KV</code>缓存量迅速增长，这会导致成本急剧上升。</p>
</li>
<li>
<p><strong>通信开销大</strong>：大规模分布式推理依赖于节点间和节点内的低延迟、高吞吐量通信，这对网络架构提出了很高要求。</p>
</li>
<li>
<p><strong>动态负载均衡</strong>：在多模型AI管道中，需要能够动态分配<code>GPU</code>资源以响应波动的用户需求并处理流量瓶颈。</p>
</li>
</ol>
<p><code>NVIDIA Dynamo</code>正是为解决这些挑战而设计的，它通过分离式服务、智能路由、分布式<code>KV</code>缓存管理和优化的数据传输等创新技术，显著提高了推理性能和资源利用效率。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=2-架构设计>2. 架构设计<a href=#2-架构设计 class=hash-link aria-label="Direct link to 2. 架构设计" title="Direct link to 2. 架构设计">​</a></h2>
<p><code>NVIDIA Dynamo</code>采用模块化架构设计，旨在为分布式环境中的生成式AI模型提供高效的推理服务。它支持所有主要的LLM框架，包括<code>NVIDIA TensorRT-LLM</code>、<code>vLLM</code>和<code>SGLang</code>，并整合了最先进的<code>LLM</code>推理服务优化技术。</p>
<p><img decoding=async loading=lazy alt=Dynamo架构图 src=/assets/images/image-9b7803052b1b5cf6d0bdbd8aef54d28a.png width=1430 height=809 class=img_ev3q></p>
<p><code>NVIDIA Dynamo</code>包含多项关键特性，使其能够实现大规模分布式和分离式推理服务。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=3-核心技术与工作原理>3. 核心技术与工作原理<a href=#3-核心技术与工作原理 class=hash-link aria-label="Direct link to 3. 核心技术与工作原理" title="Direct link to 3. 核心技术与工作原理">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=31-分离式服务disaggregated-serving>3.1 分离式服务（Disaggregated Serving）<a href=#31-分离式服务disaggregated-serving class=hash-link aria-label="Direct link to 3.1 分离式服务（Disaggregated Serving）" title="Direct link to 3.1 分离式服务（Disaggregated Serving）">​</a></h3>
<p><img decoding=async loading=lazy alt="alt text" src=/assets/images/image-6-bc8267d8dbb71fff885e56c0b7e76cf3.png width=1999 height=962 class=img_ev3q></p>
<p><img decoding=async loading=lazy alt="alt text" src=/assets/images/image-1-92a9fdb33c1f84d4fdc0c7730f67653d.png width=1777 height=698 class=img_ev3q></p>
<p>传统的<code>LLM</code>部署将推理的预填充（<code>Prefill</code>）和解码（<code>Decode</code>）阶段放在同一个<code>GPU</code>或节点上，这种方法阻碍了性能优化，无法充分利用<code>GPU</code>资源：</p>
<ul>
<li><strong>预填充阶段</strong>：处理用户输入以生成第一个输出标记，受计算能力限制</li>
<li><strong>解码阶段</strong>：生成后续标记，受内存带宽限制</li>
</ul>
<p>将这两个阶段放在同一个<code>GPU</code>上会导致资源使用效率低下，特别是对于长输入序列。分离式服务将预填充和解码阶段分离到不同的<code>GPU</code>或节点上，使开发者能够独立优化每个阶段，为每个阶段应用不同的模型并行策略并分配不同的<code>GPU</code>设备。</p>
<p>在<code>NVIDIA GB200 NVL72</code>上服务开源<code>DeepSeek-R1</code>模型时，使用分离式服务的<code>NVIDIA Dynamo</code>将处理的请求数量提高了30倍。在<code>NVIDIA Hopper</code>上服务<code>Llama 70B</code>模型时，吞吐量性能提高了一倍以上。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=32-nvidia-dynamo-规划器优化分布式推理中的gpu资源>3.2 NVIDIA Dynamo 规划器：优化分布式推理中的GPU资源<a href=#32-nvidia-dynamo-规划器优化分布式推理中的gpu资源 class=hash-link aria-label="Direct link to 3.2 NVIDIA Dynamo 规划器：优化分布式推理中的GPU资源" title="Direct link to 3.2 NVIDIA Dynamo 规划器：优化分布式推理中的GPU资源">​</a></h3>
<p><img decoding=async loading=lazy alt="Dynamo 规划器工作流程" src=/assets/images/image-2-149281da83407f2e81c713a88dcdeb5e.png width=1080 height=534 class=img_ev3q></p>
<p>在大规模分布式和解耦式推理系统中，高效管理<code>GPU</code>资源是提升吞吐量和降低延迟的关键。虽然解耦式服务能显著提高推理吞吐量和效率，但并非所有请求都适合这种方案。</p>
<p>设想一个场景：大量需要长输入序列长度（<code>ISL</code>）但短输出序列长度（<code>OSL</code>）的摘要请求突然涌入，导致预填充<code>GPU</code>过载。此时，解码<code>GPU</code>可能处于闲置状态，而预填充<code>GPU</code>成为瓶颈。此时，允许解码<code>GPU</code>以传统聚合方式同时执行预填充和解码任务，或调整解码<code>GPU</code>执行预填充任务，可能更高效。这种策略可平衡负载、缓解预填充<code>GPU</code>压力并提升整体吞吐量。</p>
<p>决定采用解耦式或聚合式服务，或分配多少<code>GPU</code>到各阶段，需综合考量多个因素。这些因素包括预填充与解码<code>GPU</code>间KV缓存传输所需时间、<code>GPU</code>队列等待时间，以及解耦式和聚合式配置的预估处理时间。在数百<code>GPU</code>的大规模环境中，这些决策会迅速变得复杂。</p>
<p>此时，<code>NVIDIA Dynamo 规划器</code>发挥作用。它持续监控分布式推理环境中<code>GPU</code>的容量指标，并结合应用服务等级目标（<code>SLO</code>）如<code>TTFT</code>和<code>ITL</code>，判断是否应以分散或聚合方式处理新请求，或是否需向各阶段分配更多<code>GPU</code>。<code>NVIDIA Dynamo 规划器</code>确保预填充和解码阶段的<code>GPU</code>资源高效分配，适应波动负载的同时保持系统峰值性能。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=33-nvidia-dynamo-智能路由减少kv缓存的高成本重新计算>3.3 NVIDIA Dynamo 智能路由：减少KV缓存的高成本重新计算<a href=#33-nvidia-dynamo-智能路由减少kv缓存的高成本重新计算 class=hash-link aria-label="Direct link to 3.3 NVIDIA Dynamo 智能路由：减少KV缓存的高成本重新计算" title="Direct link to 3.3 NVIDIA Dynamo 智能路由：减少KV缓存的高成本重新计算">​</a></h3>
<p><img decoding=async loading=lazy alt="Dynamo Smart Router工作原理" src=/assets/images/image-4-282ce4fcee78050a6ff2230859cc89a0.png width=1080 height=577 class=img_ev3q></p>
<p>在响应用户提示前，<code>LLM</code>需构建输入请求的上下文理解，即<code>KV</code>缓存。这一过程计算密集且随输入请求大小呈二次增长。复用<code>KV</code>缓存可避免从头计算，减少推理时间和计算资源消耗。这对频繁执行相同请求的场景（如系统提示、单用户多轮聊天机器人交互、代理工作流）尤为有利。为此需要高效的数据管理机制，判断何时何地可复用<code>KV</code>缓存。</p>
<p><code>NVIDIA Dynamo</code>智能路由跨多节点和解耦式部署的大规模<code>GPU</code>集群追踪<code>KV</code>缓存，智能路由新请求，最大限度减少其重新计算。它通过<code>Radix Tree</code>哈希存储请求，实现在分布式环境中追踪<code>KV</code>位置。同时采用专用算法管理<code>KV</code>缓存的插入和淘汰，确保保留最相关数据块。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=34-nvidia-dynamo-分布式kv缓存管理器将kv缓存卸载至成本效益更高的存储>3.4 NVIDIA Dynamo 分布式KV缓存管理器：将KV缓存卸载至成本效益更高的存储<a href=#34-nvidia-dynamo-分布式kv缓存管理器将kv缓存卸载至成本效益更高的存储 class=hash-link aria-label="Direct link to 3.4 NVIDIA Dynamo 分布式KV缓存管理器：将KV缓存卸载至成本效益更高的存储" title="Direct link to 3.4 NVIDIA Dynamo 分布式KV缓存管理器：将KV缓存卸载至成本效益更高的存储">​</a></h3>
<p>构建用户请求的<code>KV</code>缓存资源密集且成本高昂。复用<code>KV</code>缓存以减少重新计算是常见做法。但随着AI需求增长，需存储在<code>GPU</code>内存中供复用的<code>KV</code>缓存量可能迅速超出预算。这对试图高效管理<code>KV</code>缓存复用的AI推理团队构成重大挑战。</p>
<p><code>NVIDIA Dynamo</code>分布式<code>KV</code>缓存管理器通过将较旧或访问频率较低的<code>KV</code>缓存块卸载到成本更低的存储（如<code>CPU</code>主机内存、本地存储或网络对象存储），解决了这一问题。该功能使组织能以<code>GPU</code>内存成本的极小部分存储<code>PB</code>级<code>KV</code>缓存数据。通过将<code>KV</code>缓存卸载至不同存储层级，开发者可释放宝贵<code>GPU</code>资源，同时保留历史<code>KV</code>缓存以减少推理计算成本。</p>
<p><img decoding=async loading=lazy alt="Dynamo KV Cache Manager架构" src=/assets/images/image-3-ccb9e7eaca7fc4ebfb28c98a114d6fb7.png width=936 height=596 class=img_ev3q></p>
<p><code>NVIDIA Dynamo</code>分布式<code>KV</code>缓存管理器采用先进缓存策略，优先将高频访问数据保留在<code>GPU</code>内存，低频数据迁移至共享<code>CPU</code>主机内存、SSD或网络对象存储。其智能淘汰策略平衡过度缓存（引发查找延迟）与缓存不足（导致查找失败和<code>KV</code>缓存重新计算）的问题。</p>
<p>此外，该功能支持跨多GPU节点管理KV缓存，适用于分布式和解耦式推理服务，并提供分层缓存能力，在GPU、节点和集群层级制定卸载策略。</p>
<p><code>NVIDIA Dynamo</code>分布式<code>KV</code>缓存管理器与<code>PyTorch</code>、<code>SGLang</code>、<code>TensorRT-LLM</code>和<code>vLLM</code>等后端兼容，支持通过<code>NVIDIA NVLink</code>、<code>NVIDIA Quantum</code>交换机和<code>NVIDIA Spectrum</code>交换机扩展大规模分布式集群的<code>KV</code>缓存存储。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=35-nvidia推理传输库nixl低延迟硬件无关的通信>3.5 NVIDIA推理传输库（NIXL）：低延迟、硬件无关的通信<a href=#35-nvidia推理传输库nixl低延迟硬件无关的通信 class=hash-link aria-label="Direct link to 3.5 NVIDIA推理传输库（NIXL）：低延迟、硬件无关的通信" title="Direct link to 3.5 NVIDIA推理传输库（NIXL）：低延迟、硬件无关的通信">​</a></h3>
<p>大规模分布式推理依赖张量、流水线和专家并行等模型并行技术，需跨节点和节点内低延迟、高吞吐通信，利用<code>GPUDirect RDMA</code>。这些系统还需在解耦式服务环境中快速传输预填充与解码<code>GPU</code>工作者间的<code>KV</code>缓存。</p>
<p>此外，它们需支持硬件和网络无关的加速通信库，可高效跨<code>GPU</code>和存储层级（如<code>CPU</code>内存、块、文件及对象存储）移动数据，并兼容多种网络协议。</p>
<p><img decoding=async loading=lazy alt=NVIDIA推理传输库（NIXL）抽象了异构存储设备间的数据移动复杂性 src=/assets/images/image-5-7b15fd20a9f7c6f54995deb91f564bd1.png width=909 height=456 class=img_ev3q></p>
<p><code>NVIDIA</code>推理传输库（<code>NIXL</code>）是高性能、低延迟的点对点通信库，提供一致的数据移动<code>API</code>，利用相同语义快速异步跨不同存储层级移动数据。其专为推理数据移动优化，支持非阻塞和非连续数据传输。</p>
<p><code>NIXL</code>支持异构数据路径和多种存储类型（包括本地<code>SSD</code>），以及<code>NVIDIA</code>存储合作伙伴的网络化存储。</p>
<p><code>NIXL</code>使<code>NVIDIA Dynamo</code>能通过统一<code>API</code>与<code>GPUDirect</code>存储、<code>UCX</code>和<code>S3</code>等通信库交互，无论传输通过<code>NVLink</code>（<code>C2C</code>或<code>NVSwitch</code>）、<code>InfiniBand</code>、<code>RoCE</code>还是<code>以太网</code>。结合<code>NVIDIA Dynamo</code>策略引擎，<code>NIXL</code>自动选择最佳后端连接，并抽象不同存储类型的差异。这通过通用“内存区”实现，可为<code>HBM</code>、<code>DRAM</code>、<code>本地SSD</code>或<code>网络化存储</code>（块、对象或文件存储）。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=4-参考资料>4. 参考资料<a href=#4-参考资料 class=hash-link aria-label="Direct link to 4. 参考资料" title="Direct link to 4. 参考资料">​</a></h2>
<ol>
<li><a href=https://github.com/ai-dynamo/dynamo target=_blank rel="noopener noreferrer">https://github.com/ai-dynamo/dynamo</a></li>
<li><a href=https://cloud.tencent.com/developer/article/2508888 target=_blank rel="noopener noreferrer">https://cloud.tencent.com/developer/article/2508888</a></li>
<li><a href=https://developer.nvidia.com/blog/introducing-nvidia-dynamo-a-low-latency-distributed-inference-framework-for-scaling-reasoning-ai-models/ target=_blank rel="noopener noreferrer">https://developer.nvidia.com/blog/introducing-nvidia-dynamo-a-low-latency-distributed-inference-framework-for-scaling-reasoning-ai-models/</a></li>
</ol></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/ai/pd-separation><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>PD(Prefill&Decode)分离</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/ai/single-node-multi-gpu-nvlink-communication-issue><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>单机多卡部署，GPU之间无法使用NVLINK通信问题排查</div></a></nav><div class=docusaurus-mt-lg></div></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#1-背景与用途 class="table-of-contents__link toc-highlight">1. 背景与用途</a><li><a href=#2-架构设计 class="table-of-contents__link toc-highlight">2. 架构设计</a><li><a href=#3-核心技术与工作原理 class="table-of-contents__link toc-highlight">3. 核心技术与工作原理</a><ul><li><a href=#31-分离式服务disaggregated-serving class="table-of-contents__link toc-highlight">3.1 分离式服务（Disaggregated Serving）</a><li><a href=#32-nvidia-dynamo-规划器优化分布式推理中的gpu资源 class="table-of-contents__link toc-highlight">3.2 NVIDIA Dynamo 规划器：优化分布式推理中的GPU资源</a><li><a href=#33-nvidia-dynamo-智能路由减少kv缓存的高成本重新计算 class="table-of-contents__link toc-highlight">3.3 NVIDIA Dynamo 智能路由：减少KV缓存的高成本重新计算</a><li><a href=#34-nvidia-dynamo-分布式kv缓存管理器将kv缓存卸载至成本效益更高的存储 class="table-of-contents__link toc-highlight">3.4 NVIDIA Dynamo 分布式KV缓存管理器：将KV缓存卸载至成本效益更高的存储</a><li><a href=#35-nvidia推理传输库nixl低延迟硬件无关的通信 class="table-of-contents__link toc-highlight">3.5 NVIDIA推理传输库（NIXL）：低延迟、硬件无关的通信</a></ul><li><a href=#4-参考资料 class="table-of-contents__link toc-highlight">4. 参考资料</a></ul></div></div></div></div></main></div></div></div><footer class=footer><div class="container container-fluid"><div class="footer__bottom text--center"><div class=footer__copyright>Copyright 2025 johng.cn</div></div></div></footer></div>