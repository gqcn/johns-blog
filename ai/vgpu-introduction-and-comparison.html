<!doctype html><html lang=en dir=ltr class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-docs/AI技术/基础架构/vGPU/vGPU介绍及主流方案对比" data-has-hydrated=false><meta charset=UTF-8><meta name=generator content="Docusaurus v3.6.3"><title data-rh=true>vGPU介绍及主流方案对比 | John's Blog</title><meta data-rh=true name=viewport content="width=device-width,initial-scale=1.0"><meta data-rh=true name=twitter:card content=summary_large_image><meta data-rh=true property=og:url content=https://johng.cn/ai/vgpu-introduction-and-comparison><meta data-rh=true property=og:locale content=en><meta data-rh=true name=docusaurus_locale content=en><meta data-rh=true name=docsearch:language content=en><meta data-rh=true name=author content="John Guo"><meta data-rh=true name=docusaurus_version content=current><meta data-rh=true name=docusaurus_tag content=docs-default-current><meta data-rh=true name=docsearch:version content=current><meta data-rh=true name=docsearch:docusaurus_tag content=docs-default-current><meta data-rh=true property=og:title content="vGPU介绍及主流方案对比 | John's Blog"><meta data-rh=true name=description content="全面介绍vGPU技术原理与核心价值,深入对比NVIDIA MPS、NVIDIA MIG、HAMi等主流GPU虚拟化方案,从隔离性、性能、易用性、兼容性等多维度分析用户态、内核态和硬件层虚拟化方案的优劣,为AI算力资源管理提供技术选型参考。"><meta data-rh=true property=og:description content="全面介绍vGPU技术原理与核心价值,深入对比NVIDIA MPS、NVIDIA MIG、HAMi等主流GPU虚拟化方案,从隔离性、性能、易用性、兼容性等多维度分析用户态、内核态和硬件层虚拟化方案的优劣,为AI算力资源管理提供技术选型参考。"><meta data-rh=true name=keywords content="vGPU,GPU虚拟化,GPU共享,NVIDIA MPS,NVIDIA MIG,HAMi,用户态虚拟化,内核态虚拟化,硬件虚拟化,GPU资源隔离,显存隔离,算力隔离,Kubernetes GPU,云原生AI"><meta data-rh=true property=og:image content=https://johng.cn/og-images/gpu.png><meta data-rh=true name=twitter:image content=https://johng.cn/og-images/gpu.png><link data-rh=true rel=icon href=/img/favicon.ico><link data-rh=true rel=canonical href=https://johng.cn/ai/vgpu-introduction-and-comparison><link data-rh=true rel=alternate href=https://johng.cn/ai/vgpu-introduction-and-comparison hreflang=en><link data-rh=true rel=alternate href=https://johng.cn/ai/vgpu-introduction-and-comparison hreflang=x-default><link data-rh=true rel=preconnect href=https://XGS1CPQERK-dsn.algolia.net crossorigin><link rel=alternate type=application/rss+xml href=/blog/rss.xml title="John's Blog RSS Feed"><link rel=alternate type=application/atom+xml href=/blog/atom.xml title="John's Blog Atom Feed"><link rel=search type=application/opensearchdescription+xml title="John's Blog" href=/opensearch.xml><script src=https://hm.baidu.com/hm.js?6b4ae23dc83ee5efe875b7172af6c7c1 async></script><link rel=stylesheet href=/assets/css/styles.2f56d3c4.css><script src=/assets/js/runtime~main.2185099a.js defer></script><script src=/assets/js/main.139bc18f.js defer></script><body class=navigation-with-keyboard><script>!function(){var t,e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t=null!==e?e:"light",document.documentElement.setAttribute("data-theme",t)}(),function(){try{for(var[t,e]of new URLSearchParams(window.location.search).entries())if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id=__docusaurus><div role=region aria-label="Skip to main content"><a class=skipToContent_fXgn href=#__docusaurus_skipToContent_fallback>Skip to main content</a></div><nav aria-label=Main class="navbar navbar--fixed-top"><div class=navbar__inner><div class=navbar__items><button aria-label="Toggle navigation bar" aria-expanded=false class="navbar__toggle clean-btn" type=button><svg width=30 height=30 viewBox="0 0 30 30" aria-hidden=true><path stroke=currentColor stroke-linecap=round stroke-miterlimit=10 stroke-width=2 d="M4 7h22M4 15h22M4 23h22"/></svg></button><a class=navbar__brand href=/><b class="navbar__title text--truncate">John's Blog</b></a><a aria-current=page class="navbar__item navbar__link navbar__link--active" href=/ai>AI技术</a><a class="navbar__item navbar__link" href=/cloud-native>云原生</a><a class="navbar__item navbar__link" href=/notes>日常笔记</a><a class="navbar__item navbar__link" href=/programming>开发语言</a><a class="navbar__item navbar__link" href=/architecture>技术架构</a><a class="navbar__item navbar__link" href=/observability>可观测性</a><a class="navbar__item navbar__link" href=/life>生活笔记</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href=/aboutme>关于我</a><a class="navbar__item navbar__link" href=/blog>博客</a><a href=https://goframe.org/ target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-goframe-link"></a><a href=https://github.com/gqcn target=_blank rel="noopener noreferrer" class="navbar__item navbar__link header-github-link"></a><div class=navbarSearchContainer_Bca1><button type=button class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class=DocSearch-Button-Container><svg width=20 height=20 class=DocSearch-Search-Icon viewBox="0 0 20 20" aria-hidden=true><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke=currentColor fill=none fill-rule=evenodd stroke-linecap=round stroke-linejoin=round /></svg><span class=DocSearch-Button-Placeholder>Search</span></span><span class=DocSearch-Button-Keys></span></button></div></div></div><div role=presentation class=navbar-sidebar__backdrop></div></nav><div id=__docusaurus_skipToContent_fallback class="main-wrapper mainWrapper_z2l0"><div class=docsWrapper_hBAB><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type=button></button><div class=docRoot_UBD9><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class=sidebarViewport_aRkj><div class=sidebar_njMd><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" href=/ai>AI技术</a><button aria-label="Collapse sidebar category 'AI技术'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/agents>智能体</a><button aria-label="Expand sidebar category '智能体'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/app>应用开发</a><button aria-label="Expand sidebar category '应用开发'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/inference>推理服务</a><button aria-label="Expand sidebar category '推理服务'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/training-fine-tuning>训练微调</a><button aria-label="Expand sidebar category '训练微调'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/model-development>模型开发</a><button aria-label="Expand sidebar category '模型开发'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" tabindex=0 href=/ai/infra>基础架构</a><button aria-label="Collapse sidebar category '基础架构'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist menu__link--active" tabindex=0 href=/ai/vgpu>vGPU</a><button aria-label="Collapse sidebar category 'vGPU'" aria-expanded=true type=button class="clean-btn menu__caret"></button></div><ul style=display:block;overflow:visible;height:auto class=menu__list><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current=page tabindex=0 href=/ai/vgpu-introduction-and-comparison>vGPU介绍及主流方案对比</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class=menu__link tabindex=0 href=/ai/vgpu-hami>HAMi vGPU介绍及原理分析</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class=menu__link tabindex=0 href=/ai/vgpu-hami-with-volcano>HAMi With Volcano</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class=menu__link tabindex=0 href=/ai/vgpu-hami-test-with-volcano>HAMi Volcano安装测试</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class=menu__link tabindex=0 href=/ai/vgpu-hami-cuda-driver-api>HAMi CUDA Driver API</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/rdma>RDMA</a><button aria-label="Expand sidebar category 'RDMA'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/nvidia>NVIDIA</a><button aria-label="Expand sidebar category 'NVIDIA'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/scheduling>算力调度</a><button aria-label="Expand sidebar category '算力调度'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/ai/what-is-llmops-mlops>LLMOps介绍</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/ai/common-acceleration-cards>常见智算加速卡汇总</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/ai/ai-training-inference-scenarios>AI基础架构中常见业务场景痛点</a><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class=menu__link tabindex=0 href=/ai/cpu-gpu>CPU&GPU架构差异及AI场景中的应用</a></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" tabindex=0 href=/ai/basic>入门知识</a><button aria-label="Expand sidebar category '入门知识'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div></ul><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/cloud-native>云原生</a><button aria-label="Expand sidebar category '云原生'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/notes>日常笔记</a><button aria-label="Expand sidebar category '日常笔记'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/programming>开发语言</a><button aria-label="Expand sidebar category '开发语言'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/architecture>技术架构</a><button aria-label="Expand sidebar category '技术架构'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/management>技术管理</a><button aria-label="Expand sidebar category '技术管理'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/observability>可观测性</a><button aria-label="Expand sidebar category '可观测性'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/database-and-middleware>数据库与中间件</a><button aria-label="Expand sidebar category '数据库与中间件'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/operating-systems-and-networks>操作系统和网络</a><button aria-label="Expand sidebar category '操作系统和网络'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class=menu__list-item-collapsible><a class="menu__link menu__link--sublist" href=/life>生活笔记</a><button aria-label="Expand sidebar category '生活笔记'" aria-expanded=false type=button class="clean-btn menu__caret"></button></div></ul></nav></div></div></aside><main class=docMainContainer_TBSr><div class="container padding-top--md padding-bottom--lg"><div class=row><div class="col docItemCol_VOVn"><div class=docItemContainer_Djhp><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label=Breadcrumbs><ul class=breadcrumbs itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumbs__item><a aria-label="Home page" class=breadcrumbs__link href=/><svg viewBox="0 0 24 24" class=breadcrumbHomeIcon_YNFT><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill=currentColor /></svg></a><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/ai><span itemprop=name>AI技术</span></a><meta itemprop=position content=1><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/ai/infra><span itemprop=name>基础架构</span></a><meta itemprop=position content=2><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class=breadcrumbs__item><a class=breadcrumbs__link itemprop=item href=/ai/vgpu><span itemprop=name>vGPU</span></a><meta itemprop=position content=3><li itemscope itemprop=itemListElement itemtype=https://schema.org/ListItem class="breadcrumbs__item breadcrumbs__item--active"><span class=breadcrumbs__link itemprop=name>vGPU介绍及主流方案对比</span><meta itemprop=position content=4></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type=button class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id=背景介绍>背景介绍<a href=#背景介绍 class=hash-link aria-label="Direct link to 背景介绍" title="Direct link to 背景介绍">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=为什么需要vgpu>为什么需要vGPU？<a href=#为什么需要vgpu class=hash-link aria-label="Direct link to 为什么需要vGPU？" title="Direct link to 为什么需要vGPU？">​</a></h3>
<p>在当下的<code>AI/ML</code>应用实践中，我们能明显感受到两股趋势的并行发展：</p>
<p><strong>小模型推理场景</strong>：传统的小模型推理与部署方兴未艾，它们在推荐系统、实时预测、路径优化等业务中依然有着广阔的应用场景。然而，这类服务往往只需要<code>2-4GB</code>显存，却独占<code>24GB</code>的<code>GPU</code>卡，导致资源利用率低下，造成严重的资源浪费。</p>
<p><strong>大模型部署场景</strong>：以大语言模型为代表的大模型部署正在快速兴起，对<code>GPU</code>多卡资源算力弹性的要求越来越高。在多租户环境下，缺乏有效的资源隔离机制，资源碎片化导致大任务无法调度。</p>
<p>这两个场景共同面临的核心问题是：<strong>如何在云原生环境下实现<code>GPU</code>资源的高效利用、精细化共享与隔离？</strong></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=vgpu的核心价值>vGPU的核心价值<a href=#vgpu的核心价值 class=hash-link aria-label="Direct link to vGPU的核心价值" title="Direct link to vGPU的核心价值">​</a></h3>
<p><code>vGPU</code>（虚拟<code>GPU</code>）技术通过将物理<code>GPU</code>虚拟化为多个逻辑<code>GPU</code>，具有以下优势：</p>
<ul>
<li>
<p><strong>资源利用率显著提升</strong>：通过将单张<code>GPU</code>卡虚拟化为多个逻辑<code>GPU</code>，让多个小任务可以共享同一张物理<code>GPU</code>卡，大幅提高<code>GPU</code>的实际使用率，减少资源闲置。</p>
</li>
<li>
<p><strong>成本优化</strong>：在相同的工作负载下，通过提高单卡利用率，可以减少所需的物理<code>GPU</code>数量，从而降低硬件采购和运维成本。</p>
</li>
<li>
<p><strong>灵活的资源管理</strong>：支持为每个任务精确分配显存和算力配额，实现细粒度的资源控制。可以根据业务需求动态调整资源分配策略，提高资源使用的灵活性。</p>
</li>
<li>
<p><strong>多租户隔离保障</strong>：提供硬件级或软件级的资源隔离机制，确保不同租户或任务之间互不干扰。配合完善的配额管理和监控能力，为生产环境提供可靠的多租户支持。</p>
</li>
</ul>
<p><img decoding=async loading=lazy alt=vGPU的核心价值 src=/assets/images/image-5e994ce7780a8366242a852379c3f420.png width=1900 height=1668 class=img_ev3q></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=业界主流vgpu方案对比>业界主流vGPU方案对比<a href=#业界主流vgpu方案对比 class=hash-link aria-label="Direct link to 业界主流vGPU方案对比" title="Direct link to 业界主流vGPU方案对比">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=主流方案分类>主流方案分类<a href=#主流方案分类 class=hash-link aria-label="Direct link to 主流方案分类" title="Direct link to 主流方案分类">​</a></h3>
<p>业界<code>vGPU</code>方案目前可划分为以下几类：</p>
<table><thead><tr><th>分类<th>代表方案<th>实现层次<th>开源情况<tbody><tr><td><strong>NVIDIA官方方案</strong><td><code>MPS</code>、<code>MIG</code><td>硬件 + 驱动层<td>闭源<tr><td><strong>用户态API劫持</strong><td><code>HAMi</code><td><code>CUDA Runtime</code>层<td>开源<tr><td><strong>内核态API劫持</strong><td>阿里云<code>cGPU</code>、腾讯云<code>qGPU</code><td>内核驱动层<td>闭源<tr><td><strong>硬件虚拟化</strong><td><code>NVIDIA vGPU</code><td>硬件虚拟化层<td>闭源商业</table>
<p>以英伟达的<code>GPU</code>为例，<code>GPU</code>虚拟化技术从硬件到软件的实现可以分为三个层次：用户态、内核态和硬件层。</p>
<p><img decoding=async loading=lazy alt=GPU虚拟化技术 src=/assets/images/image-15-012533729e2e53daf31cb939a168ea92.png width=1526 height=1102 class=img_ev3q></p>
<ul>
<li><strong>用户态</strong>：应用程序通过<code>CUDA API</code>编写并行计算任务，并通过<code>CUDA API</code>与<code>GPU</code>的用户态驱动进行通信。在这个层次，用户态虚拟化可以通过拦截和转发标准接口（如<code>CUDA API</code>、<code>OpenGL</code>等）来实现。</li>
<li><strong>内核态</strong>：此层主要运行<code>GPU</code>的内核态驱动程序，它与操作系统内核紧密集成，受到操作系统以及<code>CPU</code>硬件的保护。内核态虚拟化方案通常通过拦截如<code>ioctl</code>、<code>mmap</code>、<code>read</code> 和 <code>write</code> 等内核态接口来实现<code>GPU</code>资源的虚拟化。</li>
<li><strong>硬件层</strong>：硬件虚拟化层，如英伟达的<code>MIG</code>（<code>Multi-Instance GPU</code>），可以直接在硬件级别进行<code>GPU</code>资源的划分与管理。</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=用户态虚拟化>用户态虚拟化<a href=#用户态虚拟化 class=hash-link aria-label="Direct link to 用户态虚拟化" title="Direct link to 用户态虚拟化">​</a></h4>
<p>用户态虚拟化利用标准的接口（如<code>CUDA</code>和<code>OpenGL</code>），通过拦截和转发<code>API</code>调用，将请求解析并转发给硬件厂商提供的用户态库中的相应函数。</p>
<p><img decoding=async loading=lazy alt=用户态虚拟化 src=/assets/images/image-13-62f48e88e590255053f90e34b77df589.png width=1910 height=1230 class=img_ev3q></p>
<p><strong>优点：</strong></p>
<ul>
<li><strong>兼容性强</strong>：基于<code>CUDA Runtime API</code>、<code>OpenGL</code>等标准化接口实现，无需修改内核或依赖特定硬件特性，适用于<code>NVIDIA Pascal</code>架构及以上的各类<code>GPU</code>（包括数据中心级和消费级），同时支持多厂商异构算力（如华为昇腾、寒武纪、海光等）。</li>
<li><strong>安全性高</strong>：运行在用户态空间，通过<code>LD_LIBRARY_PATH</code>或<code>LD_PRELOAD</code>机制注入劫持库，不涉及内核态代码修改，避免了系统级安全漏洞和<code>kernel panic</code>风险，符合企业安全合规要求。</li>
<li><strong>最小侵入性</strong>：部署方式对现有环境零侵入，无需重启节点或修改操作系统内核，仅需在容器运行时注入劫持库，支持热部署和快速回滚，适合企业生产环境的敏捷迭代。</li>
<li><strong>跨平台兼容性好</strong>：不依赖特定<code>Linux</code>内核版本，可在<code>CentOS</code>、<code>RHEL</code>、<code>Ubuntu</code>、<code>Debian</code>、<code>Kylin</code>等主流操作系统上无缝运行，无需针对不同内核版本单独适配。</li>
<li><strong>部署和维护成本低</strong>：不破坏现有<code>IT</code>基础架构，无需专业内核开发团队维护，运维人员仅需掌握容器和<code>Kubernetes</code>知识即可管理，降低了人力成本和技术门槛。</li>
<li><strong>开源生态成熟</strong>：以<code>HAMi</code>为代表的开源项目拥有活跃社区支持，用户可自主定制和优化，避免厂商锁定，且与<code>Kubernetes</code>、<code>Volcano</code>等云原生生态深度集成。</li>
<li><strong>支持统一内存</strong>：通过劫持<code>CUDA Unified Memory API</code>，支持<code>GPU</code>显存与主机内存的动态交换，突破单卡显存限制，提升大模型推理场景下的资源利用率。</li>
<li><strong>灵活的资源配置</strong>：支持任意粒度的显存和算力分配（如<code>3000MB</code>显存 + <code>50%</code>算力），不受硬件固定规格限制（如<code>MIG</code>的固定切分比例），适应多样化的业务需求。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>性能开销相对较高</strong>：每次<code>CUDA API</code>调用都需要经过劫持库的拦截、解析和转发，相比内核态方案增加了用户态函数调用开销，性能损耗约<code>10%</code>（内核态约<code>5%</code>），在高频<code>API</code>调用场景（如小算子密集型推理）下影响更明显。</li>
<li><strong>算力隔离为软限制</strong>：通过时间片轮转或<code>CUDA Stream</code>优先级实现算力限制，无法像<code>MIG</code>那样在硬件层面预留专用<code>SM</code>（流式多处理器），极端情况下可能出现算力抢占或饥饿问题。</li>
<li><strong>部分CUDA特性支持受限</strong>：对于直接操作硬件的底层<code>CUDA Driver API</code>（如<code>cuMemMap</code>、<code>cuMemAddressReserve</code>等）或特殊特性（如<code>CUDA Graphs</code>的某些高级用法），劫持库可能无法完全覆盖，存在兼容性风险。</li>
<li><strong>调试复杂度增加</strong>：劫持库的引入使得应用调用栈变长，当出现<code>CUDA</code>错误或性能问题时，需要同时排查应用代码、劫持库和<code>CUDA</code>驱动三层逻辑，增加了故障定位难度。</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=内核态虚拟化>内核态虚拟化<a href=#内核态虚拟化 class=hash-link aria-label="Direct link to 内核态虚拟化" title="Direct link to 内核态虚拟化">​</a></h4>
<p>内核态<code>API</code>拦截和转发方案的核心机制为：在内核空间新增驱动模块，为容器虚拟化<code>GPU</code>设备，通过劫持容器对<code>GPU Driver</code>的原生调用（如<code>ioctl</code>、<code>mmap</code>、<code>read</code>、<code>write</code> 等），严格限制显存，时分复用算力，实现多任务共享同一物理<code>GPU</code>。这种技术方案主要运行在操作系统的内核态中，因此其安全性和稳定性较为复杂。</p>
<p><img decoding=async loading=lazy alt=内核态虚拟化 src=/assets/images/image-16-4ecad4b708d393a38449a448494a0dd5.png width=1608 height=1172 class=img_ev3q></p>
<p><strong>优点：</strong></p>
<ul>
<li><strong>性能开销更低</strong>：在内核态直接拦截<code>ioctl</code>等系统调用，相比用户态方案减少了一层<code>API</code>解析和转发开销，性能损耗约<code>5%</code>（用户态约<code>10%</code>）。</li>
<li><strong>更底层的资源控制</strong>：直接在内核驱动层实现资源管理，能够更精细地控制显存分配、算力调度和<code>GPU</code>上下文切换。</li>
<li><strong>强隔离能力</strong>：通过内核态的权限隔离机制，能够提供更强的资源隔离保障，防止容器间的资源竞争和越界访问。</li>
<li><strong>硬件无关性</strong>：不依赖特定<code>GPU</code>硬件特性（如<code>MIG</code>），理论上可支持各类数据中心级和消费级<code>GPU</code>。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>极高的系统侵入性</strong>：需要在<code>Linux</code>内核中插入自定义驱动模块，直接修改内核态代码路径，对操作系统的侵入性极大，可能引入系统级安全漏洞和稳定性风险。</li>
<li><strong>内核版本兼容性问题</strong>：不同<code>Linux</code>内核版本（如<code>3.x</code>、<code>4.x</code>、<code>5.x</code>、<code>6.x</code>）的内核<code>API</code>和数据结构差异巨大，需要针对每个版本单独适配和维护，开发和测试成本极高。</li>
<li><strong>企业环境落地困难</strong>：国央企、金融、能源等行业的<code>IT</code>架构复杂，操作系统版本多样化（<code>CentOS</code>、<code>RHEL</code>、<code>Ubuntu</code>、<code>Kylin</code>等），内核定制化程度高，统一部署和管理难度极大。</li>
<li><strong>安全合规风险</strong>：内核态代码需要<code>root</code>权限加载，在金融、医疗、政府等高安全性行业难以通过安全审计和合规认证（如等保三级、<code>PCI-DSS</code>等）。</li>
<li><strong>法律和可持续性风险</strong>：部分方案涉及对<code>NVIDIA</code>专有驱动的逆向工程，存在知识产权侵权风险，可能被<code>GPU</code>厂商通过驱动更新封堵，影响长期可用性和商业化落地。</li>
<li><strong>维护成本高昂</strong>：需要持续跟进操作系统内核更新、<code>GPU</code>驱动版本升级，以及不同云平台（公有云、私有云、混合云）的环境适配，运维团队需要具备深厚的内核开发能力。</li>
<li><strong>故障排查复杂</strong>：内核态代码出现<code>bug</code>可能导致系统崩溃（<code>kernel panic</code>）、死锁或数据损坏，排查和修复难度远高于用户态方案，影响生产环境稳定性。</li>
<li><strong>开源生态缺失</strong>：目前主流内核态方案（如阿里云<code>cGPU</code>、腾讯云<code>qGPU</code>）均为闭源商业产品，缺乏开源社区支持，用户无法自主定制和优化，存在厂商锁定风险。</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=远程资源调用>远程资源调用<a href=#远程资源调用 class=hash-link aria-label="Direct link to 远程资源调用" title="Direct link to 远程资源调用">​</a></h4>
<p>近年来，远程<code>GPU</code>调用（<code>Remote GPU</code>）方案备受关注，它允许用户在一台<code>CPU</code>服务器上远程访问另一台服务器上的<code>GPU</code>资源，看上去能解决资源碎片化问题。</p>
<p><img decoding=async loading=lazy alt=远程GPU资源调用 src=/assets/images/image-14-5e48f7148a8c416ecfaa1b13e8fa72f1.png width=1960 height=1212 class=img_ev3q></p>
<p>然而，在现代 AI 应用（尤其是大模型、小模型混合训练和推理）背景下，远程<code>GPU</code>资源调用几乎不可用，原因包括：</p>
<ul>
<li><strong>数据传输瓶颈</strong>： 大模型训练涉及<code>PB</code>级数据，远程调用<code>GPU</code>需要在<code>CPU</code>和<code>GPU</code>之间频繁传输数据，带宽和延迟问题导致性能严重下降。远程<code>GPU</code>调用需要通过网络传输数据，特别是对于计算密集型任务（如大规模的神经网络推理），网络延迟将极大影响性能，甚至导致任务无法及时完成。此外，大模型和小模型的训练过程中需要高效的同步机制，远程调用会导致数据同步效率低下，影响模型训练的效果与效率。</li>
<li><strong>计算密集型任务难以拆分</strong>： AI 训练任务通常需要多<code>GPU</code>互相通信（如<code>AllReduce</code>、<code>Pipeline 并行</code>），远程<code>GPU</code>之间的通信成本远超本地<code>GPU</code>。远程调用<code>GPU</code>资源意味着大量数据需要在网络中传输，对于带宽要求极高。尤其是在大规模训练和推理任务中，带宽瓶颈往往会成为性能的瓶颈。</li>
<li><strong>小模型推理的实时性要求</strong>： 对于小模型的推理任务，远程调用的通信延迟远大于计算时间，导致整体效率大幅下降。</li>
</ul>
<p>因此，尽管远程<code>GPU</code>调用在某些场景下具有一定的吸引力，但在实际操作中，它通常会面临性能瓶颈和资源调度问题，特别是在现代AI应用中，几乎不可行。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id=详细方案对比>详细方案对比<a href=#详细方案对比 class=hash-link aria-label="Direct link to 详细方案对比" title="Direct link to 详细方案对比">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=常见虚拟化方案比较>常见虚拟化方案比较<a href=#常见虚拟化方案比较 class=hash-link aria-label="Direct link to 常见虚拟化方案比较" title="Direct link to 常见虚拟化方案比较">​</a></h4>
<blockquote>
<p><strong>说明</strong>：本方案对比不包含阿里云<code>cGPU</code>、腾讯云<code>qGPU</code>等内核态API劫持方案，由于其数据不明，且存在一定的使用限制，不纳入考虑范围。<code>NVIDIA MPS</code>仅考虑<code>Volta MPS</code>，不考虑旧版本的<code>Pre-Volta MPS</code>。</p>
</blockquote>
<table><thead><tr><th>维度<th>NVIDIA MPS (Volta)<th>NVIDIA MIG<th>HAMi<tbody><tr><td><strong>隔离性</strong><td><td><td><tr><td>显存隔离<td>⚠️ 地址空间隔离<td>✅ 硬件隔离<td>✅ 软件硬隔离<tr><td>算力隔离<td>⚠️ 软限制(不预留专用SM)<td>✅ 硬件隔离<td>⚠️ 软限制<tr><td>进程崩溃影响<td>⚠️ 有限隔离<td>✅ 完全隔离<td>✅ 仅影响自身<tr><td>内存越界保护<td>✅ 有保护<td>✅ 硬件保护<td>✅ 有保护<tr><td><strong>性能</strong><td><td><td><tr><td>性能开销<td>⚠️ 约5%<td>✅ 极低（ &lt; 3%）<td>⚠️ 约10%<tr><td>上下文切换<td>✅ 快速<td>✅ 极快<td>⚠️ 中等<tr><td>并发效率<td>✅ 高<td>⚠️ 中等<td>✅ 高<tr><td><strong>易用性</strong><td><td><td><tr><td>部署复杂度<td>✅ 简单<td>⚠️ 需要配置<td>⚠️ 需安装组件<tr><td>容器化支持<td>✅ 原生支持<td>✅ 原生支持<td>✅ 原生支持<tr><td>配置灵活性<td>⚠️ 支持资源配额<td>⚠️ 固定规格<td>✅ 非常灵活<tr><td>监控可观测性<td>⚠️ 有限(仅看到MPS Server)<td>✅ 完善<td>✅ 完善<tr><td><strong>兼容性</strong><td><td><td><tr><td>GPU型号支持<td>⚠️ <code>Volta+</code><td>⚠️ <code>A100/H100+</code><td>✅ <code>Pascal+</code><tr><td>CUDA版本要求<td>✅ 无特殊要求<td>⚠️ <code>11.0+</code><td>⚠️ 特定版本<tr><td>应用兼容性<td>✅ 透明<td>✅ 透明<td>⚠️ 大部分兼容<tr><td>客户端连接数<td>⚠️ 最多48个<td>⚠️ 取决于MIG配置<td>✅ 无限制<tr><td><strong>成本</strong><td><td><td><tr><td>软件成本<td>✅ 免费<td>✅ 免费<td>✅ 开源免费<tr><td>硬件要求<td>⚠️ <code>Volta+</code>架构<td>⚠️ 特定GPU<td>✅ 无特殊要求<tr><td>维护成本<td>⚠️ 中等<td>✅ 低<td>✅ 低<tr><td><strong>其他</strong><td><td><td><tr><td>开源情况<td>❌ 闭源<td>❌ 闭源<td>✅ 开源<tr><td>社区活跃度<td>⚠️ 官方支持<td>⚠️ 官方支持<td>✅ 活跃</table>
<blockquote>
<p><strong>性能测试参考数据来源：</strong></p>
<ul>
<li><strong>NVIDIA MPS</strong>: 基于<code>NVIDIA</code>官方<code>GROMACS</code>测试，在<code>8-GPU DGX A100</code>上运行多个模拟可提升<code>1.3-1.8</code>倍吞吐量，性能开销<code>&lt;5%</code>。<a href=https://developer.nvidia.com/blog/maximizing-gromacs-throughput-with-multiple-simulations-per-gpu-using-mps-and-mig/ target=_blank rel="noopener noreferrer">来源</a></li>
<li><strong>NVIDIA MIG</strong>: 某些场景下<code>MIG+MPS</code>组合比纯<code>MPS</code>高约<code>7%</code>，但存在<code>5-15%</code>的硬件分区开销。<a href=https://developer.nvidia.com/blog/maximizing-gromacs-throughput-with-multiple-simulations-per-gpu-using-mps-and-mig/ target=_blank rel="noopener noreferrer">来源</a></li>
<li><strong>HAMi</strong>: 基于<code>ai-benchmark</code>在<code>Tesla V100</code>上的测试数据，用户态API劫持性能开销约<code>10%</code>。<a href=https://github.com/Project-HAMi/HAMi/blob/master/docs/benchmark.md target=_blank rel="noopener noreferrer">来源</a></li>
</ul>
<p><strong>注意</strong>: 不同方案之间缺乏统一标准的直接对比测试，实际性能因工作负载类型、GPU型号和配置而异，上述数据仅供参考。</p>
</blockquote>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=nvidia-mps的缺点>NVIDIA MPS的缺点<a href=#nvidia-mps的缺点 class=hash-link aria-label="Direct link to NVIDIA MPS的缺点" title="Direct link to NVIDIA MPS的缺点">​</a></h4>
<p><code>NVIDIA MPS</code>的核心是在<code>CUDA Runtime</code>和<code>CUDA Driver</code>之上引入了<code>MPS</code>服务层，把多个进程的<code>CUDA</code>内核请求合并并下发给<code>GPU</code>，使得多个<code>CUDA</code>进程（或多个容器内的进程）能够共享<code>GPU</code>的<code>SM</code>（即算力部分），从而避免了<code>CUDA Context</code>频繁上下文切换，实现<code>GPU</code>利用率的提升。</p>
<p><img decoding=async loading=lazy alt="NVIDIA MPS流程" src=/assets/images/image-4-0d97c13e2b7e952bbd5a44b767090b2a.png width=2626 height=610 class=img_ev3q></p>
<p><code>NVIDIA MPS</code>分为两个版本：</p>
<ul>
<li><strong>Pre-Volta MPS</strong>：适用于<code>Volta</code>架构之前的<code>GPU</code>（如<code>Pascal</code>、<code>Kepler</code>等）</li>
<li><strong>Volta MPS</strong>：从<code>Volta</code>架构开始引入（如<code>V100</code>、<code>A100</code>、<code>H100</code>等），提供了增强的隔离能力</li>
</ul>
<blockquote>
<p><code>Volta</code>架构是<code>NVIDIA</code>在<code>2017</code>年推出的<code>GPU</code>架构。一些消费级显卡如<code>4090/4090D</code>均是属于<code>Volta</code>以后的架构。以下均以<code>Volta MPS</code>技术方案为介绍。</p>
</blockquote>
<p><code>Volta MPS</code>虽然相比<code>Pre-Volta MPS</code>在隔离性方面有显著改进（提供了完全的地址空间隔离和有限的错误隔离），但在实际生产环境中仍存在以下关键限制：</p>
<ul>
<li>
<p><strong>显存配额管理复杂</strong>：</p>
<ul>
<li>虽然支持通过<code>CUDA_MPS_PINNED_DEVICE_MEM_LIMIT</code>环境变量设置显存硬限制（超过限制会返回<code>OOM</code>错误）</li>
<li>但需要在客户端启动前通过环境变量或<code>MPS</code>控制接口预先配置，无法动态调整</li>
<li>根据 <a href=https://docs.nvidia.com/deploy/mps/index.html#volta-mps-execution-resource-provisioning target=_blank rel="noopener noreferrer">NVIDIA MPS官方文档</a>，配置管理复杂，需要为每个客户端单独设置，不适合大规模多租户场景</li>
</ul>
</li>
<li>
<p><strong>算力隔离是软限制</strong>：</p>
<ul>
<li>根据官方文档明确说明：<code>Setting the limit does not reserve dedicated resources</code>（设置限制不会预留专用资源）</li>
<li>不同客户端的内核可能在同一<code>SM</code>上执行，无法保证严格的算力配额</li>
<li>不适合需要<code>SLA</code>保障的商业场景</li>
</ul>
</li>
<li>
<p><strong>客户端故障隔离不完善</strong>：</p>
<ul>
<li>单个客户端的致命错误（如非法内存访问、内核崩溃）会影响<strong>与故障GPU有共同客户端的所有GPU</strong></li>
<li>例如：系统有<code>GPU 0/1/2</code>，客户端<code>A</code>使用<code>GPU 0</code>，客户端<code>B</code>使用<code>GPU 0+1</code>，客户端<code>C</code>使用<code>GPU 1</code>，客户端<code>D</code>使用<code>GPU 2</code>。如果客户端<code>A</code>触发故障：<!-- -->
<ul>
<li>故障会被隔离在<code>GPU 0</code>和<code>GPU 1</code>（因为它们共享客户端<code>B</code>）</li>
<li>客户端<code>A</code>、<code>B</code>、<code>C</code>都会收到故障通知（无法指明是<code>A</code>触发的）</li>
<li>客户端<code>D</code>不受影响，继续正常运行</li>
<li><code>MPS Server</code>状态变为<code>FAULT</code>，拒绝新客户端连接（返回<code>CUDA_ERROR_MPS_SERVER_NOT_READY</code>）</li>
<li>必须等待<code>A</code>、<code>B</code>、<code>C</code>全部退出后，<code>Server</code>才能重建<code>GPU 0</code>和<code>GPU 1</code>的上下文并恢复服务</li>
</ul>
</li>
<li>根据 <a href=https://docs.nvidia.com/deploy/mps/index.html#error-containment target=_blank rel="noopener noreferrer">NVIDIA MPS官方文档</a>，这种设计意味着一个有<code>bug</code>的客户端可能导致多个<code>GPU</code>上的所有任务失败，不适合多租户生产环境</li>
</ul>
</li>
<li>
<p><strong>资源监控困难</strong>：</p>
<ul>
<li><code>nvidia-smi</code>只能看到<code>MPS Server</code>进程，无法查看各客户端的实际资源使用</li>
<li>缺少细粒度的资源使用监控和告警能力</li>
</ul>
</li>
</ul>
<p><strong>适用场景</strong>：</p>
<ul>
<li>✅ 适合：可信任的单一应用多进程、开发测试环境、对隔离性要求不高的场景、使用<code>Volta+</code>架构<code>GPU</code>的环境。</li>
<li>❌ 不适合：多租户生产环境、关键业务推理服务、需要严格资源配额和故障隔离的场景、需要硬件级隔离保障的场景。</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id=nvidia-mig的缺点>NVIDIA MIG的缺点<a href=#nvidia-mig的缺点 class=hash-link aria-label="Direct link to NVIDIA MIG的缺点" title="Direct link to NVIDIA MIG的缺点">​</a></h4>
<p><code>NVIDIA MIG</code>是<code>NVIDIA</code>对<code>A100</code>、<code>H100</code>等新一代<code>GPU</code>提供的一种软硬件一体化的<code>GPU/显存隔离技术</code>。硬件层面，可以将<code>SM</code> + <code>L2 Cache</code> + <code>内存控制器</code> + <code>IO 通道</code>切割成多个独立<code>MIG</code>单元，每个单元就是一个独立的<code>GPU</code>实例，从应用和容器视角看就获得一张小<code>GPU</code>资源；从软件层面，在<code>nvidia driver</code>中增加了<code>MIG driver</code>，实现对支持隔离的硬件的调用，从而实现端到端的<code>GPU虚拟化</code>。</p>
<p>该方案在多租户场景下，虽然能够保证显存和算力的强隔离，但仅支持部分高端<code>GPU</code>（如<code>A100</code>/<code>H100</code>），对常见的<code>T4</code>、<code>A10</code>不适用。</p>
<p><img decoding=async loading=lazy alt="NVIDIA MIG演进" src=/assets/images/image-5-8c81340919c97182f27bad247a6b8e42.png width=1806 height=1012 class=img_ev3q></p>
<p>其主要缺点如下：</p>
<ul>
<li><strong>硬件支持受限</strong>：仅支持<code>A100</code>、<code>H100</code>等高端<code>GPU</code>型号，对于使用<code>V100</code>、<code>T4</code>等较早型号<code>GPU</code>的用户无法使用。</li>
<li><strong>分区规格固定</strong>：<code>MIG</code>实例的划分规格是预定义的（如<code>1g.5gb</code>、<code>2g.10gb</code>等），无法根据实际需求灵活调整显存和算力比例。</li>
<li><strong>配置复杂度高</strong>：需要在<code>GPU</code>驱动层面进行<code>MIG</code>模式配置，涉及<code>GPU</code>重启和实例创建，操作相对复杂。</li>
<li><strong>资源利用率问题</strong>：由于分区规格固定，可能出现资源碎片化，例如剩余的小规格实例无法满足大任务需求。</li>
<li><strong>动态调整困难</strong>：<code>MIG</code>实例一旦创建，调整配置需要销毁重建，不支持在线动态调整资源分配。</li>
</ul>
<p><strong>适用场景</strong>：</p>
<ul>
<li>✅ 适合：多租户生产环境、需要强隔离保障的场景、高端<code>GPU</code>集群。</li>
<li>❌ 不适合：需要灵活资源配置的场景、使用非<code>A100/H100</code>系列<code>GPU</code>的环境、频繁调整资源配额的场景。</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=hami开源项目介绍>HAMi开源项目介绍<a href=#hami开源项目介绍 class=hash-link aria-label="Direct link to HAMi开源项目介绍" title="Direct link to HAMi开源项目介绍">​</a></h2>
<p>具体请参考：<a href=/ai/vgpu-hami>HAMi vGPU介绍及原理分析</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id=参考资料>参考资料<a href=#参考资料 class=hash-link aria-label="Direct link to 参考资料" title="Direct link to 参考资料">​</a></h2>
<ul>
<li><a href=https://aws.amazon.com/cn/blogs/china/gpu-virtualization-practice-based-on-hami/ target=_blank rel="noopener noreferrer">https://aws.amazon.com/cn/blogs/china/gpu-virtualization-practice-based-on-hami/</a></li>
<li><a href=https://www.theriseunion.com/blog/GPU-Virtualization-Technology-User-vs-Kernel-Guide.html target=_blank rel="noopener noreferrer">https://www.theriseunion.com/blog/GPU-Virtualization-Technology-User-vs-Kernel-Guide.html</a></li>
<li><a href=https://www.theriseunion.com/blog/Project-HAMi.html target=_blank rel="noopener noreferrer">https://www.theriseunion.com/blog/Project-HAMi.html</a></li>
</ul></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href=/ai/vgpu><div class=pagination-nav__sublabel>Previous</div><div class=pagination-nav__label>vGPU</div></a><a class="pagination-nav__link pagination-nav__link--next" href=/ai/vgpu-hami><div class=pagination-nav__sublabel>Next</div><div class=pagination-nav__label>HAMi vGPU介绍及原理分析</div></a></nav><div class=docusaurus-mt-lg></div></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href=#背景介绍 class="table-of-contents__link toc-highlight">背景介绍</a><ul><li><a href=#为什么需要vgpu class="table-of-contents__link toc-highlight">为什么需要vGPU？</a><li><a href=#vgpu的核心价值 class="table-of-contents__link toc-highlight">vGPU的核心价值</a></ul><li><a href=#业界主流vgpu方案对比 class="table-of-contents__link toc-highlight">业界主流vGPU方案对比</a><ul><li><a href=#主流方案分类 class="table-of-contents__link toc-highlight">主流方案分类</a><ul><li><a href=#用户态虚拟化 class="table-of-contents__link toc-highlight">用户态虚拟化</a><li><a href=#内核态虚拟化 class="table-of-contents__link toc-highlight">内核态虚拟化</a><li><a href=#远程资源调用 class="table-of-contents__link toc-highlight">远程资源调用</a></ul><li><a href=#详细方案对比 class="table-of-contents__link toc-highlight">详细方案对比</a><ul><li><a href=#常见虚拟化方案比较 class="table-of-contents__link toc-highlight">常见虚拟化方案比较</a><li><a href=#nvidia-mps的缺点 class="table-of-contents__link toc-highlight">NVIDIA MPS的缺点</a><li><a href=#nvidia-mig的缺点 class="table-of-contents__link toc-highlight">NVIDIA MIG的缺点</a></ul></ul><li><a href=#hami开源项目介绍 class="table-of-contents__link toc-highlight">HAMi开源项目介绍</a><li><a href=#参考资料 class="table-of-contents__link toc-highlight">参考资料</a></ul></div></div></div></div></main></div></div></div><footer class=footer><div class="container container-fluid"><div class="footer__bottom text--center"><div class=footer__copyright>Copyright 2026 johng.cn</div></div></div></footer></div>