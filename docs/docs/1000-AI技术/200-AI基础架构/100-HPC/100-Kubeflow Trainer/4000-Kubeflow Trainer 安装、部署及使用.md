---
slug: /ai/kubeflow-trainer-deploy-usage
title: Kubeflow Trainer å®‰è£…ã€éƒ¨ç½²åŠä½¿ç”¨
hide_title: true
keywords: [kubeflow, trainer, pytorch, åˆ†å¸ƒå¼è®­ç»ƒ, kubernetes, hpc, cpuè®­ç»ƒ, configmap, hostpath, docker]
description: "è¯¦ç»†ä»‹ç»Kubeflow Trainerçš„çœŸå®å®‰è£…ã€éƒ¨ç½²åŠä½¿ç”¨è¿‡ç¨‹ï¼ŒåŸºäºCPUçš„PyTorchåˆ†å¸ƒå¼è®­ç»ƒæ¡ˆä¾‹ï¼ŒåŒ…æ‹¬ConfigMapã€HostPathã€é•œåƒæ„å»ºä¸‰ç§éƒ¨ç½²æ–¹å¼çš„å®Œæ•´å®è·µæŒ‡å—"
---


## ç®€ä»‹

æœ¬æ–‡å°†ä»‹ç»å¦‚ä½•åœ¨`Kubernetes`é›†ç¾¤ä¸­å®‰è£…å’Œéƒ¨ç½²`Kubeflow Trainer`ï¼Œå¹¶é€šè¿‡çœŸå®çš„åŸºäº`CPU`çš„`PyTorch`åˆ†å¸ƒå¼è®­ç»ƒæ¡ˆä¾‹ï¼Œè¯¦ç»†æ¼”ç¤ºä¸‰ç§ä¸åŒçš„è®­ç»ƒä»»åŠ¡éƒ¨ç½²ä½¿ç”¨æ–¹å¼ã€‚


æœ¬æ–‡åŸºäºä»¥ä¸‹ç¯å¢ƒè¿›è¡Œå®è·µï¼š

| ç»„ä»¶ | ç‰ˆæœ¬ |
|------|------|
| **æ“ä½œç³»ç»Ÿ** | `MacOS M4` |
| **Kubernetes** | `1.27.3` |
| **Kubeflow Trainer** | `v2.1.0` |
| **PyTorch** | `2.4.1` |
| **Python** | `3.10+` |
| **è®­ç»ƒè®¾å¤‡** | `CPU` |

:::info è¯´æ˜
è™½ç„¶æœ¬æ–‡ä½¿ç”¨`CPU`è¿›è¡Œè®­ç»ƒæ¼”ç¤ºï¼Œä½†æ‰€æœ‰æ–¹æ³•åŒæ ·é€‚ç”¨äº`GPU`è®­ç»ƒåœºæ™¯ï¼Œåªéœ€è°ƒæ•´ç›¸åº”çš„èµ„æºé…ç½®å³å¯ã€‚
:::

## å®‰è£…éƒ¨ç½²

### å‰ç½®æ¡ä»¶

åœ¨å¼€å§‹å®‰è£…ä¹‹å‰ï¼Œè¯·ç¡®ä¿ï¼š

1. âœ… å·²å®‰è£…å¹¶é…ç½®`Kubernetes`é›†ç¾¤ï¼ˆç‰ˆæœ¬ >= `1.27`ï¼‰
2. âœ… å·²å®‰è£…`kubectl`å‘½ä»¤è¡Œå·¥å…·
3. âœ… å·²å®‰è£…`Helm`ï¼ˆç‰ˆæœ¬ >= `3.0`ï¼‰
4. âœ… é›†ç¾¤æœ‰è¶³å¤Ÿçš„`CPU/å†…å­˜`èµ„æºç”¨äºè®­ç»ƒä»»åŠ¡

### æ–¹å¼ä¸€ï¼šHelm éƒ¨ç½²ï¼ˆæ¨èï¼‰

`Helm`æ˜¯æœ€ç®€å•å’Œæ¨èçš„éƒ¨ç½²æ–¹å¼ï¼Œå¯ä»¥æ–¹ä¾¿åœ°ç®¡ç†`Kubeflow Trainer`çš„ç”Ÿå‘½å‘¨æœŸã€‚

#### æ‹‰å– Chart åˆ°æœ¬åœ°

```bash
helm pull oci://ghcr.io/kubeflow/charts/kubeflow-trainer --version 2.1.0 --untar
```

#### éƒ¨ç½² Kubeflow Trainer

```bash
helm install kubeflow-trainer ./kubeflow-trainer -n kubeflow-system --create-namespace
```

#### éªŒè¯éƒ¨ç½²ç»“æœ

```bash
kubectl get pods -n kubeflow-system
```

é¢„æœŸè¾“å‡ºï¼š

```bash
NAME                                                   READY   STATUS    RESTARTS   AGE
jobset-controller-5b77f7f78-q6ftq                      1/1     Running   0          9m53s
kubeflow-trainer-controller-manager-56c44969b8-7mghh   1/1     Running   0          9m53s
```

:::tip æç¤º
- `jobset-controller`ï¼šè´Ÿè´£ç®¡ç†`JobSet CRD`
- `kubeflow-trainer-controller-manager`ï¼šè´Ÿè´£ç®¡ç†`TrainJob`ã€`TrainingRuntime`ç­‰è®­ç»ƒç›¸å…³èµ„æº
:::

### æ–¹å¼äºŒï¼škubectl éƒ¨ç½²

å¦‚æœä¸ä½¿ç”¨`Helm`ï¼Œä¹Ÿå¯ä»¥ç›´æ¥é€šè¿‡`kubectl`éƒ¨ç½²ï¼š

```bash
export VERSION=v2.1.0
kubectl apply --server-side -k "https://github.com/kubeflow/trainer.git/manifests/overlays/manager?ref=${VERSION}"
```

### éªŒè¯ API èµ„æº

éƒ¨ç½²å®Œæˆåï¼ŒéªŒè¯`Kubeflow Trainer`æä¾›çš„`API`èµ„æºï¼š

```bash
kubectl api-resources | grep trainer
```

é¢„æœŸè¾“å‡ºï¼š

```text
NAME                          SHORTNAMES   APIVERSION                      NAMESPACED   KIND
clustertrainingruntimes       ctr          trainer.kubeflow.org/v1alpha1   false        ClusterTrainingRuntime
trainingruntimes              tr           trainer.kubeflow.org/v1alpha1   true         TrainingRuntime
trainjobs                     tj           trainer.kubeflow.org/v1alpha1   true         TrainJob
```

## Training Runtimes é…ç½®

### ä»€ä¹ˆæ˜¯Training Runtimesï¼Ÿ

`Training Runtimes`æ˜¯`Kubeflow Trainer`çš„ä¸Šå±‚å¢å¼ºç»„ä»¶ï¼Œç”¨äºæ ‡å‡†åŒ–è®­ç»ƒç¯å¢ƒé…ç½®ã€‚å®ƒçš„æ ¸å¿ƒä»·å€¼åœ¨äºï¼š

| åŠŸèƒ½ | è¯´æ˜ |
|------|------|
| **æ ‡å‡†åŒ–è®­ç»ƒç¯å¢ƒ** | é¢„å®šä¹‰è®­ç»ƒè¿è¡Œæ—¶ï¼ˆå¦‚`pytorch-2.1-cpu`ã€`tensorflow-2.15-gpu`ï¼‰ï¼Œæäº¤ä»»åŠ¡æ—¶ç›´æ¥å¼•ç”¨ |
| **å¤šæ¡†æ¶é€‚é…** | æ”¯æŒä¸åŒæ¡†æ¶ï¼ˆ`PyTorch/TensorFlow/MXNet`ï¼‰ã€ä¸åŒç‰ˆæœ¬ã€ä¸åŒç¡¬ä»¶ï¼ˆ`CPU/GPU`ï¼‰ |
| **ç®€åŒ–ä»»åŠ¡æäº¤** | `TrainJob`åªéœ€æŒ‡å®š`runtimeRef`å³å¯å¤ç”¨é…ç½®ï¼Œæ— éœ€é‡å¤å®šä¹‰ |
| **å¢å¼ºçš„èµ„æºè°ƒåº¦** | æ”¯æŒæ›´ç²¾ç»†çš„èµ„æºè°ƒåº¦ç­–ç•¥ï¼ˆå¦‚`GPU`åˆ†ç‰‡ã€èŠ‚ç‚¹äº²å’Œæ€§ï¼‰å’Œä»»åŠ¡å®¹é”™æœºåˆ¶ |

### Training Runtimeså¸¸ç”¨æ¨¡æ¿

é€šè¿‡ä»¥ä¸‹æ–¹å¼å®‰è£…ç¤¾åŒºæä¾›çš„`Training Runtimes`å¸¸ç”¨æ¨¡æ¿ï¼š

```bash
export VERSION=v2.1.0
kubectl apply --server-side -k "https://github.com/kubeflow/trainer.git/manifests/overlays/manager?ref=${VERSION}"
```

:::info ä½¿ç”¨å»ºè®®
- **åŸºç¡€åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šä»…éœ€`TrainJob/PyTorchJob/TFJob`ï¼Œç°æœ‰ç»„ä»¶å·²æ»¡è¶³éœ€æ±‚
- **çµæ´»è®­ç»ƒç¯å¢ƒç®¡ç†**ï¼šéœ€è¦å¤šæ¡†æ¶é€‚é…ã€èµ„æºè°ƒåº¦ä¼˜åŒ–ï¼Œå»ºè®®å®‰è£…`Training Runtimes`
:::

### åˆ›å»ºClusterTrainingRuntime

ä¸‹é¢åˆ›å»ºä¸€ä¸ªç”¨äº`PyTorch CPU`è®­ç»ƒçš„è‡ªå®šä¹‰è¿è¡Œæ—¶æ¨¡æ¿ï¼š

```yaml
apiVersion: trainer.kubeflow.org/v1alpha1
kind: ClusterTrainingRuntime
metadata:
  name: training-runtime-demo
  labels:
    # æŒ‡å®šè®­ç»ƒæ¡†æ¶ç±»å‹ä¸º PyTorch
    trainer.kubeflow.org/framework: torch
spec:
  # MLç­–ç•¥é…ç½®ï¼šå®šä¹‰åˆ†å¸ƒå¼è®­ç»ƒçš„åŸºç¡€å‚æ•°
  mlPolicy:
    # ClusterTrainingRuntime æ¨¡æ¿ä¸­çš„é»˜è®¤èŠ‚ç‚¹æ•°ï¼ˆä¼šè¢« TrainJob ä¸­çš„ numNodes è¦†ç›–ï¼‰
    numNodes: 1
    # PyTorch æ¡†æ¶ä¸“ç”¨é…ç½®
    torch:
      # æ¯ä¸ªèŠ‚ç‚¹çš„è¿›ç¨‹æ•°
      # auto: è‡ªåŠ¨æ£€æµ‹ï¼ˆé€šå¸¸ç­‰äº GPU/CPU æ ¸å¿ƒæ•°ï¼‰
      # ä¹Ÿå¯ä»¥è®¾ç½®å…·ä½“æ•°å­—å¦‚ 2ã€4 ç­‰
      numProcPerNode: auto
  
  # JobSet æ¨¡æ¿é…ç½®
  template:
    spec:
      replicatedJobs:
      # ä¸èƒ½éšæ„ä¿®æ”¹è¯¥åç§°
      # Kubeflow Traineråœ¨ç”ŸæˆPET_MASTER_ADDRç¯å¢ƒå˜é‡æ—¶ç¡¬ç¼–ç ä½¿ç”¨äº†node
      - name: node
        template:
          metadata:
            labels:
              # trainjob-ancestor-step: æ ‡è¯†è¯¥ Job åœ¨è®­ç»ƒæµç¨‹ä¸­çš„è§’è‰²
              # trainer: è¡¨ç¤ºè¿™æ˜¯è®­ç»ƒä»»åŠ¡çš„ä¸»æ‰§è¡Œå™¨
              trainer.kubeflow.org/trainjob-ancestor-step: trainer
          spec:
            # è°ƒè¯•é…ç½®ï¼šå‡å°‘é‡è¯•æ¬¡æ•°ï¼Œå¿«é€Ÿå¤±è´¥
            backoffLimit: 2
            # è°ƒè¯•é…ç½®ï¼šå¤±è´¥åä¿ç•™ Pod 30åˆ†é’Ÿä»¥ä¾¿æŸ¥çœ‹æ—¥å¿—
            ttlSecondsAfterFinished: 1800
            template:
              spec:
                # è°ƒè¯•é…ç½®ï¼šå¤±è´¥åä¸è‡ªåŠ¨é‡å¯ï¼Œä¿ç•™ç°åœº
                restartPolicy: Never
                containers:
                # å½“Jobæ¨¡æ¿æ ‡ç­¾ä¸º trainer.kubeflow.org/trainjob-ancestor-step: trainer æ—¶
                # å¿…é¡»åŒ…å«åä¸º node çš„å®¹å™¨
                - name: node
                  image: pytorch/pytorch:2.4.1-cuda11.8-cudnn9-runtime
                  # ä½¿ç”¨ torchrun å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
                  # Kubeflow Trainer ä¼šè‡ªåŠ¨æ³¨å…¥ PET_* ç¯å¢ƒå˜é‡
                  command:
                    - torchrun
                    - --nproc_per_node=$(PET_NPROC_PER_NODE)
                    - --nnodes=$(PET_NNODES)
                    - --node_rank=$(PET_NODE_RANK)
                    - --master_addr=$(PET_MASTER_ADDR)
                    - --master_port=$(PET_MASTER_PORT)
                    - /workspace/pytorch-demo.py
                  env:
                  - name: PYTHONUNBUFFERED
                    value: "1"
```

åº”ç”¨é…ç½®ï¼š

```bash
kubectl apply -f training-runtime-demo.yaml
```

:::tip å…³é”®ç¯å¢ƒå˜é‡è¯´æ˜
`Kubeflow Trainer`ä¼šè‡ªåŠ¨ä¸ºè®­ç»ƒ`Pod`æ³¨å…¥ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š

| ç¯å¢ƒå˜é‡ | è¯´æ˜ | ç¤ºä¾‹å€¼ |
|----------|------|--------|
| `PET_NNODES` | è®­ç»ƒèŠ‚ç‚¹æ€»æ•° | `2` |
| `PET_NPROC_PER_NODE` | æ¯èŠ‚ç‚¹è¿›ç¨‹æ•° | `2` |
| `PET_NODE_RANK` | å½“å‰èŠ‚ç‚¹ç¼–å·ï¼ˆ0å¼€å§‹ï¼‰ | `0`, `1` |
| `PET_MASTER_ADDR` | ä¸»èŠ‚ç‚¹åœ°å€ | `trainjob-demo-node-0-0.trainjob-demo` |
| `PET_MASTER_PORT` | ä¸»èŠ‚ç‚¹ç«¯å£ | `29500` |
:::

## PyTorch åˆ†å¸ƒå¼è®­ç»ƒæ¡ˆä¾‹

### è®­ç»ƒè„šæœ¬è¯´æ˜

æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç®€å•çš„`PyTorch`åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬æ¥æ¼”ç¤ºä¸‰ç§éƒ¨ç½²æ–¹å¼ã€‚è¯¥è„šæœ¬å®ç°äº†ä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹è®­ç»ƒï¼š

```python
import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.distributed import DistributedSampler

# ====================== 1. å®šä¹‰ç®€å•çš„æ•°æ®é›†å’Œæ¨¡å‹ ======================
class SimpleDataset(Dataset):
    """ç®€å•çš„æ•°æ®é›†ï¼šè¾“å…¥æ˜¯éšæœºæ•°ï¼Œæ ‡ç­¾æ˜¯è¾“å…¥çš„2å€ï¼ˆç®€å•å›å½’ä»»åŠ¡ï¼‰"""
    def __init__(self, size=100):
        self.data = torch.randn(size, 1)  # è¾“å…¥ï¼š(size, 1)
        self.labels = self.data * 2 + 0.1 * torch.randn_like(self.data)  # æ ‡ç­¾ï¼š2x + å™ªå£°

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

class SimpleModel(nn.Module):
    """ç®€å•çš„çº¿æ€§æ¨¡å‹ï¼šy = wx + b"""
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.linear = nn.Linear(1, 1)

    def forward(self, x):
        return self.linear(x)

# ====================== 2. åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„ ======================
def init_distributed():
    """
    åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼ˆCPU + glooåç«¯ï¼‰
    
    Kubeflow Trainer ä¼šè‡ªåŠ¨è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š
    - RANK: å…¨å±€è¿›ç¨‹æ’åï¼ˆç”± torchrun è®¾ç½®ï¼‰
    - WORLD_SIZE: æ€»è¿›ç¨‹æ•°ï¼ˆç”± torchrun è®¾ç½®ï¼‰
    - LOCAL_RANK: èŠ‚ç‚¹å†…è¿›ç¨‹æ’åï¼ˆç”± torchrun è®¾ç½®ï¼‰
    - MASTER_ADDR: ä¸»èŠ‚ç‚¹åœ°å€ï¼ˆç”± torchrun è®¾ç½®ï¼‰
    - MASTER_PORT: ä¸»èŠ‚ç‚¹ç«¯å£ï¼ˆç”± torchrun è®¾ç½®ï¼‰
    """
    # ä»ç¯å¢ƒå˜é‡ä¸­è·å–åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°ï¼ˆtorchrun è®¾ç½®çš„æ ‡å‡†ç¯å¢ƒå˜é‡ï¼‰
    rank = int(os.environ.get("RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    
    print(f"[åˆå§‹åŒ–] RANK={rank}, WORLD_SIZE={world_size}, LOCAL_RANK={local_rank}")
    print(f"[åˆå§‹åŒ–] MASTER_ADDR={os.environ.get('MASTER_ADDR', 'N/A')}")
    print(f"[åˆå§‹åŒ–] MASTER_PORT={os.environ.get('MASTER_PORT', 'N/A')}")
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„
    # ä½¿ç”¨ gloo åç«¯ï¼ˆCPUä¸“ç”¨ï¼‰ï¼Œè‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
    dist.init_process_group(backend="gloo")
    
    print(f"[åˆå§‹åŒ–å®Œæˆ] æˆåŠŸåˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„")

    return rank, world_size, local_rank

# ====================== 3. æ ¸å¿ƒè®­ç»ƒå‡½æ•° ======================
def train():
    print("=" * 60)
    print("å¼€å§‹ PyTorch åˆ†å¸ƒå¼è®­ç»ƒï¼ˆKubeflow Trainer ç‰ˆæœ¬ï¼‰")
    print("=" * 60)
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    rank, world_size, local_rank = init_distributed()

    # è®¾ç½®å½“å‰è¿›ç¨‹çš„è®¾å¤‡ï¼ˆCPUï¼‰
    device = torch.device("cpu")

    # 1. æ„å»ºæ•°æ®é›†å’Œåˆ†å¸ƒå¼é‡‡æ ·å™¨ï¼ˆæ ¸å¿ƒï¼šå°†æ•°æ®åˆ†å‘ç»™ä¸åŒè¿›ç¨‹ï¼‰
    dataset = SimpleDataset(size=100)
    # DistributedSamplerï¼šä¿è¯ä¸åŒè¿›ç¨‹è¯»å–ä¸åŒçš„æ•°æ®åˆ†ç‰‡
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)
    dataloader = DataLoader(
        dataset,
        batch_size=10,
        sampler=sampler,  # å¿…é¡»ç”¨åˆ†å¸ƒå¼é‡‡æ ·å™¨ï¼Œæ›¿ä»£shuffle
        num_workers=0  # ç®€åŒ–demoï¼Œå…³é—­å¤šçº¿ç¨‹
    )

    # 2. æ„å»ºæ¨¡å‹å¹¶åŒ…è£…ä¸ºåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDDPï¼‰
    model = SimpleModel()
    # DDPï¼šè‡ªåŠ¨å¤„ç†å‚æ•°åŒæ­¥ã€æ¢¯åº¦èšåˆ
    model = nn.parallel.DistributedDataParallel(model, device_ids=None)  # CPUè®­ç»ƒï¼Œdevice_idsè®¾ä¸ºNone

    # 3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01)

    # 4. è®­ç»ƒå¾ªç¯
    epochs = 5
    print(f"\n[Rank {rank}] å¼€å§‹è®­ç»ƒï¼Œå…± {epochs} ä¸ª epoch")
    print("-" * 60)
    
    for epoch in range(epochs):
        # æ¯ä¸ªepochå¼€å§‹å‰ï¼Œæ›´æ–°é‡‡æ ·å™¨çš„epochï¼ˆä¿è¯åˆ†å¸ƒå¼æ•°æ®æ‰“ä¹±çš„ä¸€è‡´æ€§ï¼‰
        sampler.set_epoch(epoch)
        model.train()
        total_loss = 0.0

        for batch_idx, (data, labels) in enumerate(dataloader):
            # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            data, labels = data.to(device), labels.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(data)
            loss = criterion(outputs, labels)

            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        # åªåœ¨ä¸»è¿›ç¨‹ï¼ˆrank=0ï¼‰æ‰“å°è®­ç»ƒä¿¡æ¯
        if rank == 0:
            avg_loss = total_loss / len(dataloader)
            print(f"Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}")

    print("-" * 60)
    print(f"[Rank {rank}] è®­ç»ƒå®Œæˆï¼")
    
    # åœ¨ä¸»è¿›ç¨‹æ‰“å°æœ€ç»ˆçš„æ¨¡å‹å‚æ•°
    if rank == 0:
        print("\n" + "=" * 60)
        print("æœ€ç»ˆæ¨¡å‹å‚æ•°:")
        for name, param in model.named_parameters():
            print(f"  {name}: {param.data}")
        print("=" * 60)

    # æ¸…ç†åˆ†å¸ƒå¼è¿›ç¨‹ç»„
    dist.destroy_process_group()
    print(f"[Rank {rank}] è¿›ç¨‹ç»„å·²é”€æ¯")

if __name__ == "__main__":
    train()
```

:::tip ä»£ç å…³é”®ç‚¹
1. **åˆ†å¸ƒå¼åˆå§‹åŒ–**ï¼šä½¿ç”¨`dist.init_process_group(backend="gloo")`åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼Œ`CPU`è®­ç»ƒä½¿ç”¨`gloo`åç«¯
2. **æ•°æ®å¹¶è¡Œ**ï¼šä½¿ç”¨`DistributedSampler`ç¡®ä¿ä¸åŒè¿›ç¨‹è¯»å–ä¸åŒçš„æ•°æ®åˆ†ç‰‡
3. **æ¨¡å‹å¹¶è¡Œ**ï¼šä½¿ç”¨`DistributedDataParallel`åŒ…è£…æ¨¡å‹ï¼Œè‡ªåŠ¨å¤„ç†æ¢¯åº¦èšåˆ
4. **ç¯å¢ƒå˜é‡**ï¼š`Kubeflow Trainer`ä¼šè‡ªåŠ¨è®¾ç½®æ‰€æœ‰å¿…è¦çš„ç¯å¢ƒå˜é‡ï¼ˆ`RANK`ã€`WORLD_SIZE`ã€`MASTER_ADDR`ç­‰ï¼‰
:::

### æœ¬åœ°æµ‹è¯•

åœ¨æäº¤åˆ°`Kubernetes`ä¹‹å‰ï¼Œå¯ä»¥å…ˆåœ¨æœ¬åœ°æµ‹è¯•è„šæœ¬æ˜¯å¦æ­£å¸¸è¿è¡Œï¼š

```bash
# å¯åŠ¨2ä¸ªCPUè¿›ç¨‹æ¨¡æ‹Ÿåˆ†å¸ƒå¼è®­ç»ƒï¼ˆworld_size=2ï¼‰
torchrun --nproc_per_node=2 --master_port=23456 pytorch-demo.py
```

é¢„æœŸè¾“å‡ºï¼š

```bash
[åˆå§‹åŒ–] RANK=0, WORLD_SIZE=2, LOCAL_RANK=0
[åˆå§‹åŒ–å®Œæˆ] æˆåŠŸåˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„
[Rank 0] å¼€å§‹è®­ç»ƒï¼Œå…± 5 ä¸ª epoch
------------------------------------------------------------
Epoch [1/5], Average Loss: 0.0234
Epoch [2/5], Average Loss: 0.0189
Epoch [3/5], Average Loss: 0.0156
Epoch [4/5], Average Loss: 0.0128
Epoch [5/5], Average Loss: 0.0103
------------------------------------------------------------
[Rank 0] è®­ç»ƒå®Œæˆï¼
```

## éƒ¨ç½²æ–¹å¼ä¸€ï¼šConfigMap æŒ‚è½½è„šæœ¬

### æ–¹æ¡ˆè¯´æ˜

`ConfigMap`æ–¹å¼é€‚åˆå¿«é€Ÿå¼€å‘å’Œæµ‹è¯•åœºæ™¯ï¼Œå°†è®­ç»ƒè„šæœ¬å­˜å‚¨åœ¨`ConfigMap`ä¸­ï¼Œç„¶åæŒ‚è½½åˆ°è®­ç»ƒ`Pod`ä¸­ã€‚

**ä¼˜ç‚¹ï¼š**
- âœ… å¿«é€Ÿè¿­ä»£ï¼šä¿®æ”¹è„šæœ¬åªéœ€æ›´æ–°`ConfigMap`
- âœ… æ— éœ€æ„å»ºé•œåƒï¼šé¿å…é•œåƒæ„å»ºå’Œæ¨é€çš„å¼€é”€
- âœ… é€‚åˆè°ƒè¯•ï¼šå¿«é€ŸéªŒè¯è®­ç»ƒé€»è¾‘

**ç¼ºç‚¹ï¼š**
- âŒ æ–‡ä»¶å¤§å°é™åˆ¶ï¼š`ConfigMap`æœ€å¤§`1MB`
- âŒ ä¸é€‚åˆç”Ÿäº§ï¼šç¼ºå°‘ç‰ˆæœ¬æ§åˆ¶å’Œå›æ»šèƒ½åŠ›

### éƒ¨ç½²æ­¥éª¤

#### åˆ›å»º ConfigMap

å°†è®­ç»ƒè„šæœ¬æ‰“åŒ…åˆ°`ConfigMap`ä¸­ï¼š

```bash
kubectl create configmap pytorch-demo-script --from-file=pytorch-demo.py
```

éªŒè¯`ConfigMap`ï¼š

```bash
kubectl get configmap pytorch-demo-script
kubectl describe configmap pytorch-demo-script
```

#### åˆ›å»º TrainJob

åˆ›å»º`trainjob-demo-with-configmap.yaml`ï¼š

```yaml
apiVersion: trainer.kubeflow.org/v1alpha1
kind: TrainJob
metadata:
  name: trainjob-demo-with-configmap
spec:
  # å¼•ç”¨å‰é¢å®šä¹‰çš„ ClusterTrainingRuntime
  runtimeRef:
    kind: ClusterTrainingRuntime
    name: training-runtime-demo
  
  # è®­ç»ƒå™¨é…ç½®
  trainer:
    # è®­ç»ƒèŠ‚ç‚¹æ•°é‡
    numNodes: 2
    
    # æ¯ä¸ªèŠ‚ç‚¹çš„è¿›ç¨‹æ•°
    numProcPerNode: 2
    
    # æ¯ä¸ªèŠ‚ç‚¹çš„èµ„æºé…ç½®
    resourcesPerNode:
      requests:
        cpu: "1"
        memory: "2Gi"
      limits:
        cpu: "1"
        memory: "2Gi"

  # Pod æ¨¡æ¿è¦†ç›–é…ç½® - ç”¨äºæŒ‚è½½ ConfigMap
  podTemplateOverrides:
    - targetJobs:
        - name: node
      spec:
        containers:
          - name: node
            volumeMounts:
              - name: training-script
                mountPath: /workspace
        volumes:
          - name: training-script
            configMap:
              name: pytorch-demo-script
```

åº”ç”¨é…ç½®ï¼š

```bash
kubectl apply -f trainjob-demo-with-configmap.yaml
```

#### æŸ¥çœ‹è®­ç»ƒçŠ¶æ€

```bash
# æŸ¥çœ‹ TrainJob çŠ¶æ€
kubectl get trainjob trainjob-demo-with-configmap

# æŸ¥çœ‹ Pod çŠ¶æ€
kubectl get pods -l training.kubeflow.org/job-name=trainjob-demo-with-configmap

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
kubectl logs -l training.kubeflow.org/job-name=trainjob-demo-with-configmap -f

# æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
kubectl describe trainjob trainjob-demo-with-configmap
```

#### é¢„æœŸè¾“å‡º

ä»ä»»ä¸€è®­ç»ƒ`Pod`çš„æ—¥å¿—ä¸­ï¼Œä½ åº”è¯¥çœ‹åˆ°ç±»ä¼¼çš„è¾“å‡ºï¼š

```bash
[åˆå§‹åŒ–] RANK=0, WORLD_SIZE=4, LOCAL_RANK=0
[åˆå§‹åŒ–] MASTER_ADDR=trainjob-demo-with-configmap-node-0-0.trainjob-demo-with-configmap
[åˆå§‹åŒ–] MASTER_PORT=29500
[åˆå§‹åŒ–å®Œæˆ] æˆåŠŸåˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„
[Rank 0] å¼€å§‹è®­ç»ƒï¼Œå…± 5 ä¸ª epoch
------------------------------------------------------------
Epoch [1/5], Average Loss: 0.0234
Epoch [2/5], Average Loss: 0.0189
Epoch [3/5], Average Loss: 0.0156
Epoch [4/5], Average Loss: 0.0128
Epoch [5/5], Average Loss: 0.0103
------------------------------------------------------------
[Rank 0] è®­ç»ƒå®Œæˆï¼
```

:::tip åˆ†å¸ƒå¼è®­ç»ƒéªŒè¯
- æŸ¥çœ‹`WORLD_SIZE`ç¡®è®¤æ€»è¿›ç¨‹æ•°ï¼š`2èŠ‚ç‚¹ Ã— 2è¿›ç¨‹/èŠ‚ç‚¹ = 4è¿›ç¨‹`
- æŸ¥çœ‹`RANK`ç¡®è®¤è¿›ç¨‹ç¼–å·ï¼š`0, 1, 2, 3`
- æŸ¥çœ‹`MASTER_ADDR`ç¡®è®¤ä¸»èŠ‚ç‚¹åœ°å€
:::

#### æ¸…ç†èµ„æº

```bash
kubectl delete trainjob trainjob-demo-with-configmap
kubectl delete configmap pytorch-demo-script
```

## éƒ¨ç½²æ–¹å¼äºŒï¼šHostPath æŒ‚è½½è„šæœ¬

### æ–¹æ¡ˆè¯´æ˜

`HostPath`æ–¹å¼å°†å®¿ä¸»æœºç›®å½•æŒ‚è½½åˆ°è®­ç»ƒ`Pod`ä¸­ï¼Œé€‚åˆè®­ç»ƒè„šæœ¬é¢‘ç¹å˜æ›´çš„åœºæ™¯ã€‚

**ä¼˜ç‚¹ï¼š**
- âœ… æ— éœ€é‡å»ºï¼šç›´æ¥ä¿®æ”¹å®¿ä¸»æœºæ–‡ä»¶ï¼Œ`Pod`é‡å¯å³å¯ç”Ÿæ•ˆ
- âœ… å¤§æ–‡ä»¶æ”¯æŒï¼šä¸å—`ConfigMap`å¤§å°é™åˆ¶
- âœ… å…±äº«å­˜å‚¨ï¼šå¤šä¸ª`Pod`å¯ä»¥å…±äº«ç›¸åŒçš„è„šæœ¬å’Œæ•°æ®

**ç¼ºç‚¹ï¼š**
- âŒ èŠ‚ç‚¹ä¾èµ–ï¼šéœ€è¦ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰ç›¸åŒè·¯å¾„çš„æ–‡ä»¶ï¼ˆå¯ä»¥ä½¿ç”¨äº‘ç›˜ï¼‰
- âŒ å®‰å…¨é£é™©ï¼šç›´æ¥è®¿é—®å®¿ä¸»æœºæ–‡ä»¶ç³»ç»Ÿ
- âŒ ä¸é€‚åˆç”Ÿäº§ï¼šç¼ºå°‘éš”ç¦»å’Œç‰ˆæœ¬æ§åˆ¶

### éƒ¨ç½²æ­¥éª¤

#### å‡†å¤‡å®¿ä¸»æœºç›®å½•

åœ¨æ‰€æœ‰è®­ç»ƒèŠ‚ç‚¹ä¸Šåˆ›å»ºç›¸åŒçš„ç›®å½•ç»“æ„å¹¶æ”¾ç½®è„šæœ¬ï¼š

```bash
# åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šæ‰§è¡Œ
sudo mkdir -p /data/kubeflow-training/scripts
sudo cp pytorch-demo.py /data/kubeflow-training/scripts/
sudo chmod -R 755 /data/kubeflow-training/scripts
```

:::warning æ³¨æ„
ä½¿ç”¨`HostPath`æ—¶ï¼Œå¿…é¡»ç¡®ä¿è„šæœ¬æ–‡ä»¶åœ¨æ‰€æœ‰å¯èƒ½è°ƒåº¦è®­ç»ƒ`Pod`çš„èŠ‚ç‚¹ä¸Šéƒ½å­˜åœ¨ï¼Œä¸”è·¯å¾„å®Œå…¨ä¸€è‡´ã€‚
:::

#### åˆ›å»º TrainJob

åˆ›å»º`trainjob-demo-with-hostpath.yaml`ï¼š

```yaml
apiVersion: trainer.kubeflow.org/v1alpha1
kind: TrainJob
metadata:
  name: trainjob-demo-with-hostpath
spec:
  # å¼•ç”¨å‰é¢å®šä¹‰çš„ ClusterTrainingRuntime
  runtimeRef:
    kind: ClusterTrainingRuntime
    name: training-runtime-demo
  
  # è®­ç»ƒå™¨é…ç½®
  trainer:
    # è®­ç»ƒèŠ‚ç‚¹æ•°é‡
    numNodes: 2
    
    # æ¯ä¸ªèŠ‚ç‚¹çš„è¿›ç¨‹æ•°
    numProcPerNode: 2
    
    # æ¯ä¸ªèŠ‚ç‚¹çš„èµ„æºé…ç½®
    resourcesPerNode:
      requests:
        cpu: "1"
        memory: "2Gi"
      limits:
        cpu: "1"
        memory: "2Gi"

  # Pod æ¨¡æ¿è¦†ç›–é…ç½® - ç”¨äºæŒ‚è½½ HostPath
  podTemplateOverrides:
    - targetJobs:
        - name: node
      spec:
        containers:
          - name: node
            volumeMounts:
              - name: training-script
                mountPath: /workspace
                readOnly: true
        volumes:
          - name: training-script
            # ä½¿ç”¨ HostPath æŒ‚è½½å®¿ä¸»æœºç›®å½•
            hostPath:
              # å®¿ä¸»æœºä¸Šçš„è„šæœ¬ç›®å½•è·¯å¾„
              path: /data/kubeflow-training/scripts
              type: Directory
```

åº”ç”¨é…ç½®ï¼š

```bash
kubectl apply -f trainjob-demo-with-hostpath.yaml
```

#### æŸ¥çœ‹è®­ç»ƒçŠ¶æ€

```bash
# æŸ¥çœ‹ TrainJob çŠ¶æ€
kubectl get trainjob trainjob-demo-with-hostpath

# æŸ¥çœ‹ Pod çŠ¶æ€
kubectl get pods -l training.kubeflow.org/job-name=trainjob-demo-with-hostpath

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
kubectl logs -l training.kubeflow.org/job-name=trainjob-demo-with-hostpath -f
```

#### åŠ¨æ€æ›´æ–°è„šæœ¬

`HostPath`çš„ä¼˜åŠ¿åœ¨äºå¯ä»¥åŠ¨æ€æ›´æ–°è„šæœ¬è€Œæ— éœ€é‡å»ºé•œåƒï¼š

```bash
# ä¿®æ”¹å®¿ä¸»æœºä¸Šçš„è„šæœ¬
sudo vi /data/kubeflow-training/scripts/pytorch-demo.py

# åˆ é™¤ç°æœ‰çš„ TrainJobï¼ˆä¼šè§¦å‘ Pod é‡å¯ï¼‰
kubectl delete trainjob trainjob-demo-with-hostpath

# é‡æ–°åˆ›å»º TrainJobï¼ˆPod ä¼šè¯»å–æœ€æ–°çš„è„šæœ¬ï¼‰
kubectl apply -f trainjob-demo-with-hostpath.yaml
```

#### æ¸…ç†èµ„æº

```bash
kubectl delete trainjob trainjob-demo-with-hostpath

# æ¸…ç†å®¿ä¸»æœºæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
sudo rm -rf /data/kubeflow-training/scripts
```

## éƒ¨ç½²æ–¹å¼ä¸‰ï¼šé•œåƒæ„å»ºæ–¹å¼ï¼ˆæ¨èï¼‰

### æ–¹æ¡ˆè¯´æ˜

é•œåƒæ„å»ºæ–¹å¼æ˜¯ç”Ÿäº§ç¯å¢ƒçš„æ¨èæ–¹å¼ï¼Œå°†è®­ç»ƒè„šæœ¬æ‰“åŒ…åˆ°`Docker`é•œåƒä¸­ã€‚

**ä¼˜ç‚¹ï¼š**
- âœ… ç”Ÿäº§å°±ç»ªï¼šå®Œæ•´çš„ç‰ˆæœ¬æ§åˆ¶å’Œå›æ»šèƒ½åŠ›
- âœ… å¯ç§»æ¤æ€§ï¼šé•œåƒå¯ä»¥åœ¨ä»»ä½•`Kubernetes`é›†ç¾¤ä¸­è¿è¡Œ
- âœ… ä¾èµ–ç®¡ç†ï¼šå¯ä»¥åœ¨é•œåƒä¸­é¢„è£…æ‰€æœ‰ä¾èµ–
- âœ… å®‰å…¨éš”ç¦»ï¼šä¸ä¾èµ–å®¿ä¸»æœºæ–‡ä»¶ç³»ç»Ÿ

**ç¼ºç‚¹ï¼š**
- âŒ æ„å»ºå¼€é”€ï¼šæ¯æ¬¡ä¿®æ”¹éƒ½éœ€è¦é‡æ–°æ„å»ºå’Œæ¨é€é•œåƒ
- âŒ å­˜å‚¨æˆæœ¬ï¼šéœ€è¦é•œåƒä»“åº“å­˜å‚¨ç©ºé—´

### éƒ¨ç½²æ­¥éª¤

#### åˆ›å»º Dockerfile

åˆ›å»º`Dockerfile`ï¼š

```dockerfile
FROM --platform=linux/amd64 pytorch/pytorch:2.4.1-cuda11.8-cudnn9-runtime

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /workspace

# å¤åˆ¶è®­ç»ƒè„šæœ¬
COPY pytorch-demo.py /workspace/pytorch-demo.py

# è®¾ç½®Pythonä¸ç¼“å†²è¾“å‡ºï¼ˆä¾¿äºæŸ¥çœ‹æ—¥å¿—ï¼‰
ENV PYTHONUNBUFFERED=1
```

:::info é•œåƒè¯´æ˜
- åŸºç¡€é•œåƒï¼šä½¿ç”¨å®˜æ–¹`PyTorch`é•œåƒï¼Œå·²åŒ…å«`PyTorch`å’Œå¸¸ç”¨ä¾èµ–
- å¹³å°æŒ‡å®šï¼š`--platform=linux/amd64`ç¡®ä¿é•œåƒå…¼å®¹æ€§ï¼ˆç‰¹åˆ«æ˜¯åœ¨`Apple Silicon`ä¸Šæ„å»ºæ—¶ï¼‰
- å·¥ä½œç›®å½•ï¼šå°†è„šæœ¬æ”¾åœ¨`/workspace`ç›®å½•ï¼Œä¸å‰é¢çš„é…ç½®ä¿æŒä¸€è‡´
:::

#### æ„å»ºé•œåƒ

```bash
docker buildx build --load --platform linux/amd64 -t pytorch-demo:latest .
```

:::tip æ„å»ºé€‰é¡¹è¯´æ˜
- `--load`ï¼šæ„å»ºå®ŒæˆååŠ è½½åˆ°æœ¬åœ°`Docker`
- `--platform linux/amd64`ï¼šæŒ‡å®šç›®æ ‡å¹³å°ä¸º`amd64`
- `-t pytorch-demo:latest`ï¼šæŒ‡å®šé•œåƒæ ‡ç­¾
:::

éªŒè¯é•œåƒï¼š

```bash
docker images | grep pytorch-demo
```

#### æ¨é€é•œåƒåˆ°ä»“åº“ï¼ˆå¯é€‰ï¼‰

å¦‚æœæ˜¯å¤šèŠ‚ç‚¹é›†ç¾¤ï¼Œéœ€è¦å°†é•œåƒæ¨é€åˆ°é•œåƒä»“åº“ï¼š

```bash
# æ ‡è®°é•œåƒ
docker tag pytorch-demo:latest your-registry/pytorch-demo:latest

# æ¨é€åˆ°é•œåƒä»“åº“
docker push your-registry/pytorch-demo:latest
```

:::info æœ¬åœ°æµ‹è¯•
å¦‚æœä½¿ç”¨å•èŠ‚ç‚¹é›†ç¾¤ï¼ˆå¦‚`Kind`ã€`Minikube`ï¼‰ï¼Œå¯ä»¥å°†é•œåƒç›´æ¥åŠ è½½åˆ°é›†ç¾¤ï¼š

```bash
# Kind ç¤ºä¾‹
kind load docker-image pytorch-demo:latest

# Minikube ç¤ºä¾‹
minikube image load pytorch-demo:latest
```
:::

#### åˆ›å»º TrainJob

åˆ›å»º`trainjob-demo-with-image.yaml`ï¼š

```yaml
apiVersion: trainer.kubeflow.org/v1alpha1
kind: TrainJob
metadata:
  name: trainjob-demo-with-image
spec:
  # å¼•ç”¨å‰é¢å®šä¹‰çš„ ClusterTrainingRuntime
  runtimeRef:
    kind: ClusterTrainingRuntime
    name: training-runtime-demo
  
  # è®­ç»ƒå™¨é…ç½®
  trainer:
    # å®¹å™¨é•œåƒï¼ˆä¼šè¦†ç›– runtime ä¸­çš„é•œåƒï¼‰
    image: pytorch-demo:latest

    # è®­ç»ƒèŠ‚ç‚¹æ•°é‡
    numNodes: 2
    
    # æ¯ä¸ªèŠ‚ç‚¹çš„è¿›ç¨‹æ•°
    numProcPerNode: 2
    
    # æ¯ä¸ªèŠ‚ç‚¹çš„èµ„æºé…ç½®
    resourcesPerNode:
      requests:
        cpu: "1"
        memory: "2Gi"
      limits:
        cpu: "1"
        memory: "2Gi"
```

:::tip é•œåƒè¦†ç›–
`trainer.image`å­—æ®µä¼šè¦†ç›–`ClusterTrainingRuntime`ä¸­å®šä¹‰çš„é•œåƒï¼Œå› æ­¤ä¸éœ€è¦ä¿®æ”¹`runtime`é…ç½®ã€‚
:::

åº”ç”¨é…ç½®ï¼š

```bash
kubectl apply -f trainjob-demo-with-image.yaml
```

#### æŸ¥çœ‹è®­ç»ƒçŠ¶æ€

```bash
# æŸ¥çœ‹ TrainJob çŠ¶æ€
kubectl get trainjob trainjob-demo-with-image

# æŸ¥çœ‹ Pod çŠ¶æ€
kubectl get pods -l training.kubeflow.org/job-name=trainjob-demo-with-image

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
kubectl logs -l training.kubeflow.org/job-name=trainjob-demo-with-image -f
```

#### æ›´æ–°è®­ç»ƒè„šæœ¬

å½“éœ€è¦æ›´æ–°è®­ç»ƒè„šæœ¬æ—¶ï¼š

```bash
# 1. ä¿®æ”¹è„šæœ¬
vi pytorch-demo.py

# 2. é‡æ–°æ„å»ºé•œåƒï¼ˆä½¿ç”¨æ–°çš„ç‰ˆæœ¬æ ‡ç­¾ï¼‰
docker buildx build --load --platform linux/amd64 -t pytorch-demo:v1.1 .

# 3. æ›´æ–° TrainJob é…ç½®ä¸­çš„é•œåƒç‰ˆæœ¬
# ä¿®æ”¹ trainjob-demo-with-image.yaml ä¸­çš„ image: pytorch-demo:v1.1

# 4. é‡æ–°éƒ¨ç½²
kubectl delete trainjob trainjob-demo-with-image
kubectl apply -f trainjob-demo-with-image.yaml
```

:::tip ç‰ˆæœ¬ç®¡ç†æœ€ä½³å®è·µ
- ä½¿ç”¨è¯­ä¹‰åŒ–ç‰ˆæœ¬å·ï¼š`v1.0.0`ã€`v1.1.0`ç­‰
- é¿å…ä½¿ç”¨`latest`æ ‡ç­¾ï¼ˆé™¤éæ˜¯å¼€å‘æµ‹è¯•ç¯å¢ƒï¼‰
- åœ¨`Git`ä¸­ä¸ºæ¯ä¸ªé•œåƒç‰ˆæœ¬æ‰“`tag`
- ä½¿ç”¨`CI/CD`è‡ªåŠ¨åŒ–é•œåƒæ„å»ºå’Œæ¨é€æµç¨‹
:::

#### æ¸…ç†èµ„æº

```bash
kubectl delete trainjob trainjob-demo-with-image

# æ¸…ç†æœ¬åœ°é•œåƒï¼ˆå¯é€‰ï¼‰
docker rmi pytorch-demo:latest
```

## ä¸‰ç§éƒ¨ç½²æ–¹å¼å¯¹æ¯”

| å¯¹æ¯”ç»´åº¦ | ConfigMap | HostPath | é•œåƒæ„å»º |
|---------|-----------|----------|---------|
| **é€‚ç”¨åœºæ™¯** | å¿«é€Ÿå¼€å‘ã€å°è„šæœ¬æµ‹è¯• | é¢‘ç¹ä¿®æ”¹ã€æœ¬åœ°å¼€å‘ | ç”Ÿäº§ç¯å¢ƒã€å¤šé›†ç¾¤éƒ¨ç½² |
| **æ›´æ–°é€Ÿåº¦** | âš¡ å¿«ï¼ˆæ›´æ–°ConfigMapï¼‰ | âš¡âš¡ æœ€å¿«ï¼ˆä¿®æ”¹æ–‡ä»¶ï¼‰ | ğŸ¢ æ…¢ï¼ˆé‡æ–°æ„å»ºé•œåƒï¼‰ |
| **ç‰ˆæœ¬æ§åˆ¶** | âŒ ä¸æ”¯æŒ | âŒ ä¸æ”¯æŒ | âœ… å®Œæ•´æ”¯æŒ |
| **ä¾èµ–ç®¡ç†** | âŒ éœ€åŸºç¡€é•œåƒåŒ…å« | âŒ éœ€åŸºç¡€é•œåƒåŒ…å« | âœ… å¯è‡ªå®šä¹‰å®‰è£… |
| **æ–‡ä»¶å¤§å°é™åˆ¶** | âš ï¸ 1MB | âœ… æ— é™åˆ¶ | âœ… æ— é™åˆ¶ |
| **å¤šèŠ‚ç‚¹åŒæ­¥** | âœ… è‡ªåŠ¨åŒæ­¥ | âŒ éœ€æ‰‹åŠ¨åŒæ­¥ | âœ… è‡ªåŠ¨åŒæ­¥ |
| **å®‰å…¨æ€§** | âœ… è¾ƒå¥½ | âš ï¸ è¾ƒå·® | âœ… æœ€å¥½ |
| **å¯ç§»æ¤æ€§** | âš¡ ä¸­ç­‰ | âŒ å·® | âœ… æœ€å¥½ |
| **ç”Ÿäº§å°±ç»ª** | âŒ | âŒ | âœ… |


## å¸¸è§é—®é¢˜æ’æŸ¥

### Pod Pending

**é—®é¢˜æè¿°ï¼š**
```bash
kubectl get pods
NAME                                   READY   STATUS    RESTARTS   AGE
trainjob-demo-node-0-0-xxxxx          0/1     Pending   0          5m
```

**æ’æŸ¥æ­¥éª¤ï¼š**

```bash
# æŸ¥çœ‹ Pod è¯¦æƒ…
kubectl describe pod <pod-name>

# å¸¸è§åŸå› ï¼š
# 1. èµ„æºä¸è¶³ï¼šInsufficient cpu/memory
# 2. èŠ‚ç‚¹ä¸å¯è°ƒåº¦ï¼šNode Unschedulable
# 3. é•œåƒæ‹‰å–å¤±è´¥ï¼šImagePullBackOff
```

**è§£å†³æ–¹æ¡ˆï¼š**

```yaml
# 1. é™ä½èµ„æºè¯·æ±‚
resourcesPerNode:
  requests:
    cpu: "500m"    # é™ä½åˆ°500æ¯«æ ¸
    memory: "1Gi"  # é™ä½åˆ°1GB

# 2. æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
kubectl get nodes
kubectl describe node <node-name>

# 3. ä½¿ç”¨æœ¬åœ°é•œåƒï¼ˆå¼€å‘ç¯å¢ƒï¼‰
imagePullPolicy: Never  # æ·»åŠ åˆ°å®¹å™¨é…ç½®ä¸­
```

### ConfigMap Not Found

**é—®é¢˜æè¿°ï¼š**
```bash
Error: configmaps "pytorch-demo-script" not found
```

**è§£å†³æ–¹æ¡ˆï¼š**

```bash
# 1. ç¡®è®¤ ConfigMap æ˜¯å¦å­˜åœ¨
kubectl get configmap

# 2. ç¡®è®¤å‘½åç©ºé—´æ˜¯å¦æ­£ç¡®
kubectl get configmap -n <namespace>

# 3. é‡æ–°åˆ›å»º ConfigMap
kubectl create configmap pytorch-demo-script --from-file=pytorch-demo.py
```

### HostPath æ–‡ä»¶ä¸å­˜åœ¨

**é—®é¢˜æè¿°ï¼š**
```bash
Error: failed to start container: stat /data/kubeflow-training/scripts: no such file or directory
```

**è§£å†³æ–¹æ¡ˆï¼š**

```bash
# 1. ç™»å½•åˆ°è®­ç»ƒ Pod è°ƒåº¦çš„èŠ‚ç‚¹
kubectl get pod <pod-name> -o wide  # æŸ¥çœ‹ NODE åˆ—

# 2. åœ¨è¯¥èŠ‚ç‚¹ä¸Šåˆ›å»ºç›®å½•å¹¶å¤åˆ¶æ–‡ä»¶
ssh <node>
sudo mkdir -p /data/kubeflow-training/scripts
sudo cp pytorch-demo.py /data/kubeflow-training/scripts/
sudo chmod -R 755 /data/kubeflow-training/scripts

# 3. å¯¹æ‰€æœ‰å¯èƒ½çš„è®­ç»ƒèŠ‚ç‚¹é‡å¤ä¸Šè¿°æ­¥éª¤
```

### è®­ç»ƒä»»åŠ¡å¡ä½ä¸å¯åŠ¨

**é—®é¢˜æè¿°ï¼š**
æ‰€æœ‰`Pod`éƒ½å¤„äº`Running`çŠ¶æ€ï¼Œä½†è®­ç»ƒæ—¥å¿—æ²¡æœ‰è¾“å‡ºã€‚

**æ’æŸ¥æ­¥éª¤ï¼š**

```bash
# 1. æŸ¥çœ‹æ‰€æœ‰ Pod çš„æ—¥å¿—
kubectl logs -l training.kubeflow.org/job-name=<trainjob-name> --all-containers

# 2. æ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦æ­£ç¡®æ³¨å…¥
kubectl exec <pod-name> -- env | grep PET_

# 3. æ£€æŸ¥ç½‘ç»œè¿é€šæ€§
kubectl exec <pod-name> -- ping <master-pod-ip>
```

**å¸¸è§åŸå› ï¼š**
1. ä¸»èŠ‚ç‚¹åœ°å€`MASTER_ADDR`ä¸æ­£ç¡®
2. ç«¯å£`29500`è¢«å ç”¨æˆ–æ— æ³•è®¿é—®
3. é˜²ç«å¢™è§„åˆ™é˜»æ­¢äº†`Pod`é—´é€šä¿¡

**è§£å†³æ–¹æ¡ˆï¼š**

```yaml
# 1. æ£€æŸ¥ ClusterTrainingRuntime é…ç½®
# ç¡®ä¿ replicatedJobs åç§°ä¸º "node"
replicatedJobs:
- name: node  # å¿…é¡»æ˜¯ "node"

# 2. æ£€æŸ¥ç«¯å£é…ç½®
# å¦‚æœç«¯å£å†²çªï¼Œå¯ä»¥ä¿®æ”¹é»˜è®¤ç«¯å£
env:
- name: MASTER_PORT
  value: "29501"  # ä½¿ç”¨ä¸åŒçš„ç«¯å£
```

### API ç‰ˆæœ¬é”™è¯¯

**é—®é¢˜æè¿°ï¼š**
```bash
error: unable to recognize "trainjob.yaml": no matches for kind "TrainJob" in version "trainer.kubeflow.org/v2alpha1"
```

**è§£å†³æ–¹æ¡ˆï¼š**

```bash
# 1. ç¡®è®¤é›†ç¾¤æ”¯æŒçš„ API ç‰ˆæœ¬
kubectl api-resources | grep trainjob

# 2. ä½¿ç”¨æ­£ç¡®çš„ API ç‰ˆæœ¬
apiVersion: trainer.kubeflow.org/v1alpha1  # è€Œä¸æ˜¯ v2alpha1
```

## æœ€ä½³å®è·µ

### èµ„æºé…ç½®

```yaml
# ç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®
trainer:
  numNodes: 4
  numProcPerNode: 8
  resourcesPerNode:
    requests:
      cpu: "16"
      memory: "64Gi"
      nvidia.com/gpu: 4  # GPUè®­ç»ƒ
    limits:
      cpu: "16"
      memory: "64Gi"
      nvidia.com/gpu: 4
```

### ç›‘æ§å’Œæ—¥å¿—

```yaml
# æ·»åŠ åˆ° ClusterTrainingRuntime çš„å®¹å™¨é…ç½®ä¸­
env:
# æ—¥å¿—é…ç½®
- name: PYTHONUNBUFFERED
  value: "1"
- name: LOGLEVEL
  value: "INFO"

# Prometheus ç›‘æ§ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
- name: PROMETHEUS_PUSHGATEWAY
  value: "http://prometheus-pushgateway:9091"
```

### ä»»åŠ¡å®¹é”™

```yaml
# åœ¨ ClusterTrainingRuntime ä¸­é…ç½®
spec:
  template:
    spec:
      replicatedJobs:
      - name: node
        template:
          spec:
            # å¤±è´¥é‡è¯•æ¬¡æ•°
            backoffLimit: 3
            # ä»»åŠ¡å®Œæˆåä¿ç•™æ—¶é—´
            ttlSecondsAfterFinished: 3600
```

### å¼¹æ€§è®­ç»ƒ

```yaml
# æ”¯æŒåŠ¨æ€æ‰©ç¼©å®¹ï¼ˆPyTorch Elasticï¼‰
spec:
  mlPolicy:
    numNodes: 4
    torch:
      numProcPerNode: auto
      # å¯ç”¨å¼¹æ€§è®­ç»ƒ
      elasticPolicy:
        minNodes: 2       # æœ€å°‘èŠ‚ç‚¹æ•°
        maxNodes: 8       # æœ€å¤šèŠ‚ç‚¹æ•°
        maxRestarts: 3    # æœ€å¤§é‡å¯æ¬¡æ•°
```

### GPU è®­ç»ƒé…ç½®

```yaml
# GPU è®­ç»ƒç¤ºä¾‹
spec:
  mlPolicy:
    torch:
      numProcPerNode: gpu  # è‡ªåŠ¨æ£€æµ‹ GPU æ•°é‡
  template:
    spec:
      replicatedJobs:
      - name: node
        template:
          spec:
            template:
              spec:
                containers:
                - name: node
                  resources:
                    limits:
                      nvidia.com/gpu: 4  # æ¯èŠ‚ç‚¹4ä¸ªGPU
                  # GPUè®­ç»ƒä½¿ç”¨NCCLåç«¯
                  env:
                  - name: NCCL_DEBUG
                    value: "INFO"
```

## å‚è€ƒèµ„æ–™

- [Kubeflow Trainer å®˜æ–¹æ–‡æ¡£](https://www.kubeflow.org/docs/components/trainer/)
- [Kubeflow Trainer GitHub](https://github.com/kubeflow/trainer)
- [PyTorch åˆ†å¸ƒå¼è®­ç»ƒæ•™ç¨‹](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
- [torchrun æ–‡æ¡£](https://pytorch.org/docs/stable/elastic/run.html)
- [Kubernetes ConfigMap](https://kubernetes.io/docs/concepts/configuration/configmap/)
- [Kubernetes HostPath](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath)
